{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Curated Topic Modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>H1-1</th>\n",
       "      <th>Meta Description 1</th>\n",
       "      <th>Source</th>\n",
       "      <th>Title 1</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>level_1</th>\n",
       "      <th>Cluster_k</th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>level_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.simplilearn.com/machine-learning-i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prepping for machine learning interview? here ...</td>\n",
       "      <td>simplilearn</td>\n",
       "      <td>12 important machine learning interview questi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>machine</td>\n",
       "      <td>0</td>\n",
       "      <td>5696</td>\n",
       "      <td>61</td>\n",
       "      <td>0.2359</td>\n",
       "      <td>ai, mean, think, facebook, update, interview, ...</td>\n",
       "      <td>prepping for machine learning interview? here ...</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.datasciencecentral.com/profiles/bl...</td>\n",
       "      <td>k means clustering algorithm &amp; its application</td>\n",
       "      <td>what is k means clustering?\\n\\nclustering mean...</td>\n",
       "      <td>data science central</td>\n",
       "      <td>k means clustering algorithm &amp; its application...</td>\n",
       "      <td>541.0</td>\n",
       "      <td>others</td>\n",
       "      <td>0</td>\n",
       "      <td>3259</td>\n",
       "      <td>61</td>\n",
       "      <td>0.1618</td>\n",
       "      <td>ai, mean, think, facebook, update, interview, ...</td>\n",
       "      <td>what is k means clustering? clustering means g...</td>\n",
       "      <td>clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.datasciencecentral.com/profiles/bl...</td>\n",
       "      <td>an introduction to bayesian reasoning</td>\n",
       "      <td>an introduction to bayesian reasoning\\n\\nyou m...</td>\n",
       "      <td>data science central</td>\n",
       "      <td>an introduction to bayesian reasoning - data s...</td>\n",
       "      <td>1881.0</td>\n",
       "      <td>others</td>\n",
       "      <td>0</td>\n",
       "      <td>3506</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>ai, mean, think, facebook, update, interview, ...</td>\n",
       "      <td>an introduction to bayesian reasoning you migh...</td>\n",
       "      <td>bayesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.datasciencecentral.com/profiles/bl...</td>\n",
       "      <td>python: implementing a k-means algorithm with ...</td>\n",
       "      <td>the below is an example of how sklearn in pyth...</td>\n",
       "      <td>data science central</td>\n",
       "      <td>python: implementing a k-means algorithm with ...</td>\n",
       "      <td>1191.0</td>\n",
       "      <td>others</td>\n",
       "      <td>0</td>\n",
       "      <td>1150</td>\n",
       "      <td>61</td>\n",
       "      <td>0.1285</td>\n",
       "      <td>ai, mean, think, facebook, update, interview, ...</td>\n",
       "      <td>the below is an example of how sklearn in pyth...</td>\n",
       "      <td>clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.datasciencecentral.com/profiles/bl...</td>\n",
       "      <td>numeric measures for association rules</td>\n",
       "      <td>in today's post, we dive into understanding as...</td>\n",
       "      <td>data science central</td>\n",
       "      <td>numeric measures for association rules - data ...</td>\n",
       "      <td>1419.0</td>\n",
       "      <td>others</td>\n",
       "      <td>0</td>\n",
       "      <td>3791</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>algorithm, involve, neural_network, introducti...</td>\n",
       "      <td>in todays post, we dive into understanding ass...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Address  \\\n",
       "0  https://www.simplilearn.com/machine-learning-i...   \n",
       "1  https://www.datasciencecentral.com/profiles/bl...   \n",
       "2  https://www.datasciencecentral.com/profiles/bl...   \n",
       "3  https://www.datasciencecentral.com/profiles/bl...   \n",
       "4  https://www.datasciencecentral.com/profiles/bl...   \n",
       "\n",
       "                                                H1-1  \\\n",
       "0                                                NaN   \n",
       "1     k means clustering algorithm & its application   \n",
       "2              an introduction to bayesian reasoning   \n",
       "3  python: implementing a k-means algorithm with ...   \n",
       "4             numeric measures for association rules   \n",
       "\n",
       "                                  Meta Description 1                Source  \\\n",
       "0  prepping for machine learning interview? here ...           simplilearn   \n",
       "1  what is k means clustering?\\n\\nclustering mean...  data science central   \n",
       "2  an introduction to bayesian reasoning\\n\\nyou m...  data science central   \n",
       "3  the below is an example of how sklearn in pyth...  data science central   \n",
       "4  in today's post, we dive into understanding as...  data science central   \n",
       "\n",
       "                                             Title 1  Word Count  level_1  \\\n",
       "0  12 important machine learning interview questi...         NaN  machine   \n",
       "1  k means clustering algorithm & its application...       541.0   others   \n",
       "2  an introduction to bayesian reasoning - data s...      1881.0   others   \n",
       "3  python: implementing a k-means algorithm with ...      1191.0   others   \n",
       "4  numeric measures for association rules - data ...      1419.0   others   \n",
       "\n",
       "   Cluster_k  Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0          0         5696              61              0.2359   \n",
       "1          0         3259              61              0.1618   \n",
       "2          0         3506              61              0.0889   \n",
       "3          0         1150              61              0.1285   \n",
       "4          0         3791              66              0.0820   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  ai, mean, think, facebook, update, interview, ...   \n",
       "1  ai, mean, think, facebook, update, interview, ...   \n",
       "2  ai, mean, think, facebook, update, interview, ...   \n",
       "3  ai, mean, think, facebook, update, interview, ...   \n",
       "4  algorithm, involve, neural_network, introducti...   \n",
       "\n",
       "                                                Text     level_2  \n",
       "0  prepping for machine learning interview? here ...      career  \n",
       "1  what is k means clustering? clustering means g...  clustering  \n",
       "2  an introduction to bayesian reasoning you migh...    bayesian  \n",
       "3  the below is an example of how sklearn in pyth...  clustering  \n",
       "4  in todays post, we dive into understanding ass...        math  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','also','test','datum','blog','mastery','articles','science','central','complete','methods','method','comprehensive','learning','tutorial','guide','article','complete','learn','also','code','practical','simplilearn','python','machine','deep','vidhya'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'from',\n",
       " 'subject',\n",
       " 're',\n",
       " 'edu',\n",
       " 'use',\n",
       " 'also',\n",
       " 'test',\n",
       " 'datum',\n",
       " 'blog',\n",
       " 'mastery',\n",
       " 'articles',\n",
       " 'science',\n",
       " 'central',\n",
       " 'complete',\n",
       " 'methods',\n",
       " 'method',\n",
       " 'comprehensive',\n",
       " 'learning',\n",
       " 'tutorial',\n",
       " 'guide',\n",
       " 'article',\n",
       " 'complete',\n",
       " 'learn',\n",
       " 'also',\n",
       " 'code',\n",
       " 'practical',\n",
       " 'simplilearn',\n",
       " 'python',\n",
       " 'machine',\n",
       " 'deep',\n",
       " 'vidhya']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prepping for machine learning interview? here are the 12 potential interview '\n",
      " 'questions and answers that will help to crack the interview.']\n"
     ]
    }
   ],
   "source": [
    "data = df['Text'].values.tolist()\n",
    "\n",
    "# Convert to list\n",
    "data = df['Text'].values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['prepping',\n",
       "  'for',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'interview',\n",
       "  'here',\n",
       "  'are',\n",
       "  'the',\n",
       "  'potential',\n",
       "  'interview',\n",
       "  'questions',\n",
       "  'and',\n",
       "  'answers',\n",
       "  'that',\n",
       "  'will',\n",
       "  'help',\n",
       "  'to',\n",
       "  'crack',\n",
       "  'the',\n",
       "  'interview']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['prepping',\n",
       "  'interview',\n",
       "  'potential',\n",
       "  'interview',\n",
       "  'questions',\n",
       "  'answers',\n",
       "  'help',\n",
       "  'crack',\n",
       "  'interview'],\n",
       " ['means',\n",
       "  'clustering',\n",
       "  'clustering',\n",
       "  'means',\n",
       "  'grouping',\n",
       "  'things',\n",
       "  'similar',\n",
       "  'features',\n",
       "  'common',\n",
       "  'purpose',\n",
       "  'means',\n",
       "  'clustering',\n",
       "  'means'],\n",
       " ['introduction',\n",
       "  'bayesian',\n",
       "  'reasoning',\n",
       "  'might',\n",
       "  'using',\n",
       "  'bayesian',\n",
       "  'techniques',\n",
       "  'data',\n",
       "  'without',\n",
       "  'knowing',\n",
       "  'youre',\n",
       "  'could',\n",
       "  'enhance'],\n",
       " ['example',\n",
       "  'sklearn',\n",
       "  'used',\n",
       "  'develop',\n",
       "  'means',\n",
       "  'clustering',\n",
       "  'algorithm',\n",
       "  'purpose',\n",
       "  'means',\n",
       "  'clustering',\n",
       "  'able',\n",
       "  'part'],\n",
       " ['todays',\n",
       "  'post',\n",
       "  'dive',\n",
       "  'understanding',\n",
       "  'association',\n",
       "  'rules',\n",
       "  'market',\n",
       "  'basket',\n",
       "  'analysis',\n",
       "  'discuss',\n",
       "  'three',\n",
       "  'numeric',\n",
       "  'measures',\n",
       "  'considered',\n",
       "  'de'],\n",
       " ['neural',\n",
       "  'networks',\n",
       "  'require',\n",
       "  'considerable',\n",
       "  'time',\n",
       "  'computational',\n",
       "  'firepower',\n",
       "  'train',\n",
       "  'previously',\n",
       "  'researchers',\n",
       "  'believed',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'costly',\n",
       "  'train'],\n",
       " ['top',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'algorithms',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'takes',\n",
       "  'place',\n",
       "  'basis',\n",
       "  'sample',\n",
       "  'population',\n",
       "  'study',\n",
       "  'course',\n",
       "  'com'],\n",
       " ['algorithm',\n",
       "  'theoretical',\n",
       "  'limitations',\n",
       "  'data',\n",
       "  'give',\n",
       "  'computational',\n",
       "  'time',\n",
       "  'give'],\n",
       " ['quick',\n",
       "  'introduction',\n",
       "  'pymc',\n",
       "  'bayesian',\n",
       "  'models',\n",
       "  'part',\n",
       "  'post',\n",
       "  'give',\n",
       "  'brief',\n",
       "  'introduction',\n",
       "  'using',\n",
       "  'specific',\n",
       "  'hopefully',\n",
       "  'relate',\n",
       "  'able'],\n",
       " ['ok',\n",
       "  'read',\n",
       "  'bunch',\n",
       "  'stuff',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'many',\n",
       "  'layers',\n",
       "  'nodes',\n",
       "  'add',\n",
       "  'etc',\n",
       "  'start',\n",
       "  'implement',\n",
       "  'actua'],\n",
       " ['hadoop',\n",
       "  'rmr',\n",
       "  'implementation',\n",
       "  'distributed',\n",
       "  'kmeans',\n",
       "  'clustering',\n",
       "  'described',\n",
       "  'sample',\n",
       "  'dataset',\n",
       "  'first',\n",
       "  'dataset',\n",
       "  'shown'],\n",
       " ['algorithm',\n",
       "  'theoretical',\n",
       "  'limitations',\n",
       "  'data',\n",
       "  'give',\n",
       "  'computational',\n",
       "  'time',\n",
       "  'give'],\n",
       " ['post',\n",
       "  'originally',\n",
       "  'published',\n",
       "  'part',\n",
       "  'ongoing',\n",
       "  'series',\n",
       "  'popular',\n",
       "  'algorithms',\n",
       "  'explained',\n",
       "  'simple',\n",
       "  'english',\n",
       "  'aylien',\n",
       "  'text',\n",
       "  'analysis',\n",
       "  'pictu'],\n",
       " ['solution',\n",
       "  'broadly',\n",
       "  'divided',\n",
       "  'parts',\n",
       "  'typical',\n",
       "  'ml',\n",
       "  'exercise',\n",
       "  'would',\n",
       "  'involve',\n",
       "  'experimentation',\n",
       "  'iteration',\n",
       "  'parts',\n",
       "  'together'],\n",
       " ['ward',\n",
       "  'looks',\n",
       "  'cluster',\n",
       "  'analysis',\n",
       "  'analysis',\n",
       "  'variance',\n",
       "  'problem',\n",
       "  'involves',\n",
       "  'agglomerative',\n",
       "  'clustering',\n",
       "  'algorithm',\n",
       "  'starts'],\n",
       " ['summary',\n",
       "  'unless',\n",
       "  'involved',\n",
       "  'anomaly',\n",
       "  'detection',\n",
       "  'may',\n",
       "  'never',\n",
       "  'heard',\n",
       "  'unsupervised',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'interesting',\n",
       "  'approach',\n",
       "  'decision',\n",
       "  'tre'],\n",
       " ['probabilistic',\n",
       "  'graphical',\n",
       "  'models',\n",
       "  'principles',\n",
       "  'techniques',\n",
       "  'daphne',\n",
       "  'koller',\n",
       "  'evaluating',\n",
       "  'algorithms',\n",
       "  'nathalie',\n",
       "  'japkowicz',\n",
       "  'algorithms'],\n",
       " ['originally',\n",
       "  'started',\n",
       "  'writing',\n",
       "  'notebook',\n",
       "  'serve',\n",
       "  'introduction',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'description',\n",
       "  'rule',\n",
       "  'algorithm',\n",
       "  'think',\n",
       "  'worth',\n",
       "  'stud'],\n",
       " ['joint',\n",
       "  'post',\n",
       "  'inbar',\n",
       "  'naor',\n",
       "  'originally',\n",
       "  'published',\n",
       "  'engineering',\n",
       "  'taboola',\n",
       "  'com',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'dnn',\n",
       "  'become',\n",
       "  'powerful',\n",
       "  'complexity'],\n",
       " ['new',\n",
       "  'yorker',\n",
       "  'wrote',\n",
       "  'retired',\n",
       "  'reporter',\n",
       "  'using',\n",
       "  'algorithm',\n",
       "  'sort',\n",
       "  'murder',\n",
       "  'statistics',\n",
       "  'identify',\n",
       "  'link',\n",
       "  'murders',\n",
       "  'serial',\n",
       "  'killer'],\n",
       " ['introduction',\n",
       "  'post',\n",
       "  'designed',\n",
       "  'show',\n",
       "  'internal',\n",
       "  'changes',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'ann',\n",
       "  'nn',\n",
       "  'shows',\n",
       "  'outputs',\n",
       "  'neurons',\n",
       "  'beginning'],\n",
       " ['deepmind',\n",
       "  'attempted',\n",
       "  'measure',\n",
       "  'reasoning',\n",
       "  'ability',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'understand',\n",
       "  'nature',\n",
       "  'generalization',\n",
       "  'dataset',\n",
       "  'paper',\n",
       "  'links',\n",
       "  'included'],\n",
       " ['neural',\n",
       "  'networks',\n",
       "  'often',\n",
       "  'difficult',\n",
       "  'interpret',\n",
       "  'deepmind',\n",
       "  'attempted',\n",
       "  'demystify',\n",
       "  'using',\n",
       "  'neuron',\n",
       "  'deletion',\n",
       "  'latest',\n",
       "  'research'],\n",
       " ['random',\n",
       "  'forests',\n",
       "  'algorithm',\n",
       "  'always',\n",
       "  'fascinated',\n",
       "  'like',\n",
       "  'algorithm',\n",
       "  'easily',\n",
       "  'explained',\n",
       "  'anyone',\n",
       "  'without',\n",
       "  'much',\n",
       "  'hassle',\n",
       "  'one',\n",
       "  'quick',\n",
       "  'example'],\n",
       " ['neural',\n",
       "  'network',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'one',\n",
       "  'frequently',\n",
       "  'used',\n",
       "  'buzzwords',\n",
       "  'analytics',\n",
       "  'days',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'technique',\n",
       "  'wh'],\n",
       " ['top',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'algorithms',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'takes',\n",
       "  'place',\n",
       "  'basis',\n",
       "  'sample',\n",
       "  'population',\n",
       "  'study',\n",
       "  'course',\n",
       "  'com'],\n",
       " ['introduction',\n",
       "  'web',\n",
       "  'scraping',\n",
       "  'crawling',\n",
       "  'process',\n",
       "  'extracting',\n",
       "  'data',\n",
       "  'website',\n",
       "  'data',\n",
       "  'necessarily',\n",
       "  'form',\n",
       "  'text',\n",
       "  'co'],\n",
       " ['introduction',\n",
       "  'post',\n",
       "  'unsupervised',\n",
       "  'technique',\n",
       "  'called',\n",
       "  'kmeans',\n",
       "  'clustering',\n",
       "  'find',\n",
       "  'naturual',\n",
       "  'structures',\n",
       "  'data',\n",
       "  'oth'],\n",
       " ['multilayer',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'provides',\n",
       "  'thorough',\n",
       "  'understanding',\n",
       "  'multilayer',\n",
       "  'ann',\n",
       "  'implementing',\n",
       "  'forward',\n",
       "  'propagation',\n",
       "  'multilayer',\n",
       "  'perceptron',\n",
       "  'capacity',\n",
       "  'model',\n",
       "  'affected',\n",
       "  'underfitting',\n",
       "  'overfitting'],\n",
       " ['note',\n",
       "  'best',\n",
       "  'viewed',\n",
       "  'chrome',\n",
       "  'firefox',\n",
       "  'display',\n",
       "  'images',\n",
       "  'best',\n",
       "  'document',\n",
       "  'read',\n",
       "  'visualization',\n",
       "  'called',\n",
       "  'tour',\n",
       "  'thro'],\n",
       " ['means',\n",
       "  'algorithm',\n",
       "  'popular',\n",
       "  'efficient',\n",
       "  'approach',\n",
       "  'clustering',\n",
       "  'classification',\n",
       "  'data',\n",
       "  'first',\n",
       "  'introduction',\n",
       "  'means',\n",
       "  'algorithm',\n",
       "  'cond'],\n",
       " ['operations',\n",
       "  'research',\n",
       "  'including',\n",
       "  'monte',\n",
       "  'carlo',\n",
       "  'simulations',\n",
       "  'clustering',\n",
       "  'scoring',\n",
       "  'feature',\n",
       "  'selection',\n",
       "  'pattern',\n",
       "  'recognition',\n",
       "  'computational',\n",
       "  'comp'],\n",
       " ['rules',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'rule',\n",
       "  'process',\n",
       "  'mathematical',\n",
       "  'logic',\n",
       "  'improves',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network'],\n",
       " ['source', 'www', 'mstecker', 'com', 'clustering', 'unsupervised', 'tec'],\n",
       " ['ann',\n",
       "  'visualizer',\n",
       "  'library',\n",
       "  'uses',\n",
       "  'one',\n",
       "  'line',\n",
       "  'generate',\n",
       "  'visualization',\n",
       "  'dense',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network'],\n",
       " ['unsupervised',\n",
       "  'algorithms',\n",
       "  'algorithms',\n",
       "  'work',\n",
       "  'without',\n",
       "  'desired',\n",
       "  'output',\n",
       "  'label',\n",
       "  'supervised',\n",
       "  'algorithm',\n",
       "  'typically'],\n",
       " ['written',\n",
       "  'manish',\n",
       "  'saraswat',\n",
       "  'introduction',\n",
       "  'road',\n",
       "  'starts',\n",
       "  'regression',\n",
       "  'ready',\n",
       "  'aspiring',\n",
       "  'become'],\n",
       " ['unsupervised',\n",
       "  'clustering',\n",
       "  'clustering',\n",
       "  'algorithms',\n",
       "  'means',\n",
       "  'clustering',\n",
       "  'algorithm',\n",
       "  'majorly'],\n",
       " ['introduction',\n",
       "  'regular',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'input',\n",
       "  'transformed',\n",
       "  'series',\n",
       "  'hidden',\n",
       "  'layers',\n",
       "  'multiple',\n",
       "  'neurons',\n",
       "  'neuron',\n",
       "  'connected'],\n",
       " ['models',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'various',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'models',\n",
       "  'main',\n",
       "  'ones',\n",
       "  'multilayer',\n",
       "  'perceptron',\n",
       "  'feedforward',\n",
       "  'artificial'],\n",
       " ['introduction',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'conversation',\n",
       "  'one',\n",
       "  'challenging',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'problems',\n",
       "  'involves',\n",
       "  'language',\n",
       "  'understanding',\n",
       "  'reasoning'],\n",
       " ['clustering',\n",
       "  'output',\n",
       "  'results',\n",
       "  'using',\n",
       "  'spectral',\n",
       "  'clustering',\n",
       "  'normalized',\n",
       "  'laplacian',\n",
       "  'going',\n",
       "  'compared',\n",
       "  'taht',\n",
       "  'obtained',\n",
       "  'using',\n",
       "  'kmeans'],\n",
       " ['introduction',\n",
       "  'covers',\n",
       "  'various',\n",
       "  'aspects',\n",
       "  'starting',\n",
       "  'evolved',\n",
       "  'programming',\n",
       "  'stacks',\n",
       "  'used',\n",
       "  'basics',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['short',\n",
       "  'introduction',\n",
       "  'time',\n",
       "  'series',\n",
       "  'modeling',\n",
       "  'forecasting',\n",
       "  'presented',\n",
       "  'time',\n",
       "  'series',\n",
       "  'appears',\n",
       "  'many',\n",
       "  'industries',\n",
       "  'today',\n",
       "  'rely',\n",
       "  'predictin'],\n",
       " ['originally',\n",
       "  'posted',\n",
       "  'linkedin',\n",
       "  'known',\n",
       "  'fact',\n",
       "  'bagging',\n",
       "  'ensemble',\n",
       "  'technique',\n",
       "  'works',\n",
       "  'well',\n",
       "  'unstable',\n",
       "  'algorithms',\n",
       "  'like',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'artificial',\n",
       "  'neural'],\n",
       " ['neural',\n",
       "  'network',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'one',\n",
       "  'frequently',\n",
       "  'used',\n",
       "  'buzzwords',\n",
       "  'analytics',\n",
       "  'days',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'technique',\n",
       "  'wh'],\n",
       " ['introduction',\n",
       "  'lot',\n",
       "  'buzzword',\n",
       "  'around',\n",
       "  'term',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  'various',\n",
       "  'ways',\n",
       "  'great',\n",
       "  'report',\n",
       "  'reasonable',\n",
       "  'accuracies'],\n",
       " ['introduction',\n",
       "  'implementing',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'using',\n",
       "  'tensorflow',\n",
       "  'posted',\n",
       "  'faizan',\n",
       "  'shaikh',\n",
       "  'faizan',\n",
       "  'data',\n",
       "  'enthusiast'],\n",
       " ['neural',\n",
       "  'network',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'one',\n",
       "  'frequently',\n",
       "  'used',\n",
       "  'buzzwords',\n",
       "  'analytics',\n",
       "  'days',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'technique',\n",
       "  'wh'],\n",
       " ['train',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'focuses',\n",
       "  'ann',\n",
       "  'trained',\n",
       "  'using',\n",
       "  'perceptron',\n",
       "  'rule',\n",
       "  'implement',\n",
       "  'adaline',\n",
       "  'rule',\n",
       "  'ann',\n",
       "  'process',\n",
       "  'minimizing',\n",
       "  'cost',\n",
       "  'functions',\n",
       "  'using',\n",
       "  'gradient',\n",
       "  'descent',\n",
       "  'rule'],\n",
       " ['introduction',\n",
       "  'alone',\n",
       "  'little',\n",
       "  'together',\n",
       "  'much',\n",
       "  'phrase',\n",
       "  'helen',\n",
       "  'keller',\n",
       "  'reflection',\n",
       "  'achievements',\n",
       "  'successful',\n",
       "  'st'],\n",
       " ['artificial',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'introduction',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'ann',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'computational',\n",
       "  'algorithms',\n",
       "  'intended',\n",
       "  'simulate',\n",
       "  'behavio'],\n",
       " ['introduction',\n",
       "  'vast',\n",
       "  'area',\n",
       "  'computer',\n",
       "  'concerned',\n",
       "  'designing',\n",
       "  'algorithms',\n",
       "  'form',\n",
       "  'good',\n",
       "  'models',\n",
       "  'world',\n",
       "  'around',\n",
       "  'us',\n",
       "  'th'],\n",
       " ['researchers',\n",
       "  'developed',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'algorithm',\n",
       "  'making',\n",
       "  'movies',\n",
       "  'lines',\n",
       "  'text',\n",
       "  'hollywood',\n",
       "  'worried'],\n",
       " ['neural',\n",
       "  'network',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'one',\n",
       "  'frequently',\n",
       "  'used',\n",
       "  'buzzwords',\n",
       "  'analytics',\n",
       "  'days',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'technique',\n",
       "  'wh'],\n",
       " ['assessing', 'comparing', 'classifier', 'performance', 'roc', 'curves'],\n",
       " ['evaluate', 'algorithms'],\n",
       " ['algorithm'],\n",
       " ['using', 'unsupervised', 'improve', 'prediction', 'performance', 'data'],\n",
       " ['evaluate', 'performance', 'algorithms', 'using', 'resampling'],\n",
       " ['introduction', 'api', 'application', 'program', 'interface', 'data'],\n",
       " ['applying',\n",
       "  'data',\n",
       "  'determine',\n",
       "  'impact',\n",
       "  'employee',\n",
       "  'development',\n",
       "  'programs',\n",
       "  'performance',\n",
       "  'time',\n",
       "  'data'],\n",
       " ['compare', 'performance', 'algorithms'],\n",
       " ['quick', 'boosting', 'algorithms'],\n",
       " ['algorithm', 'takes', 'crown', 'light', 'gbm', 'vs', 'xgboost'],\n",
       " ['bagging', 'random', 'forest', 'ensemble', 'algorithms'],\n",
       " ['gentle', 'introduction', 'curves', 'diagnosing', 'model', 'performance'],\n",
       " ['introduction', 'text', 'summarization', 'using', 'textrank', 'algorithm'],\n",
       " ['build',\n",
       "  'high',\n",
       "  'performance',\n",
       "  'time',\n",
       "  'series',\n",
       "  'models',\n",
       "  'using',\n",
       "  'auto',\n",
       "  'arima'],\n",
       " ['three',\n",
       "  'way',\n",
       "  'race',\n",
       "  'future',\n",
       "  'ai',\n",
       "  'quantum',\n",
       "  'vs',\n",
       "  'neuromorphic',\n",
       "  'vs',\n",
       "  'high',\n",
       "  'performance',\n",
       "  'computing',\n",
       "  'data'],\n",
       " ['tour', 'recurrent', 'neural', 'network', 'algorithms'],\n",
       " ['ensemble',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'reduce',\n",
       "  'variance',\n",
       "  'improve',\n",
       "  'performance'],\n",
       " ['sensitivity', 'analysis', 'history', 'size', 'forecast', 'skill', 'arima'],\n",
       " ['evaluate', 'skill', 'models'],\n",
       " ['teacher', 'forcing', 'recurrent', 'neural', 'networks'],\n",
       " ['simple', 'framework', 'crack', 'kaggle', 'problem', 'statement'],\n",
       " ['mining',\n",
       "  'frequent',\n",
       "  'items',\n",
       "  'bought',\n",
       "  'together',\n",
       "  'using',\n",
       "  'apriori',\n",
       "  'algorithm'],\n",
       " ['questions', 'teach', 'multiple', 'regression'],\n",
       " ['using', 'measure', 'job', 'skill', 'similarities', 'data'],\n",
       " ['linear',\n",
       "  'probabilistic',\n",
       "  'approaches',\n",
       "  'time',\n",
       "  'series',\n",
       "  'analysis',\n",
       "  'data'],\n",
       " ['market', 'basket', 'analysis', 'association', 'rule'],\n",
       " ['comparison', 'top', 'speech', 'processing', 'apis', 'data'],\n",
       " ['building', 'blocks', 'decision', 'tree', 'data'],\n",
       " ['quick', 'dirty', 'data', 'analysis', 'pandas'],\n",
       " ['linear', 'discriminant', 'analysis'],\n",
       " ['visualizing', 'market', 'basket', 'analysis'],\n",
       " ['close', 'relationship', 'applied', 'statistics'],\n",
       " ['train', 'xgboost', 'models', 'cloud', 'amazon', 'web', 'services'],\n",
       " ['beginners', 'web', 'scraping', 'using', 'beautifulsoup'],\n",
       " ['beautiful', 'duality', 'tda', 'topological', 'data', 'analysis', 'data'],\n",
       " ['command', 'line', 'recipes', 'amazon', 'web', 'services'],\n",
       " ['semantic', 'data', 'modeling', 'fun', 'profit', 'data'],\n",
       " ['effective', 'cross', 'selling', 'using', 'market', 'basket', 'analysis'],\n",
       " ['building', 'model', 'process', 'optimisation', 'data'],\n",
       " ['hood', 'chatbots', 'data'],\n",
       " ['applying', 'noise', 'reduction', 'stock', 'market', 'data', 'data'],\n",
       " ['hands', 'introduction', 'time', 'series', 'classification'],\n",
       " ['multivariate', 'time', 'series', 'forecasting', 'lstms', 'keras'],\n",
       " ['first', 'project', 'step', 'step'],\n",
       " ['develop', 'first', 'neural', 'network', 'keras', 'step', 'step'],\n",
       " ['time',\n",
       "  'series',\n",
       "  'forecasting',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'network'],\n",
       " ['practices', 'enhance', 'performance', 'text', 'classification', 'model'],\n",
       " ['implement', 'baseline', 'algorithms', 'scratch'],\n",
       " ['implement', 'algorithm'],\n",
       " ['distributed',\n",
       "  'word',\n",
       "  'representations',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'implementation',\n",
       "  'scratch',\n",
       "  'octave',\n",
       "  'data'],\n",
       " ['understand',\n",
       "  'algorithms',\n",
       "  'implementing',\n",
       "  'scratch',\n",
       "  'tactics',\n",
       "  'get',\n",
       "  'around',\n",
       "  'bad'],\n",
       " ['algorithms', 'work', 'mapping', 'input', 'output'],\n",
       " ['using', 'power', 'cyber', 'security'],\n",
       " ['data', 'preparation', 'variable', 'length', 'input', 'sequences'],\n",
       " ['implement', 'algorithm', 'performance', 'metrics', 'scratch'],\n",
       " ['implement', 'resampling', 'scratch'],\n",
       " ['implement', 'bagging', 'scratch'],\n",
       " ['simple', 'time', 'series', 'forecasting', 'models', 'dont', 'fool'],\n",
       " ['calculate', 'principal', 'component', 'analysis', 'pca', 'scratch'],\n",
       " ['resume',\n",
       "  'making',\n",
       "  'tricky',\n",
       "  'candidate',\n",
       "  'many',\n",
       "  'dilemmas',\n",
       "  'whether',\n",
       "  'state',\n",
       "  'project',\n",
       "  'length',\n",
       "  'mention',\n",
       "  'bare',\n",
       "  'minimum',\n",
       "  'whether',\n",
       "  'mention',\n",
       "  'many',\n",
       "  'ski'],\n",
       " ['describes',\n",
       "  'theory',\n",
       "  'application',\n",
       "  'support',\n",
       "  'vector',\n",
       "  'machines',\n",
       "  'svm',\n",
       "  'popular',\n",
       "  'supervised',\n",
       "  'algorithm',\n",
       "  'class'],\n",
       " ['trend',\n",
       "  'seasonality',\n",
       "  'accounted',\n",
       "  'linear',\n",
       "  'model',\n",
       "  'including',\n",
       "  'sinusoidal',\n",
       "  'components',\n",
       "  'given',\n",
       "  'frequency',\n",
       "  'however',\n",
       "  'finding',\n",
       "  'appropriate'],\n",
       " ['introduction',\n",
       "  'know',\n",
       "  'write',\n",
       "  'within',\n",
       "  'sql',\n",
       "  'statements',\n",
       "  'services',\n",
       "  'sqlserver',\n",
       "  'eliminates',\n",
       "  'need'],\n",
       " ['describes',\n",
       "  'basics',\n",
       "  'logistic',\n",
       "  'regression',\n",
       "  'mathematics',\n",
       "  'behind',\n",
       "  'logistic',\n",
       "  'regression',\n",
       "  'finally',\n",
       "  'implementation'],\n",
       " ['semi',\n",
       "  'supervised',\n",
       "  'classification',\n",
       "  'algorithm',\n",
       "  'implementation',\n",
       "  'described',\n",
       "  'using',\n",
       "  'markov',\n",
       "  'chains',\n",
       "  'random',\n",
       "  'walks',\n",
       "  'following',\n",
       "  'cir'],\n",
       " ['talks',\n",
       "  'catboost',\n",
       "  'categorical',\n",
       "  'boosting',\n",
       "  'library',\n",
       "  'yandex',\n",
       "  'handles',\n",
       "  'categorial',\n",
       "  'data',\n",
       "  'automatically',\n",
       "  'provides',\n",
       "  'state',\n",
       "  'art',\n",
       "  'results'],\n",
       " ['support',\n",
       "  'vector',\n",
       "  'machines',\n",
       "  'svms',\n",
       "  'supervised',\n",
       "  'models',\n",
       "  'associated',\n",
       "  'algorithms',\n",
       "  'analyze',\n",
       "  'data',\n",
       "  'recognize',\n",
       "  'patterns',\n",
       "  'used',\n",
       "  'classific'],\n",
       " ['written',\n",
       "  'devin',\n",
       "  'soni',\n",
       "  'markov',\n",
       "  'chains',\n",
       "  'work',\n",
       "  'markov',\n",
       "  'chains',\n",
       "  'fairly',\n",
       "  'common',\n",
       "  'relatively',\n",
       "  'simpl'],\n",
       " ['classification',\n",
       "  'classification',\n",
       "  'process',\n",
       "  'assigning',\n",
       "  'every',\n",
       "  'object',\n",
       "  'collection',\n",
       "  'exactly',\n",
       "  'one',\n",
       "  'class',\n",
       "  'known',\n",
       "  'set',\n",
       "  'classes',\n",
       "  'examples'],\n",
       " ['logistic',\n",
       "  'regression',\n",
       "  'one',\n",
       "  'powerful',\n",
       "  'classification',\n",
       "  'within',\n",
       "  'used',\n",
       "  'wide',\n",
       "  'variety',\n",
       "  'tasks',\n",
       "  'think',\n",
       "  'pre',\n",
       "  'polici'],\n",
       " ['explains',\n",
       "  'powerful',\n",
       "  'classification',\n",
       "  'algorithm',\n",
       "  'support',\n",
       "  'vector',\n",
       "  'svm',\n",
       "  'working',\n",
       "  'uses',\n",
       "  'support',\n",
       "  'vector',\n",
       "  'effective'],\n",
       " ['sql',\n",
       "  'envisioned',\n",
       "  'english',\n",
       "  'like',\n",
       "  'language',\n",
       "  'simple',\n",
       "  'sql',\n",
       "  'statements',\n",
       "  'read',\n",
       "  'like',\n",
       "  'english',\n",
       "  'sentences',\n",
       "  'sql',\n",
       "  'writes',\n",
       "  'statement',\n",
       "  'english',\n",
       "  'way',\n",
       "  'english',\n",
       "  'prepos'],\n",
       " ['distributed',\n",
       "  'stochastic',\n",
       "  'neighbor',\n",
       "  'embedding',\n",
       "  'sne',\n",
       "  'prize',\n",
       "  'winning',\n",
       "  'technique',\n",
       "  'dimensionality',\n",
       "  'reduction',\n",
       "  'particularly',\n",
       "  'well',\n",
       "  'suited',\n",
       "  'visual'],\n",
       " ['solid',\n",
       "  'introduction',\n",
       "  'statistical',\n",
       "  'testing',\n",
       "  'beginners',\n",
       "  'well',\n",
       "  'reference',\n",
       "  'practitioners',\n",
       "  'includes',\n",
       "  'numerous',\n",
       "  'examples',\n",
       "  'well'],\n",
       " ['distributed',\n",
       "  'stochastic',\n",
       "  'neighbor',\n",
       "  'embedding',\n",
       "  'sne',\n",
       "  'prize',\n",
       "  'winning',\n",
       "  'technique',\n",
       "  'dimensionality',\n",
       "  'reduction',\n",
       "  'particularly',\n",
       "  'well',\n",
       "  'suited',\n",
       "  'visual'],\n",
       " ['explains',\n",
       "  'investing',\n",
       "  'stocks',\n",
       "  'based',\n",
       "  'big',\n",
       "  'data',\n",
       "  'information',\n",
       "  'available',\n",
       "  'public',\n",
       "  'domain'],\n",
       " ['select',\n",
       "  'door',\n",
       "  'selected',\n",
       "  'door',\n",
       "  'number',\n",
       "  'monty',\n",
       "  'hall',\n",
       "  'opened',\n",
       "  'door',\n",
       "  'number',\n",
       "  'see',\n",
       "  'goat',\n",
       "  'silvio',\n",
       "  'santos',\n",
       "  'show',\n",
       "  'brazilian',\n",
       "  'tv',\n",
       "  'program'],\n",
       " ['abstract',\n",
       "  'paper',\n",
       "  'propose',\n",
       "  'hybrid',\n",
       "  'principal',\n",
       "  'component',\n",
       "  'analysis',\n",
       "  'hpca',\n",
       "  'extract',\n",
       "  'appearance',\n",
       "  'feature',\n",
       "  'face',\n",
       "  'inter',\n",
       "  'age',\n",
       "  'group',\n",
       "  'variation',\n",
       "  'based',\n",
       "  'cl'],\n",
       " ['introduction',\n",
       "  'post',\n",
       "  'explain',\n",
       "  'maths',\n",
       "  'simplified',\n",
       "  'manner',\n",
       "  'keep',\n",
       "  'explanation',\n",
       "  'simple',\n",
       "  'cover',\n",
       "  'workings',\n",
       "  'mlp',\n",
       "  'mode'],\n",
       " ['contents',\n",
       "  'introduction',\n",
       "  'four',\n",
       "  'pillars',\n",
       "  'success',\n",
       "  'analytics',\n",
       "  'analytics',\n",
       "  'sophistication',\n",
       "  'data',\n",
       "  'practice',\n",
       "  'ds',\n",
       "  'budai',\n",
       "  'methodology'],\n",
       " ['nd',\n",
       "  'part',\n",
       "  'series',\n",
       "  'read',\n",
       "  'first',\n",
       "  'part',\n",
       "  'logistic',\n",
       "  'regression',\n",
       "  'vs',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'vs',\n",
       "  'svm',\n",
       "  'part',\n",
       "  'part',\n",
       "  'discuss',\n",
       "  'choose',\n",
       "  'bet'],\n",
       " ['selecting',\n",
       "  'right',\n",
       "  'statistical',\n",
       "  'prove',\n",
       "  'daunting',\n",
       "  'task',\n",
       "  'anyone',\n",
       "  'infographic',\n",
       "  'presents',\n",
       "  'step',\n",
       "  'step',\n",
       "  'approach',\n",
       "  'selection',\n",
       "  'proce'],\n",
       " ['basic',\n",
       "  'quantitative',\n",
       "  'quality',\n",
       "  'indicators',\n",
       "  'last',\n",
       "  'part',\n",
       "  'introduced',\n",
       "  'basic',\n",
       "  'qualitative',\n",
       "  'model',\n",
       "  'quality',\n",
       "  'indicators',\n",
       "  'let',\n",
       "  'us',\n",
       "  'recall'],\n",
       " ['written',\n",
       "  'dallin',\n",
       "  'akagi',\n",
       "  'like',\n",
       "  'following',\n",
       "  'three',\n",
       "  'part',\n",
       "  'definition',\n",
       "  'baseline',\n",
       "  'collecti'],\n",
       " ['illegal',\n",
       "  'fishing',\n",
       "  'significant',\n",
       "  'economic',\n",
       "  'environmental',\n",
       "  'challenge',\n",
       "  'countries',\n",
       "  'around',\n",
       "  'world',\n",
       "  'fishing',\n",
       "  'catch',\n",
       "  'certain',\n",
       "  'parts',\n",
       "  'wor'],\n",
       " ['introduction',\n",
       "  'part',\n",
       "  'coding',\n",
       "  'basics',\n",
       "  'weekend',\n",
       "  'recommend',\n",
       "  'book',\n",
       "  'data',\n",
       "  'handbook',\n",
       "  'jake',\n",
       "  'vanderpl'],\n",
       " ['data',\n",
       "  'exploration',\n",
       "  'summarized',\n",
       "  'cheat',\n",
       "  'sheet',\n",
       "  'including',\n",
       "  'load',\n",
       "  'data',\n",
       "  'file',\n",
       "  'sort',\n",
       "  'data',\n",
       "  'transpose',\n",
       "  'table',\n",
       "  'similar',\n",
       "  'steps',\n",
       "  'using',\n",
       "  'numpy',\n",
       "  'pandas',\n",
       "  'matplotlib'],\n",
       " ['previous',\n",
       "  'parts',\n",
       "  'discussed',\n",
       "  'basic',\n",
       "  'notation',\n",
       "  'used',\n",
       "  'assessing',\n",
       "  'classification',\n",
       "  'models',\n",
       "  'quantitative',\n",
       "  'quality',\n",
       "  'indicators',\n",
       "  'confusion',\n",
       "  'matrix'],\n",
       " ['introduction',\n",
       "  'part',\n",
       "  'two',\n",
       "  'series',\n",
       "  'part',\n",
       "  'one',\n",
       "  'used',\n",
       "  'linear',\n",
       "  'regression',\n",
       "  'model',\n",
       "  'predict',\n",
       "  'prices',\n",
       "  'used',\n",
       "  'toyota',\n",
       "  'corollas'],\n",
       " ['beginners', 'reinforcement', 'implementation'],\n",
       " ['environment', 'time', 'series', 'forecasting'],\n",
       " ['make', 'baseline', 'predictions', 'time', 'series', 'forecasting'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'exponential',\n",
       "  'smoothing',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['taxonomy', 'time', 'series', 'forecasting', 'problems'],\n",
       " ['common', 'data', 'transforms', 'time', 'series', 'forecasting'],\n",
       " ['make', 'predictions', 'time', 'series', 'forecasting'],\n",
       " ['seasonal', 'persistence', 'forecasting'],\n",
       " ['top',\n",
       "  'pretrained',\n",
       "  'models',\n",
       "  'get',\n",
       "  'started',\n",
       "  'part',\n",
       "  'computer',\n",
       "  'vision'],\n",
       " ['getting', 'started'],\n",
       " ['get', 'started'],\n",
       " ['time', 'series', 'forecasting'],\n",
       " ['get', 'better', 'performance'],\n",
       " ['results', 'comparing', 'classical', 'time', 'series', 'forecasting'],\n",
       " ['must', 'read', 'introduction', 'sequence', 'modelling', 'cases'],\n",
       " ['time', 'series', 'datasets'],\n",
       " ['gentle', 'introduction', 'sarima', 'time', 'series', 'forecasting'],\n",
       " ['get', 'good', 'results', 'fast', 'time', 'series', 'forecasting'],\n",
       " ['develop', 'skillful', 'time', 'series', 'forecasting', 'model'],\n",
       " ['essentials', 'getting', 'know', 'capsulenets', 'codes'],\n",
       " ['multivariate', 'time', 'series', 'forecasting', 'modeling', 'codes'],\n",
       " ['time', 'series', 'forecasting', 'performance', 'measures'],\n",
       " ['arent', 'results', 'good', 'thought', 'youre', 'probably', 'overfitting'],\n",
       " ['seed', 'state', 'lstms', 'time', 'series', 'forecasting'],\n",
       " ['best', 'practices', 'document', 'classification'],\n",
       " ['difference', 'time', 'series', 'dataset'],\n",
       " ['reframe', 'time', 'series', 'forecasting', 'problem'],\n",
       " ['calculate', 'bootstrap', 'confidence', 'intervals', 'results'],\n",
       " ['today',\n",
       "  'digitization',\n",
       "  'everything',\n",
       "  'percent',\n",
       "  'data',\n",
       "  'created',\n",
       "  'unstructured',\n",
       "  'audio',\n",
       "  'video',\n",
       "  'social',\n",
       "  'footprints',\n",
       "  'data',\n",
       "  'generated',\n",
       "  'conve'],\n",
       " ['big',\n",
       "  'data',\n",
       "  'big',\n",
       "  'topic',\n",
       "  'years',\n",
       "  'going',\n",
       "  'grow',\n",
       "  'bigger',\n",
       "  'get',\n",
       "  'hands',\n",
       "  'sophisticated',\n",
       "  'forms',\n",
       "  'technology',\n",
       "  'new'],\n",
       " ['parallel',\n",
       "  'support',\n",
       "  'vector',\n",
       "  'machines',\n",
       "  'generalized',\n",
       "  'linear',\n",
       "  'models',\n",
       "  'adaptive',\n",
       "  'boosting',\n",
       "  'data'],\n",
       " ['quality',\n",
       "  'inspection',\n",
       "  'manufacturing',\n",
       "  'using',\n",
       "  'based',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  'data'],\n",
       " ['ways', 'deal', 'continuous', 'variables', 'predictive', 'modeling'],\n",
       " ['examples', 'statistical', 'project'],\n",
       " ['getting', 'started', 'audio', 'data', 'analysis', 'voice', 'using'],\n",
       " ['questions', 'data', 'scientist', 'support', 'vector', 'machines'],\n",
       " ['understanding', 'support', 'vector', 'algorithm', 'examples', 'along'],\n",
       " ['feature', 'selection', 'example', 'variable', 'selection'],\n",
       " ['hands', 'solution', 'age', 'detection', 'practice', 'problem'],\n",
       " ['support', 'vector', 'machines'],\n",
       " ['vector', 'quantization'],\n",
       " ['role', 'randomization', 'address', 'confounding', 'variables'],\n",
       " ['hybrid',\n",
       "  'content',\n",
       "  'based',\n",
       "  'collaborative',\n",
       "  'filtering',\n",
       "  'recommendations',\n",
       "  'ordinal',\n",
       "  'logistic',\n",
       "  'regression',\n",
       "  'recommendation',\n",
       "  'discrete',\n",
       "  'choice',\n",
       "  'data'],\n",
       " ['introductory', 'information', 'retrieval', 'using', 'knn', 'kdtree'],\n",
       " ['quick', 'build', 'recommendation', 'engine'],\n",
       " ['introduction', 'knn', 'nearest', 'neighbors', 'simplified'],\n",
       " ['build', 'portfolio'],\n",
       " ['build', 'image', 'classification', 'model', 'minutes'],\n",
       " ['hands', 'automated', 'feature', 'engineering', 'using', 'featuretools'],\n",
       " ['questions', 'data', 'scientist', 'nearest', 'neighbors', 'knn'],\n",
       " ['correlation', 'understand', 'relationship', 'variables'],\n",
       " ['nearest', 'neighbors'],\n",
       " ['moving',\n",
       "  'average',\n",
       "  'smoothing',\n",
       "  'data',\n",
       "  'preparation',\n",
       "  'feature',\n",
       "  'engineering',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['calculate', 'mcnemars', 'compare', 'two', 'classifiers'],\n",
       " ['introduction', 'nearest', 'neighbor', 'regression'],\n",
       " ['moved', 'median', 'data'],\n",
       " ['nlp',\n",
       "  'approach',\n",
       "  'mining',\n",
       "  'online',\n",
       "  'reviews',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'modeling'],\n",
       " ['variable', 'reduction', 'work', 'data'],\n",
       " ['building', 'faq', 'chatbot', 'future', 'information', 'searching'],\n",
       " ['variable', 'reduction', 'art', 'well', 'data'],\n",
       " ['give',\n",
       "  'introduction',\n",
       "  'simple',\n",
       "  'markov',\n",
       "  'chain',\n",
       "  'using',\n",
       "  'business',\n",
       "  'case'],\n",
       " ['cheatsheets',\n",
       "  'scikit',\n",
       "  'caret',\n",
       "  'package',\n",
       "  'gain',\n",
       "  'expertise',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'data',\n",
       "  'processing',\n",
       "  'predictive',\n",
       "  'modeling',\n",
       "  'cheatsheet'],\n",
       " ['dozens',\n",
       "  'different',\n",
       "  'hypothesis',\n",
       "  'tests',\n",
       "  'choosing',\n",
       "  'one',\n",
       "  'little',\n",
       "  'overwhelming',\n",
       "  'good',\n",
       "  'news',\n",
       "  'one',\n",
       "  'popular',\n",
       "  'tests',\n",
       "  'usually'],\n",
       " ['levels', 'competence'],\n",
       " ['gentle', 'introduction', 'dimensional', 'arrays', 'numpy'],\n",
       " ['solving',\n",
       "  'multi',\n",
       "  'label',\n",
       "  'classification',\n",
       "  'problems',\n",
       "  'case',\n",
       "  'studies',\n",
       "  'included'],\n",
       " ['xgboost', 'algorithm', 'easy', 'steps'],\n",
       " ['gentle', 'introduction', 'broadcasting', 'numpy', 'arrays'],\n",
       " ['multivariate',\n",
       "  'regression',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'unique',\n",
       "  'exact',\n",
       "  'generic',\n",
       "  'models',\n",
       "  'data'],\n",
       " ['standard',\n",
       "  'multivariate',\n",
       "  'multi',\n",
       "  'step',\n",
       "  'multi',\n",
       "  'site',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting',\n",
       "  'problem'],\n",
       " ['define', 'problem'],\n",
       " ['fifa', 'wc', 'winner', 'predicted', 'using', 'twitter', 'feed'],\n",
       " ['reasons', 'linear', 'algebra'],\n",
       " ['sentiment', 'analysis', 'twitter', 'posts', 'chennai', 'floods', 'using'],\n",
       " ['perform', 'data', 'exploration', 'using', 'elastic', 'search', 'kibana'],\n",
       " ['step',\n",
       "  'step',\n",
       "  'introduction',\n",
       "  'basic',\n",
       "  'object',\n",
       "  'detection',\n",
       "  'algorithms',\n",
       "  'part'],\n",
       " ['steps', 'thinking', 'like', 'designer'],\n",
       " ['importance', 'segmentation', 'create', 'one'],\n",
       " ['develop', 'photo', 'caption', 'generator', 'scratch'],\n",
       " ['time',\n",
       "  'series',\n",
       "  'prediction',\n",
       "  'lstm',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'keras'],\n",
       " ['comparison',\n",
       "  'performed',\n",
       "  'data',\n",
       "  'set',\n",
       "  'linear',\n",
       "  'regression',\n",
       "  'works',\n",
       "  'well',\n",
       "  'salary',\n",
       "  'offered',\n",
       "  'candidate',\n",
       "  'based',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'requirements'],\n",
       " ['computer',\n",
       "  'vision',\n",
       "  'nowadays',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  'cv',\n",
       "  'one',\n",
       "  'widely',\n",
       "  'used',\n",
       "  'dimensions',\n",
       "  'main',\n",
       "  'task',\n",
       "  'computer',\n",
       "  'vision'],\n",
       " ['popular',\n",
       "  'image',\n",
       "  'processing',\n",
       "  'problems',\n",
       "  'along',\n",
       "  'solutions',\n",
       "  'going',\n",
       "  'discussed',\n",
       "  'image',\n",
       "  'processing',\n",
       "  'libraries',\n",
       "  'going'],\n",
       " ['going',\n",
       "  'deeper',\n",
       "  'regression',\n",
       "  'analysis',\n",
       "  'assumptions',\n",
       "  'plots',\n",
       "  'amp',\n",
       "  'solutions',\n",
       "  'posted',\n",
       "  'manish',\n",
       "  'saraswat',\n",
       "  'manish',\n",
       "  'works',\n",
       "  'marketing'],\n",
       " ['though',\n",
       "  'cnns',\n",
       "  'mostly',\n",
       "  'used',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  'tasks',\n",
       "  'nothing',\n",
       "  'stops',\n",
       "  'used',\n",
       "  'nlp',\n",
       "  'applications',\n",
       "  'one',\n",
       "  'application',\n",
       "  'cnns'],\n",
       " ['written',\n",
       "  'jim',\n",
       "  'frost',\n",
       "  'regression',\n",
       "  'analysis',\n",
       "  'curve',\n",
       "  'fitting',\n",
       "  'process',\n",
       "  'specifying',\n",
       "  'model',\n",
       "  'provides',\n",
       "  'best',\n",
       "  'fit',\n",
       "  'specific'],\n",
       " ['cross',\n",
       "  'validation',\n",
       "  'technique',\n",
       "  'used',\n",
       "  'assess',\n",
       "  'accuracy',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'based',\n",
       "  'training',\n",
       "  'set',\n",
       "  'data',\n",
       "  'splits',\n",
       "  'training',\n",
       "  'sets',\n",
       "  'contr'],\n",
       " ['marketing',\n",
       "  'analytics',\n",
       "  'software',\n",
       "  'includes',\n",
       "  'various',\n",
       "  'technologies',\n",
       "  'processes',\n",
       "  'enable',\n",
       "  'organization',\n",
       "  'evaluate',\n",
       "  'quantify',\n",
       "  'marketing',\n",
       "  'efforts',\n",
       "  'marketing'],\n",
       " ['summary',\n",
       "  'performance',\n",
       "  'comparison',\n",
       "  'popular',\n",
       "  'frameworks',\n",
       "  'supported',\n",
       "  'keras',\n",
       "  'tensorflow',\n",
       "  'cntk',\n",
       "  'mxnet',\n",
       "  'theano',\n",
       "  'doubts'],\n",
       " ['time',\n",
       "  'series',\n",
       "  'classification',\n",
       "  'regression',\n",
       "  'forecasting',\n",
       "  'involves',\n",
       "  'prediction',\n",
       "  'given',\n",
       "  'information',\n",
       "  'available',\n",
       "  'till',\n",
       "  'time',\n",
       "  'obviously'],\n",
       " ['data',\n",
       "  'center',\n",
       "  'infrastructure',\n",
       "  'management',\n",
       "  'dcim',\n",
       "  'solutions',\n",
       "  'effective',\n",
       "  'way',\n",
       "  'reduce',\n",
       "  'data',\n",
       "  'center',\n",
       "  'operating',\n",
       "  'costs',\n",
       "  'dcim',\n",
       "  'solutions',\n",
       "  'provide',\n",
       "  'significant',\n",
       "  'val'],\n",
       " ['ggplot',\n",
       "  'ggplot',\n",
       "  'plotting',\n",
       "  'system',\n",
       "  'based',\n",
       "  'rs',\n",
       "  'ggplot',\n",
       "  'grammar',\n",
       "  'graphics',\n",
       "  'built',\n",
       "  'making',\n",
       "  'profressional',\n",
       "  'looking',\n",
       "  'plots',\n",
       "  'quickly',\n",
       "  'wi'],\n",
       " ['guest',\n",
       "  'jim',\n",
       "  'frost',\n",
       "  'regression',\n",
       "  'analysis',\n",
       "  'mathematically',\n",
       "  'describes',\n",
       "  'relationship',\n",
       "  'set',\n",
       "  'independent',\n",
       "  'variables',\n",
       "  'dependent',\n",
       "  'variable'],\n",
       " ['best',\n",
       "  'subset',\n",
       "  'regression',\n",
       "  'used',\n",
       "  'create',\n",
       "  'best',\n",
       "  'fitting',\n",
       "  'regression',\n",
       "  'model',\n",
       "  'technique',\n",
       "  'model',\n",
       "  'building',\n",
       "  'helps',\n",
       "  'identify',\n",
       "  'predictor',\n",
       "  'indepe'],\n",
       " ['regression',\n",
       "  'regression',\n",
       "  'types',\n",
       "  'regression',\n",
       "  'cover',\n",
       "  'detailed',\n",
       "  'overview',\n",
       "  'types',\n",
       "  'regression'],\n",
       " ['abstract',\n",
       "  'tree',\n",
       "  'boosting',\n",
       "  'highly',\n",
       "  'effective',\n",
       "  'widely',\n",
       "  'used',\n",
       "  'paper',\n",
       "  'describe',\n",
       "  'scalable',\n",
       "  'end',\n",
       "  'end',\n",
       "  'tree',\n",
       "  'boosting',\n",
       "  'system',\n",
       "  'ca'],\n",
       " ['primer',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'models',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing'],\n",
       " ['prepare', 'analytics', 'interview'],\n",
       " ['overview', 'regularization', 'techniques'],\n",
       " ['must', 'read', 'nlp', 'neural', 'translation', 'powering', 'google'],\n",
       " ['promise', 'natural', 'language', 'processing'],\n",
       " ['choose', 'right', 'options', 'evaluating', 'algorithms'],\n",
       " ['natural', 'language', 'processing', 'beginners', 'using', 'textblob'],\n",
       " ['natural', 'language', 'processing'],\n",
       " ['implement',\n",
       "  'beam',\n",
       "  'search',\n",
       "  'decoder',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing'],\n",
       " ['load', 'explore', 'time', 'series', 'data'],\n",
       " ['basics', 'image', 'processing', 'business', 'analytics'],\n",
       " ['introduction', 'flair', 'nlp', 'state', 'art', 'library', 'nlp'],\n",
       " ['datasets', 'natural', 'language', 'processing'],\n",
       " ['intro',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'framing',\n",
       "  'text',\n",
       "  'classification',\n",
       "  'familiar',\n",
       "  'terms',\n",
       "  'data'],\n",
       " ['natural', 'language', 'processing', 'nlp', 'data'],\n",
       " ['intro',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'framing',\n",
       "  'text',\n",
       "  'classification',\n",
       "  'familiar',\n",
       "  'terms',\n",
       "  'data'],\n",
       " ['automate', 'workflows', 'pipelines', 'scikit'],\n",
       " ['understanding', 'basics', 'recommendation', 'engines', 'case', 'study'],\n",
       " ['extracting', 'right', 'variables', 'regression', 'model'],\n",
       " ['generate', 'random', 'numbers'],\n",
       " ['sequence', 'prediction', 'using', 'compact', 'prediction', 'tree', 'codes'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'models',\n",
       "  'sequence',\n",
       "  'prediction',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['nlp', 'text', 'classification', 'using', 'conditional', 'random', 'fields'],\n",
       " ['gentle', 'introduction', 'law', 'large', 'numbers'],\n",
       " ['online', 'text', 'classification', 'using', 'vowpal', 'wabbit', 'vw'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'random',\n",
       "  'walk',\n",
       "  'times',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['written',\n",
       "  'koustubh',\n",
       "  'unless',\n",
       "  'living',\n",
       "  'rock',\n",
       "  'must',\n",
       "  'heard',\n",
       "  'revolution',\n",
       "  'convolutional'],\n",
       " ['frequently',\n",
       "  'get',\n",
       "  'questions',\n",
       "  'whether',\n",
       "  'chosen',\n",
       "  'right',\n",
       "  'parameters',\n",
       "  'build',\n",
       "  'model',\n",
       "  'two',\n",
       "  'scenarios',\n",
       "  'either',\n",
       "  'su'],\n",
       " ['sound',\n",
       "  'familiar',\n",
       "  'order',\n",
       "  'get',\n",
       "  'idea',\n",
       "  'choose',\n",
       "  'parameter',\n",
       "  'given',\n",
       "  'classifier',\n",
       "  'cross',\n",
       "  'reference',\n",
       "  'number',\n",
       "  'papers'],\n",
       " ['work',\n",
       "  'done',\n",
       "  'jatinder',\n",
       "  'singh',\n",
       "  'co',\n",
       "  'authored',\n",
       "  'iresh',\n",
       "  'mishra',\n",
       "  'thanks',\n",
       "  'saurabh',\n",
       "  'singh',\n",
       "  'guidance',\n",
       "  'past',\n",
       "  'written',\n",
       "  'abo'],\n",
       " ['got',\n",
       "  'exposed',\n",
       "  'different',\n",
       "  'sounds',\n",
       "  'every',\n",
       "  'day',\n",
       "  'like',\n",
       "  'sound',\n",
       "  'car',\n",
       "  'horns',\n",
       "  'siren',\n",
       "  'music',\n",
       "  'etc',\n",
       "  'teaching',\n",
       "  'computer',\n",
       "  'classify',\n",
       "  'sounds',\n",
       "  'automa'],\n",
       " ['written',\n",
       "  'lukasz',\n",
       "  'ciesla',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'get',\n",
       "  'impression',\n",
       "  'slogans',\n",
       "  'attack',\n",
       "  'us'],\n",
       " ['candlestick',\n",
       "  'chart',\n",
       "  'sometimes',\n",
       "  'used',\n",
       "  'stock',\n",
       "  'market',\n",
       "  'technicians',\n",
       "  'make',\n",
       "  'trading',\n",
       "  'decisions',\n",
       "  'candlestick',\n",
       "  'graphically',\n",
       "  'depicts',\n",
       "  'following',\n",
       "  'prices',\n",
       "  'simulta'],\n",
       " ['check',\n",
       "  'post',\n",
       "  'using',\n",
       "  'plotly',\n",
       "  'bubble',\n",
       "  'charts',\n",
       "  'display',\n",
       "  'three',\n",
       "  'even',\n",
       "  'four',\n",
       "  'dimensions',\n",
       "  'data',\n",
       "  'http',\n",
       "  'plot',\n",
       "  'ly',\n",
       "  'post',\n",
       "  'power',\n",
       "  'bubble',\n",
       "  'ch'],\n",
       " ['weve',\n",
       "  'made',\n",
       "  'new',\n",
       "  'interactive',\n",
       "  'visualization',\n",
       "  'data',\n",
       "  'patterns',\n",
       "  'original',\n",
       "  'infographics',\n",
       "  'allows',\n",
       "  'zoom',\n",
       "  'find',\n",
       "  'various',\n",
       "  'patterns',\n",
       "  'see',\n",
       "  'snapsh'],\n",
       " ['comes',\n",
       "  'storytelling',\n",
       "  'problem',\n",
       "  'fault',\n",
       "  'though',\n",
       "  'human',\n",
       "  'beings',\n",
       "  'hard',\n",
       "  'wired',\n",
       "  'birth',\n",
       "  'look',\n",
       "  'patterns',\n",
       "  'explain'],\n",
       " ['oh',\n",
       "  'poor',\n",
       "  'maligned',\n",
       "  'pie',\n",
       "  'chart',\n",
       "  'chart',\n",
       "  'type',\n",
       "  'gets',\n",
       "  'pushed',\n",
       "  'around',\n",
       "  'bullied',\n",
       "  'data',\n",
       "  'viz',\n",
       "  'playground',\n",
       "  'randal',\n",
       "  'olsen',\n",
       "  'dataisb'],\n",
       " ['outliers',\n",
       "  'patterns',\n",
       "  'data',\n",
       "  'confirm',\n",
       "  'expected',\n",
       "  'behavior',\n",
       "  'detecting',\n",
       "  'patterns',\n",
       "  'prime',\n",
       "  'importance',\n",
       "  'credit',\n",
       "  'card',\n",
       "  'fraud',\n",
       "  'stock'],\n",
       " ['models', 'human', 'activity', 'recognition'],\n",
       " ['gentle', 'introduction', 'adam', 'optimization', 'algorithm'],\n",
       " ['project', 'imbalanced', 'data', 'add', 'value', 'resume'],\n",
       " ['essentials', 'introduction', 'unsupervised', 'codes'],\n",
       " ['tour', 'algorithms'],\n",
       " ['gentle', 'introduction', 'lstm', 'autoencoders'],\n",
       " ['gentle', 'introduction', 'calculating', 'normal', 'summary', 'statistics'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'standard',\n",
       "  'human',\n",
       "  'activity',\n",
       "  'recognition',\n",
       "  'problem'],\n",
       " ['one', 'hot', 'encode', 'sequence', 'data'],\n",
       " ['develop',\n",
       "  'rnn',\n",
       "  'models',\n",
       "  'human',\n",
       "  'activity',\n",
       "  'recognition',\n",
       "  'time',\n",
       "  'series',\n",
       "  'classification'],\n",
       " ['one', 'hot', 'encode', 'data'],\n",
       " ['gentle', 'introduction', 'normality', 'tests'],\n",
       " ['work', 'regression', 'project', 'weka', 'step', 'step'],\n",
       " ['weka'],\n",
       " ['classification', 'regression', 'trees'],\n",
       " ['overfitting', 'underfitting', 'algorithms'],\n",
       " ['evaluate', 'algorithms', 'human', 'activity', 'recognition'],\n",
       " ['gentle', 'introduction', 'singular', 'value', 'decomposition'],\n",
       " ['sne', 'algorithm', 'implementation'],\n",
       " ['detailed', 'solutions', 'skilltest', 'tree', 'based', 'algorithms'],\n",
       " ['questions', 'statisitics', 'data', 'scientists', 'analysts'],\n",
       " ['questions', 'data', 'scientist', 'regression', 'skill'],\n",
       " ['beginners', 'linear', 'algebra', 'data', 'scientists'],\n",
       " ['questions', 'data', 'scientist', 'solutions'],\n",
       " ['performance',\n",
       "  'testing',\n",
       "  'types',\n",
       "  'process',\n",
       "  'important',\n",
       "  'metrics',\n",
       "  'data'],\n",
       " ['open', 'datasets', 'every', 'data', 'scientist', 'must', 'work'],\n",
       " ['lotteries', 'data'],\n",
       " ['ultimate',\n",
       "  'deal',\n",
       "  'text',\n",
       "  'data',\n",
       "  'using',\n",
       "  'data',\n",
       "  'scientists',\n",
       "  'engineers'],\n",
       " ['web', 'scraping', 'new', 'business', 'data'],\n",
       " ['introduction', 'data'],\n",
       " ['satellite', 'imagery', 'data'],\n",
       " ['simple', 'introduction', 'topic', 'modeling'],\n",
       " ['free',\n",
       "  'exploratory',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'tools',\n",
       "  'people',\n",
       "  'dont',\n",
       "  'well'],\n",
       " ['ultimate', 'path', 'become', 'data', 'scientist'],\n",
       " ['unsupervised', 'algorithms', 'computer', 'vision'],\n",
       " ['text', 'mining', 'sentiment', 'analysis', 'primer', 'data'],\n",
       " ['generalized', 'stochastic', 'calculus', 'data'],\n",
       " ['example', 'bad', 'data', 'hypothesis', 'data'],\n",
       " ['math', 'behind', 'data'],\n",
       " ['watson', 'time', 'prune', 'ml', 'tree', 'data'],\n",
       " ['beginners', 'dimension', 'reduction', 'techniques'],\n",
       " ['would', 'survive', 'titanic', 'data'],\n",
       " ['death', 'statistical', 'tests', 'hypotheses', 'data'],\n",
       " ['questions', 'true', 'data', 'knowledge', 'data'],\n",
       " ['automatically', 'determine', 'number', 'clusters', 'data', 'data'],\n",
       " ['beginners', 'content', 'based', 'recommender', 'engine'],\n",
       " ['perform', 'clustering', 'analysis'],\n",
       " ['create', 'interactive', 'data', 'visualization', 'using', 'plotly'],\n",
       " ['linear', 'algebra'],\n",
       " ['gentle', 'introduction', 'data', 'visualization'],\n",
       " ['introduction', 'clustering', 'different', 'clustering'],\n",
       " ['probability', 'distributions', 'every', 'data', 'professional', 'know'],\n",
       " ['gentle', 'introduction', 'statistical', 'data', 'distributions'],\n",
       " ['questions', 'skill', 'data'],\n",
       " ['upcoming',\n",
       "  'trends',\n",
       "  'data',\n",
       "  'visualization',\n",
       "  'big',\n",
       "  'data',\n",
       "  'visualization',\n",
       "  'tools'],\n",
       " ['transform', 'data', 'better', 'fit', 'normal', 'distribution'],\n",
       " ['top', 'data'],\n",
       " ['scale', 'data', 'scratch'],\n",
       " ['mining', 'youtube', 'using', 'performing', 'social', 'media', 'analysis'],\n",
       " ['perform', 'feature', 'selection', 'pick', 'imp', 'variables', 'boruta'],\n",
       " ['essential', 'nlp', 'data', 'scientists', 'codes', 'top', 'nlp', 'tasks'],\n",
       " ['understanding', 'math', 'behind', 'xgboost', 'algorithm'],\n",
       " ['questions', 'data', 'scientist', 'linear', 'regression'],\n",
       " ['load', 'data', 'scratch'],\n",
       " ['questions', 'data', 'scientist', 'tree', 'based', 'models'],\n",
       " ['questions', 'data', 'scientist', 'image', 'processing'],\n",
       " ['ultimate', 'tips', 'tricks', 'data', 'visualization', 'qlikview'],\n",
       " ['art', 'story', 'telling', 'data', 'create', 'data', 'stories'],\n",
       " ['feature', 'selection', 'scikit'],\n",
       " ['data', 'preprocessing', 'using', 'scikit'],\n",
       " ['beginners', 'topic', 'modeling'],\n",
       " ['steps', 'effective', 'text', 'data', 'cleaning'],\n",
       " ['must',\n",
       "  'know',\n",
       "  'questions',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'dimensionality',\n",
       "  'reduction',\n",
       "  'techniques'],\n",
       " ['visualize', 'data', 'pandas'],\n",
       " ['clean', 'data', 'quickly', 'steps', 'data'],\n",
       " ['job', 'interview', 'questions', 'data', 'scientists', 'data'],\n",
       " ['bayesian', 'nonparametric', 'models', 'data'],\n",
       " ['neutralizing', 'outliers', 'dimension', 'data'],\n",
       " ['introduction', 'market', 'mix', 'modeling', 'data'],\n",
       " ['types', 'regression', 'data'],\n",
       " ['rescaling', 'data', 'scikit'],\n",
       " ['data',\n",
       "  'discussed',\n",
       "  'earlier',\n",
       "  'post',\n",
       "  'data',\n",
       "  'visualization',\n",
       "  'one',\n",
       "  'common',\n",
       "  'buzzwords',\n",
       "  'thrown',\n",
       "  'around',\n",
       "  'tech',\n",
       "  'business',\n",
       "  'communi'],\n",
       " ['data',\n",
       "  'classification',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'technique',\n",
       "  'used',\n",
       "  'sorting',\n",
       "  'data',\n",
       "  'understanding',\n",
       "  'data',\n",
       "  'performing',\n",
       "  'outcome',\n",
       "  'predictions',\n",
       "  'small',\n",
       "  'blo'],\n",
       " ['marketing',\n",
       "  'advertising',\n",
       "  'gaining',\n",
       "  'space',\n",
       "  'internet',\n",
       "  'big',\n",
       "  'data',\n",
       "  'analytics',\n",
       "  'playing',\n",
       "  'prominent',\n",
       "  'role',\n",
       "  'following',\n",
       "  'trends',\n",
       "  'market'],\n",
       " ['linear',\n",
       "  'logistic',\n",
       "  'regression',\n",
       "  'contexts',\n",
       "  'hundreds',\n",
       "  'types',\n",
       "  'regressions',\n",
       "  'overview',\n",
       "  'data',\n",
       "  'scientists'],\n",
       " ['digitization',\n",
       "  'changed',\n",
       "  'way',\n",
       "  'process',\n",
       "  'analyze',\n",
       "  'information',\n",
       "  'exponential',\n",
       "  'increase',\n",
       "  'online',\n",
       "  'availability',\n",
       "  'information',\n",
       "  'web',\n",
       "  'pages'],\n",
       " ['data',\n",
       "  'lot',\n",
       "  'people',\n",
       "  'sanitized',\n",
       "  'datasets',\n",
       "  'downloaded',\n",
       "  'somewhere',\n",
       "  'internet',\n",
       "  'data',\n",
       "  'provided',\n",
       "  'part',\n",
       "  'class'],\n",
       " ['sophisticated',\n",
       "  'modern',\n",
       "  'businesses',\n",
       "  'like',\n",
       "  'banks',\n",
       "  'insurers',\n",
       "  'data',\n",
       "  'rich',\n",
       "  'data',\n",
       "  'fundamental',\n",
       "  'business',\n",
       "  'effectiveness',\n",
       "  'efficiency',\n",
       "  'however',\n",
       "  'data'],\n",
       " ['ever',\n",
       "  'wondered',\n",
       "  'deal',\n",
       "  'behind',\n",
       "  'hype',\n",
       "  'big',\n",
       "  'data',\n",
       "  'well',\n",
       "  'data',\n",
       "  'hit',\n",
       "  'peak',\n",
       "  'popularity',\n",
       "  'graduates'],\n",
       " ['questions',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'clustering',\n",
       "  'techniques',\n",
       "  'skilltest',\n",
       "  'solution'],\n",
       " ['questions', 'data', 'scientist', 'solution', 'skillpower', 'datafest'],\n",
       " ['questions',\n",
       "  'ask',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'ensemble',\n",
       "  'modeling',\n",
       "  'techniques',\n",
       "  'skilltest',\n",
       "  'solution'],\n",
       " ['questions', 'data', 'scientist', 'along', 'solution'],\n",
       " ['questions', 'sql', 'aspiring', 'data', 'scientists'],\n",
       " ['confusion', 'matrix'],\n",
       " ['must',\n",
       "  'know',\n",
       "  'questions',\n",
       "  'base',\n",
       "  'sas',\n",
       "  'analysts',\n",
       "  'data',\n",
       "  'scientists',\n",
       "  'skilltest',\n",
       "  'solution'],\n",
       " ['inferential', 'statistics', 'data'],\n",
       " ['questions', 'sql', 'data', 'professional', 'skilltest', 'solution'],\n",
       " ['questions', 'skill', 'data'],\n",
       " ['questions',\n",
       "  'probability',\n",
       "  'data',\n",
       "  'solution',\n",
       "  'skillpower',\n",
       "  'probability',\n",
       "  'datafest'],\n",
       " ['develop', 'neural', 'networks', 'greedy', 'layer', 'wise', 'pretraining'],\n",
       " ['diagnose', 'overfitting', 'underfitting', 'lstm', 'models'],\n",
       " ['train', 'final', 'model'],\n",
       " ['implement', 'caret'],\n",
       " ['prepare', 'text', 'data', 'scikit'],\n",
       " ['metrics', 'evaluate', 'algorithms'],\n",
       " ['ultimate', 'land', 'first', 'data', 'internship'],\n",
       " ['develop',\n",
       "  'bag',\n",
       "  'words',\n",
       "  'model',\n",
       "  'predicting',\n",
       "  'movie',\n",
       "  'review',\n",
       "  'sentiment'],\n",
       " ['develop',\n",
       "  'gram',\n",
       "  'multichannel',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'sentiment',\n",
       "  'analysis'],\n",
       " ['text', 'generation', 'lstm', 'recurrent', 'neural', 'networks', 'keras'],\n",
       " ['handwritten',\n",
       "  'digit',\n",
       "  'recognition',\n",
       "  'using',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'keras'],\n",
       " ['develop',\n",
       "  'word',\n",
       "  'embedding',\n",
       "  'model',\n",
       "  'predicting',\n",
       "  'movie',\n",
       "  'review',\n",
       "  'sentiment'],\n",
       " ['small',\n",
       "  'experiments',\n",
       "  'develop',\n",
       "  'caption',\n",
       "  'generation',\n",
       "  'model',\n",
       "  'keras'],\n",
       " ['multi', 'class', 'classification', 'keras', 'library'],\n",
       " ['develop',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'models',\n",
       "  'human',\n",
       "  'activity',\n",
       "  'recognition'],\n",
       " ['object',\n",
       "  'recognition',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'keras',\n",
       "  'library'],\n",
       " ['understanding',\n",
       "  'stateful',\n",
       "  'lstm',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'keras'],\n",
       " ['create', 'arima', 'model', 'time', 'series', 'forecasting'],\n",
       " ['develop', 'reusable', 'framework', 'spot', 'check', 'algorithms'],\n",
       " ['develop',\n",
       "  'models',\n",
       "  'multivariate',\n",
       "  'multi',\n",
       "  'step',\n",
       "  'air',\n",
       "  'pollution',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['develop',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'multi',\n",
       "  'step',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['time',\n",
       "  'series',\n",
       "  'forecast',\n",
       "  'study',\n",
       "  'monthly',\n",
       "  'sales',\n",
       "  'french',\n",
       "  'champagne'],\n",
       " ['develop',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'models',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['first', 'project', 'step', 'step', 'template', 'future', 'projects'],\n",
       " ['develop', 'neural', 'translation', 'system', 'scratch'],\n",
       " ['develop',\n",
       "  'word',\n",
       "  'level',\n",
       "  'neural',\n",
       "  'language',\n",
       "  'model',\n",
       "  'generate',\n",
       "  'text'],\n",
       " ['word', 'embedding', 'layers', 'keras'],\n",
       " ['develop',\n",
       "  'autoregressive',\n",
       "  'forecasting',\n",
       "  'models',\n",
       "  'multi',\n",
       "  'step',\n",
       "  'air',\n",
       "  'pollution',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['develop', 'models', 'univariate', 'time', 'series', 'forecasting'],\n",
       " ['written',\n",
       "  'jason',\n",
       "  'brownlee',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'fascinating',\n",
       "  'area',\n",
       "  'study',\n",
       "  'although',\n",
       "  'intimidating',\n",
       "  'getting',\n",
       "  'sta'],\n",
       " ['leading',\n",
       "  'lagging',\n",
       "  'indicators',\n",
       "  'business',\n",
       "  'important',\n",
       "  'managers',\n",
       "  'understand',\n",
       "  'difference',\n",
       "  'ensure',\n",
       "  'types'],\n",
       " ['introduction',\n",
       "  'originally',\n",
       "  'posted',\n",
       "  'text',\n",
       "  'analysis',\n",
       "  'part',\n",
       "  'text',\n",
       "  'analysis',\n",
       "  'series',\n",
       "  'recently',\n",
       "  'added',\n",
       "  'feature',\n",
       "  'api',\n",
       "  'allo'],\n",
       " ['two',\n",
       "  'part',\n",
       "  'series',\n",
       "  'explore',\n",
       "  'text',\n",
       "  'clustering',\n",
       "  'get',\n",
       "  'insights',\n",
       "  'unstructured',\n",
       "  'data',\n",
       "  'quite',\n",
       "  'powerful',\n",
       "  'industrial',\n",
       "  'strength'],\n",
       " ['preface',\n",
       "  'previously',\n",
       "  'tackled',\n",
       "  'gamblers',\n",
       "  'ruin',\n",
       "  'problem',\n",
       "  'using',\n",
       "  'conditional',\n",
       "  'probability',\n",
       "  'difference',\n",
       "  'equations',\n",
       "  'well',\n",
       "  'visualising',\n",
       "  'simulations'],\n",
       " ['one',\n",
       "  'main',\n",
       "  'goals',\n",
       "  'bitcoin',\n",
       "  'analytics',\n",
       "  'price',\n",
       "  'forecasting',\n",
       "  'many',\n",
       "  'factors',\n",
       "  'influence',\n",
       "  'price',\n",
       "  'dynamics',\n",
       "  'important',\n",
       "  'factors'],\n",
       " ['nltk',\n",
       "  'nltk',\n",
       "  'leading',\n",
       "  'platform',\n",
       "  'building',\n",
       "  'programs',\n",
       "  'work',\n",
       "  'human',\n",
       "  'language',\n",
       "  'data',\n",
       "  'provides',\n",
       "  'easy',\n",
       "  'interfaces',\n",
       "  'lexical',\n",
       "  'resources'],\n",
       " ['value',\n",
       "  'discuss',\n",
       "  'important',\n",
       "  'functionality',\n",
       "  'value',\n",
       "  'statistical',\n",
       "  'experiments',\n",
       "  'value',\n",
       "  'deciding',\n",
       "  'factor',\n",
       "  'accepting'],\n",
       " ['covers',\n",
       "  'challenges',\n",
       "  'using',\n",
       "  'clustering',\n",
       "  'lays',\n",
       "  'steps',\n",
       "  'get',\n",
       "  'good',\n",
       "  'clusters',\n",
       "  'even',\n",
       "  'uniform',\n",
       "  'population'],\n",
       " ['written',\n",
       "  'saurav',\n",
       "  'kaushik',\n",
       "  'saurav',\n",
       "  'data',\n",
       "  'enthusiast',\n",
       "  'currently',\n",
       "  'final',\n",
       "  'year',\n",
       "  'graduation',\n",
       "  'mait',\n",
       "  'new',\n",
       "  'delhi',\n",
       "  'loves'],\n",
       " ['written',\n",
       "  'prashant',\n",
       "  'gupta',\n",
       "  'gupta',\n",
       "  'engineer',\n",
       "  'android',\n",
       "  'developer',\n",
       "  'tech',\n",
       "  'enthusiast',\n",
       "  'tired',\n",
       "  'getting',\n",
       "  'low',\n",
       "  'accuracy'],\n",
       " ['even',\n",
       "  'start',\n",
       "  'writing',\n",
       "  'cv',\n",
       "  'important',\n",
       "  'understand',\n",
       "  'objective',\n",
       "  'cv',\n",
       "  'actually',\n",
       "  'answer',\n",
       "  'secure',\n",
       "  'intervi'],\n",
       " ['maxim',\n",
       "  'lapan',\n",
       "  'author',\n",
       "  'reinforcement',\n",
       "  'hands',\n",
       "  'going',\n",
       "  'discuss',\n",
       "  'gradients',\n",
       "  'pytorch',\n",
       "  'gradients',\n",
       "  'support',\n",
       "  'te'],\n",
       " ['regularization',\n",
       "  'way',\n",
       "  'avoid',\n",
       "  'fitting',\n",
       "  'regression',\n",
       "  'models',\n",
       "  'explains',\n",
       "  'business',\n",
       "  'situation',\n",
       "  'avoid',\n",
       "  'overfitting',\n",
       "  'underfitting',\n",
       "  'regularization'],\n",
       " ['written',\n",
       "  'tirthajyoti',\n",
       "  'sarkar',\n",
       "  'summary',\n",
       "  'full',\n",
       "  'accessible',\n",
       "  'link',\n",
       "  'bottom',\n",
       "  'features',\n",
       "  'courses',\n",
       "  'could'],\n",
       " ['basics', 'mathematical', 'notation'],\n",
       " ['confidence', 'intervals'],\n",
       " ['introduction', 'matrices', 'matrix', 'arithmetic'],\n",
       " ['understand',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecast',\n",
       "  'uncertainty',\n",
       "  'using',\n",
       "  'confidence',\n",
       "  'intervals'],\n",
       " ['examples', 'linear', 'algebra'],\n",
       " ['types', 'regression', 'techniques', 'know'],\n",
       " ['must', 'know', 'terms', 'concepts', 'beginners'],\n",
       " ['gentle', 'introduction', 'statistical', 'tolerance', 'intervals'],\n",
       " ['gentle', 'introduction', 'matrix', 'factorization'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'weight',\n",
       "  'constraints',\n",
       "  'reduce',\n",
       "  'generalization',\n",
       "  'error'],\n",
       " ['basics', 'newbie', 'applications'],\n",
       " ['prediction', 'intervals'],\n",
       " ['gentle', 'introduction', 'matrix', 'operations'],\n",
       " ['basics', 'ensemble', 'explained', 'simple', 'english'],\n",
       " ['document', 'similarity', 'analysis', 'using', 'elasticsearch', 'data'],\n",
       " ['introduction', 'matrix', 'types', 'linear', 'algebra'],\n",
       " ['computational', 'linear', 'algebra', 'coders', 'review'],\n",
       " ['important', 'model', 'evaluation', 'error', 'metrics', 'everyone', 'know'],\n",
       " ['excel', 'users', 'data'],\n",
       " ['nice',\n",
       "  'generalization',\n",
       "  'nn',\n",
       "  'clustering',\n",
       "  'algorithm',\n",
       "  'useful',\n",
       "  'data',\n",
       "  'reduction',\n",
       "  'data'],\n",
       " ['pattern',\n",
       "  'recognition',\n",
       "  'nearest',\n",
       "  'neighbor',\n",
       "  'algorithm',\n",
       "  'knn',\n",
       "  'classifying',\n",
       "  'objects',\n",
       "  'based',\n",
       "  'closest',\n",
       "  'training',\n",
       "  'examples',\n",
       "  'feature',\n",
       "  'space'],\n",
       " ['going',\n",
       "  'nearest',\n",
       "  'neighbors',\n",
       "  'knn',\n",
       "  'algorithm',\n",
       "  'solve',\n",
       "  'classification',\n",
       "  'problem',\n",
       "  'firstly',\n",
       "  'exactly',\n",
       "  'mean',\n",
       "  'classifica'],\n",
       " ['word', 'vec', 'algorithm', 'data'],\n",
       " ['word', 'embeddings', 'text'],\n",
       " ['tensorflow', 'keras', 'data'],\n",
       " ['best', 'tune', 'multithreading', 'support', 'xgboost'],\n",
       " ['analytical', 'vs', 'numerical', 'solutions'],\n",
       " ['introduction', 'library', 'tensorflow'],\n",
       " ['text', 'classification', 'sentiment', 'analysis', 'data'],\n",
       " ['world', 'cup', 'predictions', 'using', 'decision', 'trees', 'data'],\n",
       " ['text',\n",
       "  'classification',\n",
       "  'nlp',\n",
       "  'using',\n",
       "  'ulmfit',\n",
       "  'fastai',\n",
       "  'library',\n",
       "  'analytics'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'mini',\n",
       "  'batch',\n",
       "  'gradient',\n",
       "  'descent',\n",
       "  'configure',\n",
       "  'batch',\n",
       "  'size'],\n",
       " ['world', 'cup', 'predictions', 'using', 'decision', 'trees', 'data'],\n",
       " ['step', 'mini', 'course', 'get', 'started', 'xgboost'],\n",
       " ['flashtext', 'library', 'faster', 'regular', 'expressions', 'nlp', 'tasks'],\n",
       " ['measuring',\n",
       "  'audience',\n",
       "  'sentiments',\n",
       "  'movies',\n",
       "  'using',\n",
       "  'twitter',\n",
       "  'text',\n",
       "  'analytics'],\n",
       " ['building', 'mask', 'cnn', 'model', 'detecting', 'car', 'damage', 'codes'],\n",
       " ['beginners', 'regular', 'expressions'],\n",
       " ['word',\n",
       "  'representations',\n",
       "  'text',\n",
       "  'classification',\n",
       "  'using',\n",
       "  'fasttext',\n",
       "  'facebook',\n",
       "  'lib'],\n",
       " ['sentiment',\n",
       "  'analysis',\n",
       "  'movie',\n",
       "  'reviews',\n",
       "  'bag',\n",
       "  'words',\n",
       "  'models',\n",
       "  'data'],\n",
       " ['evaluate', 'gradient', 'boosting', 'models', 'xgboost'],\n",
       " ['optimization', 'techniques', 'finding', 'maxima', 'minima', 'data'],\n",
       " ['tune', 'rate', 'gradient', 'boosting', 'xgboost'],\n",
       " ['detect', 'spurious', 'correlations', 'find', 'real', 'ones', 'data'],\n",
       " ['questions', 'data', 'scientist', 'solution', 'skillpower', 'datafest'],\n",
       " ['tune', 'number', 'size', 'decision', 'trees', 'xgboost'],\n",
       " ['finding', 'chairs', 'data', 'scientist', 'way', 'hint', 'using', 'part'],\n",
       " ['tips', 'train', 'brain', 'analytical', 'thinking'],\n",
       " ['linear', 'algebra', 'day', 'mini', 'course'],\n",
       " ['tensorflow', 'understanding', 'tensors', 'graphs'],\n",
       " ['time', 'series', 'forecasting', 'day', 'mini', 'course'],\n",
       " ['extracting',\n",
       "  'information',\n",
       "  'reports',\n",
       "  'using',\n",
       "  'regular',\n",
       "  'expressons',\n",
       "  'library'],\n",
       " ['neural', 'networks', 'find', 'best', 'words', 'title', 'ebook', 'data'],\n",
       " ['questions',\n",
       "  'time',\n",
       "  'series',\n",
       "  'solution',\n",
       "  'skillpower',\n",
       "  'time',\n",
       "  'series',\n",
       "  'datafest'],\n",
       " ['stochastic', 'gradient', 'boosting', 'xgboost', 'scikit'],\n",
       " ['revised',\n",
       "  'version',\n",
       "  'earlier',\n",
       "  'posted',\n",
       "  'analyticbridge',\n",
       "  'recent',\n",
       "  'topic',\n",
       "  'found',\n",
       "  'hidden',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'hdt'],\n",
       " ['overview',\n",
       "  'nowadays',\n",
       "  'numerous',\n",
       "  'risks',\n",
       "  'related',\n",
       "  'bank',\n",
       "  'loans',\n",
       "  'banks',\n",
       "  'borrowers',\n",
       "  'getting',\n",
       "  'loans',\n",
       "  'risk',\n",
       "  'analysis',\n",
       "  'bank',\n",
       "  'loans',\n",
       "  'ne'],\n",
       " ['understanding',\n",
       "  'model',\n",
       "  'know',\n",
       "  'important',\n",
       "  'practitioner',\n",
       "  'perspective',\n",
       "  'end',\n",
       "  'users',\n",
       "  'many',\n",
       "  'different',\n",
       "  'learni'],\n",
       " ['last',\n",
       "  'installment',\n",
       "  'mini',\n",
       "  'series',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  'stanford',\n",
       "  'collection',\n",
       "  'imdb',\n",
       "  'reviews',\n",
       "  'originally',\n",
       "  'published',\n",
       "  'recurrentn'],\n",
       " ['topic',\n",
       "  'combines',\n",
       "  'two',\n",
       "  'difficult',\n",
       "  'least',\n",
       "  'understood',\n",
       "  'ml',\n",
       "  'techniques',\n",
       "  'challenges',\n",
       "  'fraud',\n",
       "  'detection',\n",
       "  'invariably',\n",
       "  'falls',\n",
       "  'short',\n",
       "  'automatic'],\n",
       " ['continuation',\n",
       "  'mini',\n",
       "  'series',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  'movie',\n",
       "  'reviews',\n",
       "  'originally',\n",
       "  'appeared',\n",
       "  'recurrentnull',\n",
       "  'wordpress',\n",
       "  'com',\n",
       "  'last',\n",
       "  'time',\n",
       "  'ha'],\n",
       " ['guest',\n",
       "  'post',\n",
       "  'vijay',\n",
       "  'rajan',\n",
       "  'one',\n",
       "  'best',\n",
       "  'books',\n",
       "  'started',\n",
       "  'read',\n",
       "  'recent',\n",
       "  'times',\n",
       "  'numbers',\n",
       "  'rule',\n",
       "  'world',\n",
       "  'kaiser',\n",
       "  'fung',\n",
       "  'book',\n",
       "  'talks'],\n",
       " ['service',\n",
       "  'provider',\n",
       "  'able',\n",
       "  'anticipate',\n",
       "  'customers',\n",
       "  'behaviour',\n",
       "  'three',\n",
       "  'major',\n",
       "  'benefits',\n",
       "  'generate',\n",
       "  'customer',\n",
       "  'delight',\n",
       "  'prevent',\n",
       "  'customer',\n",
       "  'exhausti'],\n",
       " ['getting',\n",
       "  'popular',\n",
       "  'applications',\n",
       "  'software',\n",
       "  'products',\n",
       "  'accounting',\n",
       "  'hot',\n",
       "  'dog',\n",
       "  'recognition',\n",
       "  'apps',\n",
       "  'add',\n",
       "  'lea'],\n",
       " ['programmers',\n",
       "  'scikit',\n",
       "  'one',\n",
       "  'best',\n",
       "  'libraries',\n",
       "  'build',\n",
       "  'applications',\n",
       "  'ideal',\n",
       "  'beginners',\n",
       "  'reall'],\n",
       " ['post',\n",
       "  'show',\n",
       "  'accuracy',\n",
       "  'classifier',\n",
       "  'influenced',\n",
       "  'bag',\n",
       "  'words',\n",
       "  'done',\n",
       "  'naive',\n",
       "  'classifier',\n",
       "  'returns',\n",
       "  'good',\n",
       "  'info'],\n",
       " ['capacity',\n",
       "  'planning',\n",
       "  'arduous',\n",
       "  'ongoing',\n",
       "  'task',\n",
       "  'many',\n",
       "  'operations',\n",
       "  'teams',\n",
       "  'especially',\n",
       "  'rely',\n",
       "  'virtual',\n",
       "  'machines',\n",
       "  'vms',\n",
       "  'power',\n",
       "  'business',\n",
       "  'pi'],\n",
       " ['studio',\n",
       "  'ml',\n",
       "  'open',\n",
       "  'source',\n",
       "  'framework',\n",
       "  'simplify',\n",
       "  'accelerate',\n",
       "  'model',\n",
       "  'development',\n",
       "  'designed',\n",
       "  'help',\n",
       "  'ml',\n",
       "  'practioners',\n",
       "  'speed',\n",
       "  'experiments'],\n",
       " ['written',\n",
       "  'adrian',\n",
       "  'rosebrock',\n",
       "  'adrian',\n",
       "  'entrepreneur',\n",
       "  'ph',\n",
       "  'launched',\n",
       "  'two',\n",
       "  'successful',\n",
       "  'image',\n",
       "  'search',\n",
       "  'engines',\n",
       "  'id',\n",
       "  'pill',\n",
       "  'chic',\n",
       "  'engi'],\n",
       " ['go',\n",
       "  'bit',\n",
       "  'detail',\n",
       "  'means',\n",
       "  'explain',\n",
       "  'calculate',\n",
       "  'distance',\n",
       "  'centroid',\n",
       "  'data',\n",
       "  'points',\n",
       "  'form'],\n",
       " ['missile',\n",
       "  'approaching',\n",
       "  'every',\n",
       "  'one',\n",
       "  'run',\n",
       "  'cover',\n",
       "  'people',\n",
       "  'sending',\n",
       "  'final',\n",
       "  'goodbye',\n",
       "  'messages',\n",
       "  'loved',\n",
       "  'ones',\n",
       "  'family',\n",
       "  'members',\n",
       "  'huddling',\n",
       "  'praying',\n",
       "  'together',\n",
       "  'moth'],\n",
       " ['cosine',\n",
       "  'similarity',\n",
       "  'cosine',\n",
       "  'similarity',\n",
       "  'measure',\n",
       "  'similarity',\n",
       "  'two',\n",
       "  'vectors',\n",
       "  'calculates',\n",
       "  'cosine',\n",
       "  'angle',\n",
       "  'similarity',\n",
       "  'ra'],\n",
       " ['digital',\n",
       "  'image',\n",
       "  'processing',\n",
       "  'discipline',\n",
       "  'studies',\n",
       "  'image',\n",
       "  'processing',\n",
       "  'techniques',\n",
       "  'image',\n",
       "  'referred',\n",
       "  'research',\n",
       "  'static',\n",
       "  'image',\n",
       "  'form',\n",
       "  'vision',\n",
       "  'sensors'],\n",
       " ['summary',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'cnn',\n",
       "  'convnet',\n",
       "  'class',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'successfully',\n",
       "  'applied',\n",
       "  'image',\n",
       "  'recogni'],\n",
       " ['image',\n",
       "  'comes',\n",
       "  'xkcd',\n",
       "  'webcomic',\n",
       "  'romance',\n",
       "  'sarcasm',\n",
       "  'math',\n",
       "  'language',\n",
       "  'created',\n",
       "  'randall',\n",
       "  'munroe',\n",
       "  'cnu',\n",
       "  'graduate',\n",
       "  'degree',\n",
       "  'physics',\n",
       "  'befor'],\n",
       " ['written',\n",
       "  'adrian',\n",
       "  'rosebrock',\n",
       "  'adrian',\n",
       "  'entrepreneur',\n",
       "  'ph',\n",
       "  'launched',\n",
       "  'two',\n",
       "  'successful',\n",
       "  'image',\n",
       "  'search',\n",
       "  'engines',\n",
       "  'id',\n",
       "  'pill',\n",
       "  'chic',\n",
       "  'engi'],\n",
       " ['explores',\n",
       "  'typical',\n",
       "  'image',\n",
       "  'identification',\n",
       "  'task',\n",
       "  'using',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'purpose',\n",
       "  'simple',\n",
       "  'javacnn'],\n",
       " ['problem',\n",
       "  'given',\n",
       "  'set',\n",
       "  'labeled',\n",
       "  'images',\n",
       "  'cats',\n",
       "  'dogs',\n",
       "  'model',\n",
       "  'learnt',\n",
       "  'later',\n",
       "  'used',\n",
       "  'classify',\n",
       "  'set',\n",
       "  'new',\n",
       "  'ima'],\n",
       " ['written',\n",
       "  'denny',\n",
       "  'britz',\n",
       "  'post',\n",
       "  'implement',\n",
       "  'simple',\n",
       "  'layer',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'scratch',\n",
       "  'derive',\n",
       "  'math',\n",
       "  'requi'],\n",
       " ['implementing',\n",
       "  'distributed',\n",
       "  'network',\n",
       "  'spark',\n",
       "  'authors',\n",
       "  'dr',\n",
       "  'vijay',\n",
       "  'srinivas',\n",
       "  'agneeswaran',\n",
       "  'director',\n",
       "  'head',\n",
       "  'big',\n",
       "  'data',\n",
       "  'labs',\n",
       "  'impetus'],\n",
       " ['ggplot',\n",
       "  'elegant',\n",
       "  'aesthetically',\n",
       "  'pleasing',\n",
       "  'graphics',\n",
       "  'framework',\n",
       "  'available',\n",
       "  'nicely',\n",
       "  'planned',\n",
       "  'structure',\n",
       "  'focusses',\n",
       "  'ex'],\n",
       " ['posted',\n",
       "  'sunil',\n",
       "  'ray',\n",
       "  'sunil',\n",
       "  'business',\n",
       "  'analytics',\n",
       "  'intelligence',\n",
       "  'professional',\n",
       "  'experience',\n",
       "  'indian',\n",
       "  'insurance',\n",
       "  'industry',\n",
       "  'introd'],\n",
       " ['written',\n",
       "  'sunil',\n",
       "  'ray',\n",
       "  'sunil',\n",
       "  'business',\n",
       "  'analytics',\n",
       "  'intelligence',\n",
       "  'professional',\n",
       "  'experience',\n",
       "  'introduction',\n",
       "  'difference',\n",
       "  'mind'],\n",
       " ['show',\n",
       "  'issue',\n",
       "  'polynomial',\n",
       "  'regression',\n",
       "  'fitting',\n",
       "  'numerical',\n",
       "  'precision',\n",
       "  'even',\n",
       "  'done',\n",
       "  'right',\n",
       "  'numerical',\n",
       "  'precision',\n",
       "  'still'],\n",
       " ['posted',\n",
       "  'arpan',\n",
       "  'gupta',\n",
       "  'indian',\n",
       "  'institute',\n",
       "  'technology',\n",
       "  'let',\n",
       "  'precise',\n",
       "  'demo',\n",
       "  'fitting',\n",
       "  'logistic',\n",
       "  'regression',\n",
       "  'titanic',\n",
       "  'data',\n",
       "  'set',\n",
       "  'fo'],\n",
       " ['posted',\n",
       "  'sunil',\n",
       "  'ray',\n",
       "  'sunil',\n",
       "  'business',\n",
       "  'analytics',\n",
       "  'bi',\n",
       "  'professional',\n",
       "  'source',\n",
       "  'picture',\n",
       "  'click',\n",
       "  'introduction',\n",
       "  'situation'],\n",
       " ['written',\n",
       "  'stephanie',\n",
       "  'kim',\n",
       "  'stephanie',\n",
       "  'professional',\n",
       "  'experience',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'processing',\n",
       "  'including',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'along'],\n",
       " ['written',\n",
       "  'sunil',\n",
       "  'ray',\n",
       "  'sunil',\n",
       "  'business',\n",
       "  'analytics',\n",
       "  'intelligence',\n",
       "  'professional',\n",
       "  'experience',\n",
       "  'indian',\n",
       "  'insurance',\n",
       "  'industry',\n",
       "  'intr'],\n",
       " ['interesting',\n",
       "  'picture',\n",
       "  'comparing',\n",
       "  'linear',\n",
       "  'logistic',\n",
       "  'poisson',\n",
       "  'regression',\n",
       "  'regression',\n",
       "  'read',\n",
       "  'ml'],\n",
       " ['per',\n",
       "  'largest',\n",
       "  'market',\n",
       "  'research',\n",
       "  'firm',\n",
       "  'speech',\n",
       "  'analytics',\n",
       "  'industry',\n",
       "  'grow',\n",
       "  'usd',\n",
       "  'billion',\n",
       "  'compound',\n",
       "  'annual',\n",
       "  'growth',\n",
       "  'rate'],\n",
       " ['decision',\n",
       "  'trees',\n",
       "  'commonly',\n",
       "  'used',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'objective',\n",
       "  'creating',\n",
       "  'model',\n",
       "  'predicts',\n",
       "  'value',\n",
       "  'target',\n",
       "  'dependent',\n",
       "  'variable',\n",
       "  'based'],\n",
       " ['may',\n",
       "  'know',\n",
       "  'aster',\n",
       "  'provides',\n",
       "  'capabilities',\n",
       "  'text',\n",
       "  'analysis',\n",
       "  'functions',\n",
       "  'easy',\n",
       "  'allow',\n",
       "  'perform',\n",
       "  'text',\n",
       "  'analysi'],\n",
       " ['introduction',\n",
       "  'exploratory',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'eda',\n",
       "  'approach',\n",
       "  'philosophy',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'employs',\n",
       "  'variety',\n",
       "  'techniques',\n",
       "  'graphical',\n",
       "  'quantitative'],\n",
       " ['gentle', 'introduction', 'neural', 'translation'],\n",
       " ['gentle', 'introduction', 'exploding', 'gradients', 'neural', 'networks'],\n",
       " ['gentle', 'introduction', 'text', 'summarization'],\n",
       " ['gentle', 'introduction', 'bootstrap'],\n",
       " ['data', 'munging', 'using', 'pandas'],\n",
       " ['gentle', 'introduction', 'nonparametric', 'statistics'],\n",
       " ['gentle', 'introduction', 'autocorrelation', 'partial', 'autocorrelation'],\n",
       " ['gentle', 'introduction', 'probability', 'scoring'],\n",
       " ['gentle', 'introduction', 'statistical', 'sampling', 'resampling'],\n",
       " ['data', 'exploration', 'using', 'numpy', 'matplotlib', 'pandas'],\n",
       " ['introductory', 'maximum', 'likelihood', 'estimation', 'case', 'study'],\n",
       " ['gentle', 'introduction', 'predictive', 'modeling'],\n",
       " ['gentle', 'introduction', 'linear', 'algebra'],\n",
       " ['gentle', 'introduction', 'scikit'],\n",
       " ['combining', 'cnns', 'rnns', 'crazy', 'genius', 'data'],\n",
       " ['introduction', 'semantic', 'segmentation', 'google', 'deeplab'],\n",
       " ['introduction', 'pseudo', 'labelling', 'semi', 'supervised', 'technique'],\n",
       " ['variance', 'clustering', 'density', 'estimation', 'revisited', 'data'],\n",
       " ['top',\n",
       "  'github',\n",
       "  'repositories',\n",
       "  'reddit',\n",
       "  'threads',\n",
       "  'every',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'know',\n",
       "  'june',\n",
       "  'analytics'],\n",
       " ['best', 'github', 'repostories', 'reddit', 'discussions'],\n",
       " ['introduction', 'pytorch', 'simple', 'yet', 'powerful', 'library'],\n",
       " ['gentle', 'introduction', 'box', 'jenkins', 'time', 'series', 'forecasting'],\n",
       " ['introduction',\n",
       "  'random',\n",
       "  'forest',\n",
       "  'simplified',\n",
       "  'business',\n",
       "  'case',\n",
       "  'study'],\n",
       " ['gentle', 'introduction', 'statistical', 'hypothesis', 'tests'],\n",
       " ['gentle', 'introduction', 'eigenvalues', 'eigenvectors'],\n",
       " ['gentle', 'introduction', 'estimation', 'statistics'],\n",
       " ['gentle', 'introduction', 'vectors'],\n",
       " ['gentle', 'introduction', 'vector', 'norms'],\n",
       " ['gentle', 'introduction', 'tensors', 'numpy'],\n",
       " ['gentle', 'introduction', 'transduction'],\n",
       " ['gentle', 'introduction', 'xgboost', 'applied'],\n",
       " ['missing', 'roadmap', 'self', 'study'],\n",
       " ['gentle', 'introduction', 'chi', 'squared'],\n",
       " ['action', 'probability', 'could', 'monty', 'hall', 'made', 'money'],\n",
       " ['gentle', 'introduction', 'limit', 'theorem'],\n",
       " ['gentle', 'introduction', 'bias', 'variance', 'trade'],\n",
       " ['gentle', 'introduction', 'dropout', 'regularizing', 'neural', 'networks'],\n",
       " ['gentle', 'introduction', 'caption', 'generation', 'models'],\n",
       " ['gentle', 'introduction', 'transfer'],\n",
       " ['introduction',\n",
       "  'conditional',\n",
       "  'probability',\n",
       "  'bayes',\n",
       "  'theorem',\n",
       "  'data',\n",
       "  'professionals'],\n",
       " ['gentle', 'introduction', 'gradient', 'boosting', 'algorithm'],\n",
       " ['automatic', 'image', 'captioning', 'using', 'cnn', 'lstm', 'pytorch'],\n",
       " ['gentle', 'introduction', 'backpropagation', 'time'],\n",
       " ['gentle', 'introduction', 'concept', 'drift'],\n",
       " ['gentle', 'introduction', 'rnn', 'unrolling'],\n",
       " ['introductory',\n",
       "  'regularized',\n",
       "  'greedy',\n",
       "  'forests',\n",
       "  'rgf',\n",
       "  'case',\n",
       "  'study',\n",
       "  'analytics'],\n",
       " ['gentle', 'introduction', 'effect', 'size', 'measures'],\n",
       " ['develop',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'model',\n",
       "  'attention',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'prediction',\n",
       "  'keras'],\n",
       " ['develop',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'model',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'prediction',\n",
       "  'keras'],\n",
       " ['keras', 'functional', 'api'],\n",
       " ['regression', 'keras', 'library'],\n",
       " ['one',\n",
       "  'wants',\n",
       "  'sold',\n",
       "  'everyone',\n",
       "  'wants',\n",
       "  'buy',\n",
       "  'us',\n",
       "  'hate',\n",
       "  'sold',\n",
       "  'moment',\n",
       "  'know',\n",
       "  'someone',\n",
       "  'selling',\n",
       "  'something',\n",
       "  'keep',\n",
       "  'guards'],\n",
       " ['written',\n",
       "  'joseph',\n",
       "  'rickert',\n",
       "  'know',\n",
       "  'correlation',\n",
       "  'imply',\n",
       "  'causation',\n",
       "  'unmeasured',\n",
       "  'unknown',\n",
       "  'factors',\n",
       "  'confound',\n",
       "  'seemin'],\n",
       " ['contributed',\n",
       "  'kelly',\n",
       "  'mejia',\n",
       "  'breton',\n",
       "  'graduated',\n",
       "  'nyc',\n",
       "  'data',\n",
       "  'academy',\n",
       "  'week',\n",
       "  'full',\n",
       "  'time',\n",
       "  'data',\n",
       "  'bootcamp',\n",
       "  'program',\n",
       "  'taking',\n",
       "  'place',\n",
       "  'april'],\n",
       " ['written',\n",
       "  'jim',\n",
       "  'frost',\n",
       "  'standard',\n",
       "  'error',\n",
       "  'regression',\n",
       "  'squared',\n",
       "  'two',\n",
       "  'key',\n",
       "  'goodness',\n",
       "  'fit',\n",
       "  'measures',\n",
       "  'regression',\n",
       "  'analysis',\n",
       "  'whi'],\n",
       " ['airbnb',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'contributed',\n",
       "  'jurgen',\n",
       "  'de',\n",
       "  'jager',\n",
       "  'takes',\n",
       "  'nyc',\n",
       "  'data',\n",
       "  'academy',\n",
       "  'week',\n",
       "  'full',\n",
       "  'time',\n",
       "  'data',\n",
       "  'bootcamp',\n",
       "  'program',\n",
       "  'july',\n",
       "  'th'],\n",
       " ['contributed',\n",
       "  'daniel',\n",
       "  'donohue',\n",
       "  'daniel',\n",
       "  'took',\n",
       "  'nyc',\n",
       "  'data',\n",
       "  'academy',\n",
       "  'week',\n",
       "  'full',\n",
       "  'time',\n",
       "  'data',\n",
       "  'bootcamp',\n",
       "  'program',\n",
       "  'sept',\n",
       "  'dec',\n",
       "  'post'],\n",
       " ['contributed',\n",
       "  'frank',\n",
       "  'wang',\n",
       "  'currently',\n",
       "  'nyc',\n",
       "  'data',\n",
       "  'academy',\n",
       "  'week',\n",
       "  'full',\n",
       "  'time',\n",
       "  'data',\n",
       "  'bootcamp',\n",
       "  'program',\n",
       "  'taking',\n",
       "  'place',\n",
       "  'april',\n",
       "  'th'],\n",
       " ['guest',\n",
       "  'chris',\n",
       "  'rigatuso',\n",
       "  'chris',\n",
       "  'founder',\n",
       "  'board',\n",
       "  'member',\n",
       "  'skyfollow',\n",
       "  'consulting',\n",
       "  'group',\n",
       "  'earned',\n",
       "  'mba',\n",
       "  'haas',\n",
       "  'school',\n",
       "  'business',\n",
       "  'uc',\n",
       "  'berke'],\n",
       " ['formal',\n",
       "  'statistical',\n",
       "  'analyse',\n",
       "  'quantitative',\n",
       "  'data',\n",
       "  'data',\n",
       "  'increased',\n",
       "  'considerably',\n",
       "  'last',\n",
       "  'years',\n",
       "  'one',\n",
       "  'approach',\n",
       "  'bayes'],\n",
       " ['purpose',\n",
       "  'variance',\n",
       "  'covariance',\n",
       "  'matrix',\n",
       "  'illustrate',\n",
       "  'variance',\n",
       "  'particular',\n",
       "  'variable',\n",
       "  'diagonals',\n",
       "  'covariance',\n",
       "  'illustrates',\n",
       "  'covariances',\n",
       "  'bet'],\n",
       " ['relatively',\n",
       "  'new',\n",
       "  'nlp',\n",
       "  'text',\n",
       "  'analysis',\n",
       "  'world',\n",
       "  'likely',\n",
       "  'come',\n",
       "  'across',\n",
       "  'pretty',\n",
       "  'technical',\n",
       "  'terms',\n",
       "  'acronyms',\n",
       "  'chall'],\n",
       " ['fictitious',\n",
       "  'retailer',\n",
       "  'calling',\n",
       "  'malwart',\n",
       "  'sells',\n",
       "  'automobiles',\n",
       "  'automotive',\n",
       "  'parts',\n",
       "  'focused',\n",
       "  'sale',\n",
       "  'particular',\n",
       "  'automobil'],\n",
       " ['last',\n",
       "  'introduced',\n",
       "  'concept',\n",
       "  'market',\n",
       "  'mix',\n",
       "  'modeling',\n",
       "  'want',\n",
       "  'refresher',\n",
       "  'please',\n",
       "  'click',\n",
       "  'link'],\n",
       " ['last',\n",
       "  'decade',\n",
       "  'seen',\n",
       "  'tremendous',\n",
       "  'interest',\n",
       "  'application',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'statistical',\n",
       "  'algorithms',\n",
       "  'first',\n",
       "  'research'],\n",
       " ['posting',\n",
       "  'last',\n",
       "  'decided',\n",
       "  'next',\n",
       "  'part',\n",
       "  'series',\n",
       "  'xgboost',\n",
       "  'versatile',\n",
       "  'highly',\n",
       "  'performant',\n",
       "  'inter',\n",
       "  'operable',\n",
       "  'platform',\n",
       "  'im'],\n",
       " ['summary',\n",
       "  'reinforcement',\n",
       "  'rl',\n",
       "  'likely',\n",
       "  'next',\n",
       "  'big',\n",
       "  'push',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'core',\n",
       "  'technique',\n",
       "  'robotics',\n",
       "  'smart',\n",
       "  'iot',\n",
       "  'game',\n",
       "  'play'],\n",
       " ['implementation',\n",
       "  'lucas',\n",
       "  'kanade',\n",
       "  'optical',\n",
       "  'flow',\n",
       "  'algorithm',\n",
       "  'going',\n",
       "  'described',\n",
       "  'problem',\n",
       "  'appeared',\n",
       "  'assignment',\n",
       "  'computer',\n",
       "  'vi'],\n",
       " ['written',\n",
       "  'sebastian',\n",
       "  'ruder',\n",
       "  'sebastian',\n",
       "  'phd',\n",
       "  'student',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'research',\n",
       "  'scientist',\n",
       "  'aylien',\n",
       "  'blogs',\n",
       "  'machi'],\n",
       " ['following',\n",
       "  'problem',\n",
       "  'appeared',\n",
       "  'assignment',\n",
       "  'coursera',\n",
       "  'course',\n",
       "  'algorithm',\n",
       "  'prof',\n",
       "  'robert',\n",
       "  'sedgewick',\n",
       "  'princeton',\n",
       "  'university',\n",
       "  'years',\n",
       "  'back'],\n",
       " ['despite',\n",
       "  'years',\n",
       "  'criticism',\n",
       "  'negative',\n",
       "  'publicity',\n",
       "  'hedge',\n",
       "  'funds',\n",
       "  'evolved',\n",
       "  'higher',\n",
       "  'return',\n",
       "  'generating',\n",
       "  'machines',\n",
       "  'thanks',\n",
       "  'amazingly',\n",
       "  'weird',\n",
       "  'hedge',\n",
       "  'fun'],\n",
       " ['long',\n",
       "  'posted',\n",
       "  'sebastian',\n",
       "  'raschka',\n",
       "  'provide',\n",
       "  'table',\n",
       "  'content',\n",
       "  'chart',\n",
       "  'showing',\n",
       "  'results',\n",
       "  'pca',\n",
       "  'applied',\n",
       "  'wine',\n",
       "  'data'],\n",
       " ['problem',\n",
       "  'appeared',\n",
       "  'assignment',\n",
       "  'coursera',\n",
       "  'course',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'stanford',\n",
       "  'following',\n",
       "  'description',\n",
       "  'problem',\n",
       "  'ta'],\n",
       " ['two',\n",
       "  'main',\n",
       "  'data',\n",
       "  'types',\n",
       "  'business',\n",
       "  'nominal',\n",
       "  'categorical',\n",
       "  'qualitative',\n",
       "  'data',\n",
       "  'interval',\n",
       "  'data',\n",
       "  'quantitative',\n",
       "  'continuous',\n",
       "  'data',\n",
       "  'nominal',\n",
       "  'data',\n",
       "  'ca'],\n",
       " ['tensorflow',\n",
       "  'launched',\n",
       "  'interactive',\n",
       "  'browser',\n",
       "  'platform',\n",
       "  'seedbank',\n",
       "  'seedbank',\n",
       "  'enables',\n",
       "  'discover',\n",
       "  'various',\n",
       "  'new',\n",
       "  'examples',\n",
       "  'assists',\n",
       "  'understanding',\n",
       "  'implementing',\n",
       "  'ideas'],\n",
       " ['introduction',\n",
       "  'business',\n",
       "  'ventures',\n",
       "  'based',\n",
       "  'existing',\n",
       "  'disruptive',\n",
       "  'business',\n",
       "  'models',\n",
       "  'taking',\n",
       "  'route',\n",
       "  'initial',\n",
       "  'public',\n",
       "  'offering',\n",
       "  'always',\n",
       "  'challenge',\n",
       "  'investo'],\n",
       " ['following',\n",
       "  'problems',\n",
       "  'appeared',\n",
       "  'first',\n",
       "  'assignments',\n",
       "  'udacity',\n",
       "  'course',\n",
       "  'google',\n",
       "  'descriptions',\n",
       "  'problems',\n",
       "  'taken'],\n",
       " ['written',\n",
       "  'aisha',\n",
       "  'javed',\n",
       "  'summary',\n",
       "  'table',\n",
       "  'content',\n",
       "  'long',\n",
       "  'whether',\n",
       "  'beginner',\n",
       "  'yo'],\n",
       " ['document',\n",
       "  'comes',\n",
       "  'keras',\n",
       "  'documentation',\n",
       "  'found',\n",
       "  'keras',\n",
       "  'keras',\n",
       "  'high',\n",
       "  'level',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'api',\n",
       "  'written',\n",
       "  'capable',\n",
       "  'running'],\n",
       " ['recently',\n",
       "  'kaggle',\n",
       "  'master',\n",
       "  'kazanova',\n",
       "  'along',\n",
       "  'friends',\n",
       "  'released',\n",
       "  'win',\n",
       "  'data',\n",
       "  'competition',\n",
       "  'coursera',\n",
       "  'course',\n",
       "  'course',\n",
       "  'involved',\n",
       "  'final'],\n",
       " ['following',\n",
       "  'problems',\n",
       "  'appeared',\n",
       "  'assignments',\n",
       "  'udacity',\n",
       "  'course',\n",
       "  'google',\n",
       "  'descriptions',\n",
       "  'problems',\n",
       "  'taken',\n",
       "  'assignm'],\n",
       " ['post',\n",
       "  'written',\n",
       "  'ofir',\n",
       "  'press',\n",
       "  'ofir',\n",
       "  'graduate',\n",
       "  'student',\n",
       "  'tel',\n",
       "  'aviv',\n",
       "  'university',\n",
       "  'lab',\n",
       "  'main',\n",
       "  'focus',\n",
       "  'using',\n",
       "  'natura'],\n",
       " ['guest',\n",
       "  'sebastian',\n",
       "  'raschka',\n",
       "  'originally',\n",
       "  'posted',\n",
       "  'tackle',\n",
       "  'supervised',\n",
       "  'problem',\n",
       "  'advice',\n",
       "  'start',\n",
       "  'simplest',\n",
       "  'hypothesis',\n",
       "  'space'],\n",
       " ['summary',\n",
       "  'based',\n",
       "  'neural',\n",
       "  'nets',\n",
       "  'launching',\n",
       "  'thousand',\n",
       "  'ventures',\n",
       "  'leaving',\n",
       "  'tens',\n",
       "  'thousands',\n",
       "  'behind',\n",
       "  'transfer',\n",
       "  'tl'],\n",
       " ['post',\n",
       "  'written',\n",
       "  'ofir',\n",
       "  'press',\n",
       "  'ofir',\n",
       "  'graduate',\n",
       "  'student',\n",
       "  'tel',\n",
       "  'aviv',\n",
       "  'university',\n",
       "  'lab',\n",
       "  'main',\n",
       "  'focus',\n",
       "  'using',\n",
       "  'natura'],\n",
       " ['recently',\n",
       "  'kaggle',\n",
       "  'master',\n",
       "  'kazanova',\n",
       "  'along',\n",
       "  'friends',\n",
       "  'released',\n",
       "  'win',\n",
       "  'data',\n",
       "  'competition',\n",
       "  'coursera',\n",
       "  'course',\n",
       "  'course',\n",
       "  'involved',\n",
       "  'final'],\n",
       " ['problem',\n",
       "  'appeared',\n",
       "  'assignment',\n",
       "  'online',\n",
       "  'coursera',\n",
       "  'course',\n",
       "  'convolution',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'prof',\n",
       "  'andrew',\n",
       "  'ng',\n",
       "  'deeplearing',\n",
       "  'ai',\n",
       "  'description'],\n",
       " ['interesting',\n",
       "  'listing',\n",
       "  'published',\n",
       "  'mashape',\n",
       "  'top',\n",
       "  'listed',\n",
       "  'would',\n",
       "  'nice',\n",
       "  'separate',\n",
       "  'voice',\n",
       "  'recognition',\n",
       "  'apis',\n",
       "  'ive',\n",
       "  'thin'],\n",
       " ['open',\n",
       "  'source',\n",
       "  'exploration',\n",
       "  'citys',\n",
       "  'neighborhoods',\n",
       "  'nightlife',\n",
       "  'airport',\n",
       "  'traffic',\n",
       "  'lens',\n",
       "  'publicly',\n",
       "  'available',\n",
       "  'taxi',\n",
       "  'uber',\n",
       "  'data',\n",
       "  'images'],\n",
       " ['written',\n",
       "  'michael',\n",
       "  'rundell',\n",
       "  'curated',\n",
       "  'list',\n",
       "  'data',\n",
       "  'interview',\n",
       "  'questions',\n",
       "  'preparing',\n",
       "  'interview',\n",
       "  'easy',\n",
       "  'naturally'],\n",
       " ['nikolay',\n",
       "  'laptev',\n",
       "  'slawek',\n",
       "  'smyl',\n",
       "  'santhosh',\n",
       "  'shanmugam',\n",
       "  'uber',\n",
       "  'event',\n",
       "  'forecasting',\n",
       "  'enables',\n",
       "  'us',\n",
       "  'future',\n",
       "  'proof',\n",
       "  'services',\n",
       "  'based',\n",
       "  'anticipated'],\n",
       " ['ever',\n",
       "  'wondered',\n",
       "  'people',\n",
       "  'affinity',\n",
       "  'towards',\n",
       "  'using',\n",
       "  'certain',\n",
       "  'apps',\n",
       "  'vs',\n",
       "  'others',\n",
       "  'even',\n",
       "  'though',\n",
       "  'apps',\n",
       "  'provide',\n",
       "  'functionality',\n",
       "  'attrac'],\n",
       " ['today',\n",
       "  'tends',\n",
       "  'open',\n",
       "  'loop',\n",
       "  'collect',\n",
       "  'tons',\n",
       "  'data',\n",
       "  'offline',\n",
       "  'process',\n",
       "  'batches',\n",
       "  'generate',\n",
       "  'insights',\n",
       "  'eventual',\n",
       "  'action'],\n",
       " ['cheat',\n",
       "  'sheet',\n",
       "  'algorithms',\n",
       "  'includes',\n",
       "  'codes',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'random',\n",
       "  'forest',\n",
       "  'gradient',\n",
       "  'boost',\n",
       "  'kmeans',\n",
       "  'knn',\n",
       "  'etc'],\n",
       " ['js',\n",
       "  'awesome',\n",
       "  'library',\n",
       "  'visualization',\n",
       "  'however',\n",
       "  'requires',\n",
       "  'developer',\n",
       "  'perform',\n",
       "  'magic',\n",
       "  'far',\n",
       "  'popular',\n",
       "  'traditional',\n",
       "  'data'],\n",
       " ['introduction',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'highly',\n",
       "  'expressive',\n",
       "  'networks',\n",
       "  'around',\n",
       "  'many',\n",
       "  'decades',\n",
       "  'gains',\n",
       "  'computing',\n",
       "  'powe'],\n",
       " ['choosing',\n",
       "  'features',\n",
       "  'improve',\n",
       "  'performance',\n",
       "  'particular',\n",
       "  'algorithm',\n",
       "  'difficult',\n",
       "  'question',\n",
       "  'currently',\n",
       "  'pca',\n",
       "  'difficult',\n",
       "  'understand',\n",
       "  'although'],\n",
       " ['bell',\n",
       "  'curve',\n",
       "  'gaussian',\n",
       "  'bell',\n",
       "  'curve',\n",
       "  'one',\n",
       "  'fundamental',\n",
       "  'concepts',\n",
       "  'statistical',\n",
       "  'analysis',\n",
       "  'based',\n",
       "  'social',\n",
       "  'sciences'],\n",
       " ['commerce',\n",
       "  'become',\n",
       "  'popular',\n",
       "  'growth',\n",
       "  'internet',\n",
       "  'network',\n",
       "  'technologies',\n",
       "  'many',\n",
       "  'people',\n",
       "  'feel',\n",
       "  'convenient',\n",
       "  'buy',\n",
       "  'products',\n",
       "  'online',\n",
       "  'using',\n",
       "  'various',\n",
       "  'forum'],\n",
       " ['introduction',\n",
       "  'series',\n",
       "  'exploratory',\n",
       "  'posts',\n",
       "  'explore',\n",
       "  'relationship',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'rnns',\n",
       "  'iot',\n",
       "  'data',\n",
       "  'wri'],\n",
       " ['written',\n",
       "  'andrej',\n",
       "  'karpathy',\n",
       "  'andrej',\n",
       "  'phd',\n",
       "  'student',\n",
       "  'stanford',\n",
       "  'research',\n",
       "  'scientist',\n",
       "  'openai',\n",
       "  'working',\n",
       "  'generative',\n",
       "  'models'],\n",
       " ['evaluate', 'performance', 'models', 'keras'],\n",
       " ['sequence',\n",
       "  'classification',\n",
       "  'lstm',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'keras'],\n",
       " ['save', 'load', 'keras', 'models'],\n",
       " ['hardly',\n",
       "  'possible',\n",
       "  'real',\n",
       "  'life',\n",
       "  'develop',\n",
       "  'good',\n",
       "  'model',\n",
       "  'single',\n",
       "  'pass',\n",
       "  'ml',\n",
       "  'modeling',\n",
       "  'iterative',\n",
       "  'process',\n",
       "  'extremely',\n",
       "  'important'],\n",
       " ['ensemble',\n",
       "  'modeling',\n",
       "  'boost',\n",
       "  'power',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'trick',\n",
       "  'select',\n",
       "  'right',\n",
       "  'model',\n",
       "  'helps',\n",
       "  'build',\n",
       "  'accurate',\n",
       "  'models'],\n",
       " ['last',\n",
       "  'series',\n",
       "  'discussed',\n",
       "  'multivariate',\n",
       "  'linear',\n",
       "  'regression',\n",
       "  'model',\n",
       "  'fernando',\n",
       "  'creates',\n",
       "  'model',\n",
       "  'estimates',\n",
       "  'price',\n",
       "  'car',\n",
       "  'based'],\n",
       " ['last',\n",
       "  'posts',\n",
       "  'series',\n",
       "  'discussed',\n",
       "  'regression',\n",
       "  'models',\n",
       "  'length',\n",
       "  'fernando',\n",
       "  'built',\n",
       "  'multivariate',\n",
       "  'regression',\n",
       "  'model',\n",
       "  'model',\n",
       "  'takes',\n",
       "  'follow'],\n",
       " ['rohan',\n",
       "  'kotwani',\n",
       "  'kerneml',\n",
       "  'kernelml',\n",
       "  'brute',\n",
       "  'force',\n",
       "  'optimizer',\n",
       "  'used',\n",
       "  'train',\n",
       "  'models',\n",
       "  'package',\n",
       "  'uses',\n",
       "  'combination',\n",
       "  'lear'],\n",
       " ['guest',\n",
       "  'kevin',\n",
       "  'gray',\n",
       "  'kevin',\n",
       "  'president',\n",
       "  'cannon',\n",
       "  'gray',\n",
       "  'marketing',\n",
       "  'analytics',\n",
       "  'consultancy',\n",
       "  'regression',\n",
       "  'arguably',\n",
       "  'workhorse',\n",
       "  'statistic'],\n",
       " ['rubens',\n",
       "  'zimbres',\n",
       "  'rubens',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'phd',\n",
       "  'business',\n",
       "  'administration',\n",
       "  'developing',\n",
       "  'nlp',\n",
       "  'ai',\n",
       "  'models',\n",
       "  'using'],\n",
       " ['implementing',\n",
       "  'program',\n",
       "  'like',\n",
       "  'first',\n",
       "  'develop',\n",
       "  'logic',\n",
       "  'model',\n",
       "  'fast',\n",
       "  'exercise',\n",
       "  'lead',\n",
       "  'insights',\n",
       "  'strengths',\n",
       "  'weaknesses'],\n",
       " ['present',\n",
       "  'backpropagation',\n",
       "  'algorithm',\n",
       "  'continuous',\n",
       "  'target',\n",
       "  'variable',\n",
       "  'activation',\n",
       "  'function',\n",
       "  'hidden',\n",
       "  'layer',\n",
       "  'although',\n",
       "  'simpler',\n",
       "  'one',\n",
       "  'used',\n",
       "  'fo'],\n",
       " ['written',\n",
       "  'sondos',\n",
       "  'atwi',\n",
       "  'cross',\n",
       "  'validation',\n",
       "  'resampling',\n",
       "  'used',\n",
       "  'model',\n",
       "  'evaluation',\n",
       "  'avoid',\n",
       "  'testing',\n",
       "  'model'],\n",
       " ['anyone',\n",
       "  'attended',\n",
       "  'statistical',\n",
       "  'training',\n",
       "  'college',\n",
       "  'level',\n",
       "  'taught',\n",
       "  'four',\n",
       "  'rules',\n",
       "  'always',\n",
       "  'abide',\n",
       "  'developing',\n",
       "  'statistical',\n",
       "  'model'],\n",
       " ['background',\n",
       "  'new',\n",
       "  'home',\n",
       "  'construction',\n",
       "  'plays',\n",
       "  'significant',\n",
       "  'role',\n",
       "  'housing',\n",
       "  'economy',\n",
       "  'simultaneously',\n",
       "  'impacting',\n",
       "  'sectors',\n",
       "  'timber',\n",
       "  'furniture',\n",
       "  'hom'],\n",
       " ['written',\n",
       "  'jacob',\n",
       "  'joseph',\n",
       "  'unlike',\n",
       "  'evaluating',\n",
       "  'accuracy',\n",
       "  'models',\n",
       "  'predict',\n",
       "  'continuous',\n",
       "  'discrete',\n",
       "  'dependent',\n",
       "  'variable',\n",
       "  'like',\n",
       "  'linear',\n",
       "  'regres'],\n",
       " ['model',\n",
       "  'evaluation',\n",
       "  'metrics',\n",
       "  'used',\n",
       "  'assess',\n",
       "  'goodness',\n",
       "  'fit',\n",
       "  'model',\n",
       "  'data',\n",
       "  'compare',\n",
       "  'different',\n",
       "  'models',\n",
       "  'context',\n",
       "  'model',\n",
       "  'selection',\n",
       "  'pred'],\n",
       " ['collinearity',\n",
       "  'new',\n",
       "  'term',\n",
       "  'especially',\n",
       "  'dealing',\n",
       "  'multiple',\n",
       "  'regression',\n",
       "  'models',\n",
       "  'phenomenon',\n",
       "  'relationship',\n",
       "  'one',\n",
       "  'resp'],\n",
       " ['logistic',\n",
       "  'regression',\n",
       "  'lr',\n",
       "  'models',\n",
       "  'estimate',\n",
       "  'probability',\n",
       "  'binary',\n",
       "  'response',\n",
       "  'based',\n",
       "  'one',\n",
       "  'predictor',\n",
       "  'variables',\n",
       "  'unlike',\n",
       "  'linear',\n",
       "  'regression',\n",
       "  'models'],\n",
       " ['guest',\n",
       "  'post',\n",
       "  'kevin',\n",
       "  'jacobs',\n",
       "  'mlps',\n",
       "  'multi',\n",
       "  'layer',\n",
       "  'perceptrons',\n",
       "  'great',\n",
       "  'many',\n",
       "  'classification',\n",
       "  'regression',\n",
       "  'tasks',\n",
       "  'however',\n",
       "  'hard',\n",
       "  'mlps',\n",
       "  'clas'],\n",
       " ['written',\n",
       "  'jean',\n",
       "  'marc',\n",
       "  'valin',\n",
       "  'demo',\n",
       "  'presents',\n",
       "  'rnnoise',\n",
       "  'project',\n",
       "  'showing',\n",
       "  'applied',\n",
       "  'noise',\n",
       "  'suppression',\n",
       "  'main',\n",
       "  'id'],\n",
       " ['linear',\n",
       "  'regression',\n",
       "  'one',\n",
       "  'widely',\n",
       "  'used',\n",
       "  'statistical',\n",
       "  'models',\n",
       "  'continuous',\n",
       "  'variable',\n",
       "  'take',\n",
       "  'decimal',\n",
       "  'values',\n",
       "  'expected',\n",
       "  'linear'],\n",
       " ['another',\n",
       "  'example',\n",
       "  'multi',\n",
       "  'layer',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'rnn',\n",
       "  'package',\n",
       "  'designed',\n",
       "  'character',\n",
       "  'level',\n",
       "  'language',\n",
       "  'models',\n",
       "  'neural',\n",
       "  'network'],\n",
       " ['written',\n",
       "  'jon',\n",
       "  'krohn',\n",
       "  'untapt',\n",
       "  'models',\n",
       "  'involve',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'nlp',\n",
       "  'one',\n",
       "  'way',\n",
       "  'another',\n",
       "  'algorithms',\n",
       "  'consider',\n",
       "  'th'],\n",
       " ['know',\n",
       "  'models',\n",
       "  'always',\n",
       "  'illusion',\n",
       "  'reality',\n",
       "  'yet',\n",
       "  'may',\n",
       "  'may',\n",
       "  'useful',\n",
       "  'data',\n",
       "  'scientists',\n",
       "  'build',\n",
       "  'simple',\n",
       "  'complex',\n",
       "  'models',\n",
       "  'reasonab'],\n",
       " ['tuning', 'random', 'forest', 'model', 'predictive', 'modeling'],\n",
       " ['text',\n",
       "  'mining',\n",
       "  'hack',\n",
       "  'extraction',\n",
       "  'made',\n",
       "  'easy',\n",
       "  'using',\n",
       "  'google',\n",
       "  'api'],\n",
       " ['deploy', 'model', 'production', 'api', 'flask'],\n",
       " ['three', 'original', 'math', 'proba', 'challenges', 'data'],\n",
       " ['calibrated', 'classification', 'model', 'scikit'],\n",
       " ['caption',\n",
       "  'generation',\n",
       "  'inject',\n",
       "  'merge',\n",
       "  'architectures',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'model'],\n",
       " ['part', 'random', 'forest', 'vs', 'cart', 'model'],\n",
       " ['natural', 'language', 'processing', 'made', 'easy', 'using', 'spacy'],\n",
       " ['fast',\n",
       "  'combinatorial',\n",
       "  'feature',\n",
       "  'selection',\n",
       "  'new',\n",
       "  'definition',\n",
       "  'predictive',\n",
       "  'power',\n",
       "  'data'],\n",
       " ['make', 'manual', 'predictions', 'arima', 'models'],\n",
       " ['model', 'volatility', 'arch', 'garch', 'time', 'series', 'forecasting'],\n",
       " ['statistics', 'evaluating', 'models'],\n",
       " ['mistakes', 'programmers', 'make', 'starting'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'statistical',\n",
       "  'language',\n",
       "  'modeling',\n",
       "  'neural',\n",
       "  'language',\n",
       "  'models'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'challenge',\n",
       "  'training',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'models'],\n",
       " ['transfer', 'art', 'using', 'pre', 'trained', 'models'],\n",
       " ['visualize', 'neural', 'network', 'model', 'keras'],\n",
       " ['loss', 'loss', 'functions', 'training', 'neural', 'networks'],\n",
       " ['primer', 'generative', 'adversarial', 'networks', 'data'],\n",
       " ['gpus', 'necessary', 'training', 'models'],\n",
       " ['new', 'perspective', 'limit', 'theorem', 'statistical', 'testing', 'data'],\n",
       " ['deploy', 'predictive', 'model', 'production'],\n",
       " ['build', 'better', 'predictive', 'models', 'using', 'segmentation'],\n",
       " ['easy', 'questions', 'ensemble', 'modeling', 'everyone', 'know'],\n",
       " ['market', 'mix', 'modeling', 'sas', 'programming'],\n",
       " ['building', 'trust', 'models', 'using', 'lime'],\n",
       " ['evolution', 'models', 'data'],\n",
       " ['step', 'life', 'cycle', 'neural', 'network', 'models', 'keras'],\n",
       " ['save', 'arima', 'time', 'series', 'forecasting', 'model'],\n",
       " ['implementation',\n",
       "  'patterns',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'rnn',\n",
       "  'architecture',\n",
       "  'attention'],\n",
       " ['comparing', 'cart', 'model', 'random', 'forest'],\n",
       " ['lstm',\n",
       "  'model',\n",
       "  'architecture',\n",
       "  'rare',\n",
       "  'event',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['case',\n",
       "  'study',\n",
       "  'building',\n",
       "  'implementing',\n",
       "  'predictive',\n",
       "  'model',\n",
       "  'analytics',\n",
       "  'salary'],\n",
       " ['step',\n",
       "  'life',\n",
       "  'cycle',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'models',\n",
       "  'keras'],\n",
       " ['ensemble', 'codes'],\n",
       " ['clean', 'text'],\n",
       " ['reduce',\n",
       "  'variance',\n",
       "  'models',\n",
       "  'keras',\n",
       "  'using',\n",
       "  'model',\n",
       "  'averaging',\n",
       "  'ensembles'],\n",
       " ['metrics', 'keras'],\n",
       " ['prepare', 'text', 'data', 'keras'],\n",
       " ['check', 'point', 'models', 'keras'],\n",
       " ['handle', 'imbalanced', 'classification', 'problems'],\n",
       " ['gentle', 'introduction', 'bag', 'words', 'model'],\n",
       " ['intuitive',\n",
       "  'understanding',\n",
       "  'word',\n",
       "  'embeddings',\n",
       "  'count',\n",
       "  'vectors',\n",
       "  'word',\n",
       "  'vec'],\n",
       " ['must', 'read', 'sequence', 'modeling', 'attention', 'models'],\n",
       " ['develop', 'stacking', 'ensemble', 'neural', 'networks', 'keras'],\n",
       " ['build', 'recommendation', 'engine', 'scratch'],\n",
       " ['develop', 'snapshot', 'ensemble', 'neural', 'network', 'keras'],\n",
       " ['building', 'intelligent', 'chatbot', 'slack', 'using', 'dialogflow', 'api'],\n",
       " ['save', 'load', 'models', 'scikit'],\n",
       " ['develop', 'character', 'based', 'neural', 'language', 'model', 'keras'],\n",
       " ['develop', 'word', 'based', 'neural', 'language', 'models', 'keras'],\n",
       " ['ensemble', 'algorithms', 'scikit'],\n",
       " ['build', 'ensemble', 'models'],\n",
       " ['reduce', 'variance', 'final', 'model', 'horizontal', 'voting', 'ensemble'],\n",
       " ['dropout', 'regularization', 'models', 'keras'],\n",
       " ['make', 'predictions', 'scikit'],\n",
       " ['develop', 'first', 'xgboost', 'model', 'scikit'],\n",
       " ['making', 'predictions', 'sequences'],\n",
       " ['handle', 'missing', 'data'],\n",
       " ['develop', 'bidirectional', 'lstm', 'sequence', 'classification', 'keras'],\n",
       " ['develop', 'word', 'embeddings', 'gensim'],\n",
       " ['encoder', 'decoder', 'models', 'text', 'summarization', 'keras'],\n",
       " ['make', 'predictions', 'keras'],\n",
       " ['define',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'model',\n",
       "  'neural',\n",
       "  'translation',\n",
       "  'keras'],\n",
       " ['keras', 'models', 'scikit'],\n",
       " ['display', 'model', 'training', 'history', 'keras'],\n",
       " ['compare', 'algorithms', 'scikit'],\n",
       " ['handle', 'missing', 'timesteps', 'sequence', 'prediction', 'problems'],\n",
       " ['roc',\n",
       "  'curve',\n",
       "  'youre',\n",
       "  'trying',\n",
       "  'find',\n",
       "  'good',\n",
       "  'model',\n",
       "  'optimizes',\n",
       "  'trade',\n",
       "  'false',\n",
       "  'positive',\n",
       "  'rate',\n",
       "  'fpr',\n",
       "  'true',\n",
       "  'positive',\n",
       "  'rate',\n",
       "  'tpr',\n",
       "  'cou'],\n",
       " ['written',\n",
       "  'george',\n",
       "  'mcintire',\n",
       "  'lie',\n",
       "  'gets',\n",
       "  'halfway',\n",
       "  'around',\n",
       "  'world',\n",
       "  'truth',\n",
       "  'chance',\n",
       "  'get',\n",
       "  'pants',\n",
       "  'winston',\n",
       "  'churchill',\n",
       "  'since'],\n",
       " ['roc',\n",
       "  'curve',\n",
       "  'youre',\n",
       "  'trying',\n",
       "  'find',\n",
       "  'good',\n",
       "  'model',\n",
       "  'optimizes',\n",
       "  'trade',\n",
       "  'false',\n",
       "  'positive',\n",
       "  'rate',\n",
       "  'fpr',\n",
       "  'true',\n",
       "  'positive',\n",
       "  'rate',\n",
       "  'tpr',\n",
       "  'cou'],\n",
       " ['image', 'processing', 'neural', 'networks', 'intuition', 'part', 'data'],\n",
       " ['social', 'network', 'analysis', 'data'],\n",
       " ['image', 'processing', 'neural', 'networks', 'intuition', 'part', 'data'],\n",
       " ['accelerate', 'neural', 'networks', 'batch', 'normalization'],\n",
       " ['reduce',\n",
       "  'overfitting',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'using',\n",
       "  'weight',\n",
       "  'constraints',\n",
       "  'keras'],\n",
       " ['reduce',\n",
       "  'generalization',\n",
       "  'error',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'activity',\n",
       "  'regularization',\n",
       "  'keras'],\n",
       " ['configure', 'rate', 'hyperparameter', 'training', 'neural', 'networks'],\n",
       " ['encoder',\n",
       "  'decoder',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'models',\n",
       "  'neural',\n",
       "  'translation'],\n",
       " ['simple',\n",
       "  'intuition',\n",
       "  'overfitting',\n",
       "  'testing',\n",
       "  'training',\n",
       "  'data',\n",
       "  'bad',\n",
       "  'idea'],\n",
       " ['configure', 'number', 'layers', 'nodes', 'neural', 'network'],\n",
       " ['accelerate', 'training', 'neural', 'networks', 'batch', 'normalization'],\n",
       " ['intuition',\n",
       "  'behind',\n",
       "  'bias',\n",
       "  'variance',\n",
       "  'trade',\n",
       "  'lasso',\n",
       "  'ridge',\n",
       "  'regression',\n",
       "  'data'],\n",
       " ['introduction',\n",
       "  'bayesian',\n",
       "  'networks',\n",
       "  'jhonatan',\n",
       "  'de',\n",
       "  'souza',\n",
       "  'oliveira'],\n",
       " ['sequence', 'modeling', 'neural', 'networks', 'part', 'data'],\n",
       " ['tricks',\n",
       "  'configuring',\n",
       "  'backpropagation',\n",
       "  'train',\n",
       "  'better',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['difference', 'batch', 'epoch', 'neural', 'network'],\n",
       " ['understanding', 'neural', 'networks', 'neuron', 'rnn', 'cnn', 'data'],\n",
       " ['email',\n",
       "  'classification',\n",
       "  'relevant',\n",
       "  'labels',\n",
       "  'using',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'data'],\n",
       " ['creating',\n",
       "  'graph',\n",
       "  'application',\n",
       "  'neo',\n",
       "  'gephi',\n",
       "  'linkurious',\n",
       "  'js',\n",
       "  'data'],\n",
       " ['initialize', 'neural', 'network', 'random', 'weights'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'networks',\n",
       "  'experts'],\n",
       " ['neural', 'networks', 'tricks', 'trade', 'review'],\n",
       " ['mlp', 'cnn', 'rnn', 'neural', 'networks'],\n",
       " ['social',\n",
       "  'network',\n",
       "  'analysis',\n",
       "  'centrality',\n",
       "  'pagerank',\n",
       "  'hits',\n",
       "  'network',\n",
       "  'generation',\n",
       "  'models',\n",
       "  'link',\n",
       "  'prediction',\n",
       "  'data'],\n",
       " ['attention',\n",
       "  'work',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['stacked', 'long', 'short', 'term', 'memory', 'networks'],\n",
       " ['activation',\n",
       "  'regularization',\n",
       "  'reducing',\n",
       "  'generalization',\n",
       "  'error',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['train', 'neural', 'networks', 'noise', 'reduce', 'overfitting'],\n",
       " ['bayesian', 'inference', 'works', 'data'],\n",
       " ['training', 'neural', 'network', 'hard'],\n",
       " ['email',\n",
       "  'classification',\n",
       "  'relevant',\n",
       "  'labels',\n",
       "  'using',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'data'],\n",
       " ['reduce', 'overfitting', 'dropout', 'regularization', 'keras'],\n",
       " ['suitability',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'networks',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['handle',\n",
       "  'long',\n",
       "  'sequences',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['attention',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['mini',\n",
       "  'course',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'keras'],\n",
       " ['weight', 'regularization', 'reduce', 'overfitting', 'models'],\n",
       " ['introduction',\n",
       "  'regularization',\n",
       "  'reduce',\n",
       "  'overfitting',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['encoder', 'decoder', 'long', 'short', 'term', 'memory', 'networks'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'global',\n",
       "  'attention',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['finding',\n",
       "  'optimal',\n",
       "  'weights',\n",
       "  'ensemble',\n",
       "  'learner',\n",
       "  'using',\n",
       "  'neural',\n",
       "  'network'],\n",
       " ['keras', 'model', 'tuning', 'theano', 'neural', 'network', 'transfer'],\n",
       " ['food', 'thought', 'measure', 'influence', 'network'],\n",
       " ['pytorch',\n",
       "  'introduction',\n",
       "  'build',\n",
       "  'quick',\n",
       "  'accurate',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['essentials', 'sequence', 'sequence', 'modelling', 'attention', 'using'],\n",
       " ['artifical', 'neural', 'network', 'ann', 'works', 'simplified'],\n",
       " ['simple', 'introduction', 'facial', 'recognition', 'codes'],\n",
       " ['introductory',\n",
       "  'understand',\n",
       "  'anns',\n",
       "  'conceptualize',\n",
       "  'new',\n",
       "  'ideas',\n",
       "  'using',\n",
       "  'embedding'],\n",
       " ['essentials', 'visualizing', 'convolutional', 'neural', 'networks'],\n",
       " ['top', 'youtube', 'videos', 'neural', 'network'],\n",
       " ['understanding', 'recurrent', 'neural', 'networks', 'rnns', 'scratch'],\n",
       " ['beginners', 'jupyter', 'notebooks', 'data', 'tips', 'tricks'],\n",
       " ['understanding', 'inception', 'network', 'scratch', 'codes'],\n",
       " ['introduction', 'artificial', 'neural', 'network', 'simplified'],\n",
       " ['essentials', 'introduction', 'long', 'short', 'term', 'memory'],\n",
       " ['debugging', 'visualising', 'training', 'neural', 'network', 'tensorboard'],\n",
       " ['implementing', 'neural', 'networks', 'using', 'theano'],\n",
       " ['creating',\n",
       "  'human',\n",
       "  'memory',\n",
       "  'structures',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'rnn',\n",
       "  'build',\n",
       "  'nlu'],\n",
       " ['essentially',\n",
       "  'good',\n",
       "  'hypotheses',\n",
       "  'lead',\n",
       "  'decision',\n",
       "  'makers',\n",
       "  'like',\n",
       "  'new',\n",
       "  'better',\n",
       "  'ways',\n",
       "  'achieve',\n",
       "  'business',\n",
       "  'goals',\n",
       "  'need',\n",
       "  'make',\n",
       "  'decisions'],\n",
       " ['new',\n",
       "  'approach',\n",
       "  'missing',\n",
       "  'values',\n",
       "  'processing',\n",
       "  'bayesian',\n",
       "  'networks',\n",
       "  'download',\n",
       "  'white',\n",
       "  'paper',\n",
       "  'mb',\n",
       "  'abundance',\n",
       "  'big',\n",
       "  'data',\n",
       "  'field'],\n",
       " ['know',\n",
       "  'today',\n",
       "  'world',\n",
       "  'quick',\n",
       "  'results',\n",
       "  'insights',\n",
       "  'nobody',\n",
       "  'wants',\n",
       "  'spend',\n",
       "  'time',\n",
       "  'understanding',\n",
       "  'core',\n",
       "  'concepts',\n",
       "  'certain',\n",
       "  'statistical',\n",
       "  'terms'],\n",
       " ['summary',\n",
       "  'major',\n",
       "  'problem',\n",
       "  'chatbots',\n",
       "  'provide',\n",
       "  'information',\n",
       "  'knowledge',\n",
       "  'base',\n",
       "  'new',\n",
       "  'approach',\n",
       "  'makes'],\n",
       " ['teaches', 'data', 'exploration', 'detail'],\n",
       " ['develop',\n",
       "  'autoregression',\n",
       "  'forecast',\n",
       "  'model',\n",
       "  'household',\n",
       "  'electricity',\n",
       "  'consumption'],\n",
       " ['creating',\n",
       "  'flawless',\n",
       "  'winning',\n",
       "  'strategy',\n",
       "  'casino',\n",
       "  'blackjack',\n",
       "  'using',\n",
       "  'data'],\n",
       " ['implementing', 'faster', 'cnn', 'object', 'detection'],\n",
       " ['practicing', 'handling', 'structured', 'imbalanced', 'datasets'],\n",
       " ['difference', 'classification', 'regression'],\n",
       " ['study', 'factors', 'contributing', 'air', 'pollution'],\n",
       " ['develop',\n",
       "  'probabilistic',\n",
       "  'forecasting',\n",
       "  'model',\n",
       "  'predict',\n",
       "  'air',\n",
       "  'pollution',\n",
       "  'days'],\n",
       " ['develop',\n",
       "  'evaluate',\n",
       "  'naive',\n",
       "  'forecasting',\n",
       "  'household',\n",
       "  'electricity',\n",
       "  'consumption'],\n",
       " ['different', 'batch', 'sizes', 'training', 'predicting', 'lstms'],\n",
       " ['predict',\n",
       "  'whether',\n",
       "  'persons',\n",
       "  'eyes',\n",
       "  'open',\n",
       "  'closed',\n",
       "  'using',\n",
       "  'brain',\n",
       "  'waves'],\n",
       " ['difference', 'validation', 'datasets'],\n",
       " ['load',\n",
       "  'visualize',\n",
       "  'explore',\n",
       "  'complex',\n",
       "  'multivariate',\n",
       "  'multistep',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting',\n",
       "  'dataset'],\n",
       " ['discover', 'feature', 'engineering', 'engineer', 'features', 'get', 'good'],\n",
       " ['commonly', 'asked', 'interview', 'puzzles'],\n",
       " ['lot', 'solving', 'problems', 'start', 'series', 'materials', 'modern'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['posted',\n",
       "  'roger',\n",
       "  'huang',\n",
       "  'roger',\n",
       "  'huang',\n",
       "  'heads',\n",
       "  'growth',\n",
       "  'marketing',\n",
       "  'springboard',\n",
       "  'broke',\n",
       "  'career',\n",
       "  'data',\n",
       "  'analyzing',\n",
       "  'million',\n",
       "  'wort'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'hadoop',\n",
       "  'decision',\n",
       "  'trees'],\n",
       " ['data',\n",
       "  'centers',\n",
       "  'used',\n",
       "  'centralizing',\n",
       "  'operations',\n",
       "  'organizations',\n",
       "  'assist',\n",
       "  'management',\n",
       "  'storage',\n",
       "  'dissemination',\n",
       "  'organizations'],\n",
       " ['multi',\n",
       "  'armed',\n",
       "  'bandit',\n",
       "  'framework',\n",
       "  'problem',\n",
       "  'algorithms',\n",
       "  'solve',\n",
       "  'problem',\n",
       "  'going',\n",
       "  'discussed',\n",
       "  'problem',\n",
       "  'appeared',\n",
       "  'lab',\n",
       "  'ass'],\n",
       " ['summary',\n",
       "  'want',\n",
       "  'win',\n",
       "  'kaggle',\n",
       "  'competition',\n",
       "  'least',\n",
       "  'get',\n",
       "  'respectable',\n",
       "  'place',\n",
       "  'leaderboard',\n",
       "  'days',\n",
       "  'ensembles',\n",
       "  'lot',\n",
       "  'practit'],\n",
       " ['optimization',\n",
       "  'problem',\n",
       "  'associated',\n",
       "  'best',\n",
       "  'decision',\n",
       "  'effective',\n",
       "  'efficient',\n",
       "  'decisions',\n",
       "  'whether',\n",
       "  'worth',\n",
       "  'maximum',\n",
       "  'minimum',\n",
       "  'way',\n",
       "  'determin'],\n",
       " ['consider',\n",
       "  'problem',\n",
       "  'working',\n",
       "  'classification',\n",
       "  'problem',\n",
       "  'get',\n",
       "  'accuracy',\n",
       "  'happy',\n",
       "  'happiness'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['resource',\n",
       "  'part',\n",
       "  'series',\n",
       "  'specific',\n",
       "  'topics',\n",
       "  'related',\n",
       "  'data',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'decision',\n",
       "  'trees',\n",
       "  'ensembles'],\n",
       " ['naive', 'bayes'],\n",
       " ['naive', 'bayes'],\n",
       " ['reinforcement', 'part', 'challenges', 'considerations', 'data'],\n",
       " ['solving', 'image', 'processing', 'problems', 'libraries', 'part', 'data'],\n",
       " ['critical', 'values', 'statistical', 'hypothesis', 'testing', 'calculate'],\n",
       " ['image', 'processing', 'feature', 'selection', 'images'],\n",
       " ['analytics',\n",
       "  'vidhyas',\n",
       "  'journey',\n",
       "  'part',\n",
       "  'time',\n",
       "  'top',\n",
       "  'data',\n",
       "  'knowledge',\n",
       "  'portal'],\n",
       " ['improve', 'model', 'robustness', 'adding', 'noise'],\n",
       " ['preprocessing', 'covariance', 'matrix', 'image', 'whitening', 'data'],\n",
       " ['hypothesis'],\n",
       " ['gentle', 'introduction', 'statistical', 'power', 'power', 'analysis'],\n",
       " ['easy', 'steps', 'naive', 'bayes', 'algorithm'],\n",
       " ['white', 'noise', 'time', 'series'],\n",
       " ['clustering', 'cluster', 'analysis', 'data'],\n",
       " ['case',\n",
       "  'study',\n",
       "  'predicting',\n",
       "  'onset',\n",
       "  'diabetes',\n",
       "  'within',\n",
       "  'five',\n",
       "  'years',\n",
       "  'part'],\n",
       " ['case',\n",
       "  'study',\n",
       "  'predicting',\n",
       "  'onset',\n",
       "  'diabetes',\n",
       "  'within',\n",
       "  'five',\n",
       "  'years',\n",
       "  'part'],\n",
       " ['top', 'medium', 'handles', 'publications', 'follow', 'data'],\n",
       " ['gentle',\n",
       "  'introduction',\n",
       "  'expected',\n",
       "  'value',\n",
       "  'variance',\n",
       "  'covariance',\n",
       "  'numpy'],\n",
       " ['identify', 'outliers', 'data'],\n",
       " ['prepare', 'news', 'text', 'summarization'],\n",
       " ['statistical', 'hypothesis', 'tests', 'cheat', 'sheet'],\n",
       " ['statistics', 'identify', 'outliers', 'data'],\n",
       " ['better', 'naive', 'bayes', 'tips', 'get', 'naive', 'bayes', 'algorithm'],\n",
       " ['remove', 'trend', 'information', 'time', 'series', 'data'],\n",
       " ['simple',\n",
       "  'introduction',\n",
       "  'complex',\n",
       "  'stochastic',\n",
       "  'processes',\n",
       "  'part',\n",
       "  'data'],\n",
       " ['power', 'transforms', 'time', 'series', 'forecast', 'data'],\n",
       " ['written',\n",
       "  'jason',\n",
       "  'brownlee',\n",
       "  'artificial',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'two',\n",
       "  'main',\n",
       "  'hyperparameters',\n",
       "  'control',\n",
       "  'architecture',\n",
       "  'topology',\n",
       "  'network'],\n",
       " ['written',\n",
       "  'jason',\n",
       "  'brownlee',\n",
       "  'jason',\n",
       "  'editor',\n",
       "  'chief',\n",
       "  'com',\n",
       "  'masters',\n",
       "  'phd',\n",
       "  'artificial',\n",
       "  'intelligence'],\n",
       " ['post',\n",
       "  'summary',\n",
       "  'different',\n",
       "  'posts',\n",
       "  'outlier',\n",
       "  'detection',\n",
       "  'find',\n",
       "  'original',\n",
       "  'posts',\n",
       "  'detailed',\n",
       "  'implementation',\n",
       "  'links',\n",
       "  'dete'],\n",
       " ['recurrent',\n",
       "  'neural',\n",
       "  'nets',\n",
       "  'rnn',\n",
       "  'detect',\n",
       "  'features',\n",
       "  'sequential',\n",
       "  'data',\n",
       "  'time',\n",
       "  'series',\n",
       "  'data',\n",
       "  'examples',\n",
       "  'applications',\n",
       "  'made',\n",
       "  'using',\n",
       "  'rnn',\n",
       "  'anomaly',\n",
       "  'dete'],\n",
       " ['longer',\n",
       "  'available',\n",
       "  'version',\n",
       "  'still',\n",
       "  'available',\n",
       "  'dsc',\n",
       "  'resources',\n",
       "  'subscribe',\n",
       "  'newsletter',\n",
       "  'repository',\n",
       "  'data',\n",
       "  'scien'],\n",
       " ['previous',\n",
       "  'created',\n",
       "  'heatmaps',\n",
       "  'seattle',\n",
       "  'call',\n",
       "  'volume',\n",
       "  'various',\n",
       "  'time',\n",
       "  'periods',\n",
       "  'groupings',\n",
       "  'analysis',\n",
       "  'based',\n",
       "  'dataset',\n",
       "  'whic'],\n",
       " ['comes',\n",
       "  'algobeans',\n",
       "  'layman',\n",
       "  'tutorials',\n",
       "  'analytics',\n",
       "  'whenever',\n",
       "  'spot',\n",
       "  'trend',\n",
       "  'plotted',\n",
       "  'time',\n",
       "  'would',\n",
       "  'looking',\n",
       "  'time',\n",
       "  'series',\n",
       "  'de'],\n",
       " ['previous',\n",
       "  'post',\n",
       "  'provided',\n",
       "  'discussion',\n",
       "  'model',\n",
       "  'stacking',\n",
       "  'popular',\n",
       "  'approach',\n",
       "  'data',\n",
       "  'competitions',\n",
       "  'boosting',\n",
       "  'predictive',\n",
       "  'performance',\n",
       "  'since'],\n",
       " ['inspired',\n",
       "  'siggraph',\n",
       "  'paper',\n",
       "  'levin',\n",
       "  'et',\n",
       "  'al',\n",
       "  'took',\n",
       "  'patent',\n",
       "  'paper',\n",
       "  'referred',\n",
       "  'course',\n",
       "  'cs',\n",
       "  'cornell'],\n",
       " ['originally',\n",
       "  'appeared',\n",
       "  'ag',\n",
       "  'systems',\n",
       "  'ad',\n",
       "  'hoc',\n",
       "  'promise',\n",
       "  'world',\n",
       "  'businesses',\n",
       "  'live',\n",
       "  'die',\n",
       "  'data',\n",
       "  'self',\n",
       "  'serve',\n",
       "  'bi',\n",
       "  'inf'],\n",
       " ['originally',\n",
       "  'published',\n",
       "  'website',\n",
       "  'ever',\n",
       "  'competed',\n",
       "  'kaggle',\n",
       "  'competition',\n",
       "  'probably',\n",
       "  'familiar',\n",
       "  'combining',\n",
       "  'diffe'],\n",
       " ['made',\n",
       "  'implementation',\n",
       "  'mask',\n",
       "  'cnn',\n",
       "  'pytorch',\n",
       "  'frontend',\n",
       "  'based',\n",
       "  'pytorch',\n",
       "  'implementations',\n",
       "  'keras',\n",
       "  'implementatio'],\n",
       " ['image',\n",
       "  'created',\n",
       "  'kirk',\n",
       "  'borne',\n",
       "  'dsc',\n",
       "  'resources',\n",
       "  'career',\n",
       "  'training',\n",
       "  'books',\n",
       "  'cheat',\n",
       "  'sheet',\n",
       "  'apprenticeship',\n",
       "  'certification',\n",
       "  'salary',\n",
       "  'surveys',\n",
       "  'jobs',\n",
       "  'knowled'],\n",
       " ['written',\n",
       "  'mariya',\n",
       "  'yao',\n",
       "  'mariya',\n",
       "  'cto',\n",
       "  'head',\n",
       "  'research',\n",
       "  'amp',\n",
       "  'design',\n",
       "  'topbots',\n",
       "  'strategy',\n",
       "  'research',\n",
       "  'firm',\n",
       "  'applied',\n",
       "  'artificial'],\n",
       " ['implement', 'perceptron', 'algorithm', 'scratch'],\n",
       " ['implement', 'simple', 'linear', 'regression', 'scratch'],\n",
       " ['implement',\n",
       "  'linear',\n",
       "  'regression',\n",
       "  'stochastic',\n",
       "  'gradient',\n",
       "  'descent',\n",
       "  'scratch'],\n",
       " ['gentle', 'introduction', 'fold', 'cross', 'validation'],\n",
       " ['data', 'scratch'],\n",
       " ['implement', 'stacked', 'generalization', 'stacking', 'scratch'],\n",
       " ['simple', 'introduction', 'anova', 'applications', 'excel'],\n",
       " ['beginners', 'build', 'data', 'visualisations', 'web', 'js'],\n",
       " ['create', 'jaw', 'dropping', 'data', 'visualizations', 'web', 'js'],\n",
       " ['tree', 'based', 'modeling', 'scratch'],\n",
       " ['ridge', 'lasso', 'regression'],\n",
       " ['list', 'comprehension', 'examples'],\n",
       " ['beginners', 'linear', 'ridge', 'lasso', 'regression'],\n",
       " ['challenges', 'data', 'merging', 'subsetting', 'beginners'],\n",
       " ['web', 'scraping', 'using', 'scrapy', 'multiple', 'examples'],\n",
       " ['create',\n",
       "  'random',\n",
       "  'split',\n",
       "  'cross',\n",
       "  'validation',\n",
       "  'bagging',\n",
       "  'ensemble',\n",
       "  'keras'],\n",
       " ['implement', 'vector', 'quantization', 'scratch'],\n",
       " ['prepare', 'french', 'english', 'dataset', 'translation'],\n",
       " ['digital', 'marketing', 'analytics'],\n",
       " ['implement',\n",
       "  'logistic',\n",
       "  'regression',\n",
       "  'stochastic',\n",
       "  'gradient',\n",
       "  'descent',\n",
       "  'scratch'],\n",
       " ['logistic', 'regression'],\n",
       " ['building',\n",
       "  'random',\n",
       "  'forest',\n",
       "  'scratch',\n",
       "  'understanding',\n",
       "  'real',\n",
       "  'world',\n",
       "  'data',\n",
       "  'products',\n",
       "  'ml',\n",
       "  'programmers',\n",
       "  'part',\n",
       "  'analytics'],\n",
       " ['implement', 'random', 'forest', 'scratch'],\n",
       " ['logistic', 'regression'],\n",
       " ['bayesian', 'statistics', 'explained', 'simple', 'english', 'beginners'],\n",
       " ['long', 'short', 'term', 'memory', 'networks'],\n",
       " ['introduction', 'time', 'series', 'forecasting'],\n",
       " ['time', 'series', 'forecasting', 'keras'],\n",
       " ['dropout', 'lstm', 'networks', 'time', 'series', 'forecasting'],\n",
       " ['create', 'time', 'series', 'forecast', 'codes'],\n",
       " ['update', 'lstm', 'networks', 'training', 'time', 'series', 'forecasting'],\n",
       " ['basics', 'probability', 'data', 'explained', 'examples'],\n",
       " ['normalize', 'standardize', 'time', 'series', 'data'],\n",
       " ['autoregression', 'models', 'time', 'series', 'forecasting'],\n",
       " ['time', 'series', 'bundle'],\n",
       " ['binary', 'classification', 'keras', 'library'],\n",
       " [],\n",
       " ['time', 'series', 'forecasting'],\n",
       " ['indoor', 'movement', 'time', 'series', 'classification', 'algorithms'],\n",
       " [],\n",
       " ['time', 'series', 'bundle'],\n",
       " ['backtest', 'models', 'time', 'series', 'forecasting'],\n",
       " ['parameter', 'tuning', 'xgboost', 'codes'],\n",
       " ['check', 'time', 'series', 'data', 'stationary'],\n",
       " ['xgboost'],\n",
       " ['parameter', 'tuning', 'gradient', 'boosting', 'gbm'],\n",
       " ['covolutional', 'neural', 'networks', 'cnns'],\n",
       " ['prepare',\n",
       "  'univariate',\n",
       "  'time',\n",
       "  'series',\n",
       "  'data',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'networks'],\n",
       " ['understand', 'implement', 'text', 'classification'],\n",
       " ['improve',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecast',\n",
       "  'including',\n",
       "  'arima',\n",
       "  'holts',\n",
       "  'winter'],\n",
       " ['make', 'sample', 'forecasts', 'arima'],\n",
       " ['classical', 'time', 'series', 'forecasting', 'cheat', 'sheet'],\n",
       " ['stateful', 'stateless', 'lstm', 'time', 'series', 'forecasting'],\n",
       " ['classification', 'accuracy', 'enough', 'performance', 'measures'],\n",
       " ['features', 'lstm', 'networks', 'time', 'series', 'forecasting'],\n",
       " ['time',\n",
       "  'series',\n",
       "  'forecast',\n",
       "  'case',\n",
       "  'study',\n",
       "  'monthly',\n",
       "  'armed',\n",
       "  'robberies',\n",
       "  'boston'],\n",
       " ['useful', 'pandas', 'techniques', 'data', 'manipulation'],\n",
       " ['roc', 'curves', 'precision', 'recall', 'curves', 'classification'],\n",
       " ['time',\n",
       "  'series',\n",
       "  'forecast',\n",
       "  'case',\n",
       "  'study',\n",
       "  'annual',\n",
       "  'water',\n",
       "  'usage',\n",
       "  'baltimore'],\n",
       " ['top', 'bloggers', 'data', 'data'],\n",
       " ['echo',\n",
       "  'random',\n",
       "  'integers',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks'],\n",
       " ['neural',\n",
       "  'networks',\n",
       "  'hyperparameter',\n",
       "  'tuning',\n",
       "  'regularization',\n",
       "  'optimization'],\n",
       " ['comparison', 'deepnet', 'neuralnet', 'data'],\n",
       " ['make', 'predictions', 'long', 'short', 'term', 'memory', 'models', 'keras'],\n",
       " ['must', 'know', 'tips', 'tricks', 'neural', 'networks', 'data'],\n",
       " ['applications', 'markov', 'chain', 'data'],\n",
       " ['cnn', 'long', 'short', 'term', 'memory', 'networks'],\n",
       " ['ai', 'explained', 'simply', 'data'],\n",
       " ['data', 'preparation', 'gradient', 'boosting', 'xgboost'],\n",
       " ['grid',\n",
       "  'search',\n",
       "  'sarima',\n",
       "  'model',\n",
       "  'hyperparameters',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['tune', 'arima', 'parameters'],\n",
       " ['data', 'dimensionality', 'reduction', 'clustering', 'data'],\n",
       " ['development', 'art', 'hopscotch', 'robots', 'data'],\n",
       " ['applications', 'markov', 'chain', 'data'],\n",
       " ['introduction', 'feature', 'selection'],\n",
       " ['grid', 'search', 'arima', 'model', 'hyperparameters'],\n",
       " ['understanding', 'coding', 'neural', 'networks', 'scratch'],\n",
       " ['demonstration', 'memory', 'long', 'short', 'term', 'memory', 'network'],\n",
       " ['identify', 'remove', 'seasonality', 'time', 'series', 'data'],\n",
       " ['basic', 'feature', 'engineering', 'time', 'series', 'data'],\n",
       " ['resample', 'interpolate', 'time', 'series', 'data'],\n",
       " ['feature', 'importance', 'feature', 'selection', 'xgboost'],\n",
       " ['exploratory',\n",
       "  'configuration',\n",
       "  'multilayer',\n",
       "  'perceptron',\n",
       "  'network',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['timesteps', 'lstm', 'networks', 'time', 'series', 'forecasting'],\n",
       " ['strategies', 'multi', 'step', 'time', 'series', 'forecasting'],\n",
       " ['feature', 'selection', 'time', 'series', 'forecasting'],\n",
       " ['students', 'scratch'],\n",
       " ['decompose', 'time', 'series', 'data', 'trend', 'seasonality'],\n",
       " ['time', 'series', 'data', 'visualization'],\n",
       " ['promise',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['scale', 'data', 'long', 'short', 'term', 'memory', 'networks'],\n",
       " ['weight',\n",
       "  'regularization',\n",
       "  'lstm',\n",
       "  'networks',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['dimensionality', 'reduction', 'techniques'],\n",
       " ['random', 'forest', 'testing', 'classifiers', 'datasets'],\n",
       " ['spot', 'check', 'regression', 'algorithms', 'scikit'],\n",
       " ['answers', 'dozens', 'data', 'job', 'interview', 'questions', 'data'],\n",
       " ['standard', 'datasets', 'practicing', 'applied'],\n",
       " ['crash', 'course', 'multi', 'layer', 'perceptron', 'neural', 'networks'],\n",
       " ['data', 'understand', 'fight', 'cancer', 'data'],\n",
       " ['introduction', 'scikit'],\n",
       " ['build',\n",
       "  'multi',\n",
       "  'layer',\n",
       "  'perceptron',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'models',\n",
       "  'keras'],\n",
       " ['techniques',\n",
       "  'understand',\n",
       "  'algorithms',\n",
       "  'without',\n",
       "  'background',\n",
       "  'mathematics'],\n",
       " ['understand',\n",
       "  'difference',\n",
       "  'return',\n",
       "  'sequences',\n",
       "  'return',\n",
       "  'states',\n",
       "  'lstms',\n",
       "  'keras'],\n",
       " ['commonly',\n",
       "  'asked',\n",
       "  'questions',\n",
       "  'correlation',\n",
       "  'simple',\n",
       "  'linear',\n",
       "  'regression'],\n",
       " ['questions', 'understand', 'algorithm'],\n",
       " ['frequently', 'asked', 'questions', 'answers'],\n",
       " ['big', 'data', 'changing', 'world', 'soccer', 'data'],\n",
       " ['spot', 'check', 'classification', 'algorithms', 'scikit'],\n",
       " ['questions', 'understanding', 'logistic', 'regression'],\n",
       " ['one',\n",
       "  'popular',\n",
       "  'frameworks',\n",
       "  'used',\n",
       "  'data',\n",
       "  'scientists',\n",
       "  'rose',\n",
       "  'data',\n",
       "  'professional',\n",
       "  'practice',\n",
       "  'group',\n",
       "  'random',\n",
       "  'forests',\n",
       "  'random'],\n",
       " ['sat',\n",
       "  'across',\n",
       "  'interviewer',\n",
       "  'dream',\n",
       "  'job',\n",
       "  'may',\n",
       "  'start',\n",
       "  'feel',\n",
       "  'pressure',\n",
       "  'sure',\n",
       "  'fire',\n",
       "  'way',\n",
       "  'quash',\n",
       "  'interview',\n",
       "  'jitters',\n",
       "  'prepare',\n",
       "  'much'],\n",
       " ['written',\n",
       "  'koustuch',\n",
       "  'cv',\n",
       "  'tricks',\n",
       "  'series',\n",
       "  'post',\n",
       "  'shall',\n",
       "  'algorithm',\n",
       "  'image',\n",
       "  'segmentation',\n",
       "  'implementation'],\n",
       " ['nvidias',\n",
       "  'fastphotostyle',\n",
       "  'library',\n",
       "  'makes',\n",
       "  'easier',\n",
       "  'ever',\n",
       "  'transform',\n",
       "  'images',\n",
       "  'works',\n",
       "  'art',\n",
       "  'read',\n",
       "  'access',\n",
       "  'codes'],\n",
       " ['jitesh',\n",
       "  'shah',\n",
       "  'data',\n",
       "  'amp',\n",
       "  'stats',\n",
       "  'jockey',\n",
       "  'perpetual',\n",
       "  'beta',\n",
       "  'located',\n",
       "  'fremont',\n",
       "  'california',\n",
       "  'includes',\n",
       "  'data',\n",
       "  'set'],\n",
       " ['team',\n",
       "  'researchers',\n",
       "  'built',\n",
       "  'algorithm',\n",
       "  'detect',\n",
       "  'isolate',\n",
       "  'classify',\n",
       "  'historical',\n",
       "  'graffiti',\n",
       "  'using',\n",
       "  'techniques',\n",
       "  'like',\n",
       "  'cnn'],\n",
       " ['auto',\n",
       "  'keras',\n",
       "  'open',\n",
       "  'source',\n",
       "  'library',\n",
       "  'built',\n",
       "  'automated',\n",
       "  'based',\n",
       "  'popular',\n",
       "  'keras',\n",
       "  'package',\n",
       "  'could',\n",
       "  'game',\n",
       "  'changer',\n",
       "  'automl'],\n",
       " ['researchers',\n",
       "  'university',\n",
       "  'pittsburgh',\n",
       "  'built',\n",
       "  'model',\n",
       "  'generating',\n",
       "  'faces',\n",
       "  'appear',\n",
       "  'coming',\n",
       "  'different',\n",
       "  'types',\n",
       "  'ads'],\n",
       " ['naive', 'bayes', 'classifier', 'scratch'],\n",
       " ['implement', 'backpropagation', 'algorithm', 'scratch'],\n",
       " ['setup', 'environment', 'anaconda'],\n",
       " ['implement', 'decision', 'tree', 'algorithm', 'scratch'],\n",
       " ['one',\n",
       "  'typical',\n",
       "  'tasks',\n",
       "  'classification',\n",
       "  'tasks',\n",
       "  'may',\n",
       "  'seem',\n",
       "  'evaluating',\n",
       "  'effectiveness',\n",
       "  'model',\n",
       "  'easy',\n",
       "  'let',\n",
       "  'assume'],\n",
       " ['reading',\n",
       "  'academic',\n",
       "  'literature',\n",
       "  'text',\n",
       "  'analytics',\n",
       "  'seems',\n",
       "  'difficult',\n",
       "  'however',\n",
       "  'applying',\n",
       "  'practice',\n",
       "  'shown',\n",
       "  'us',\n",
       "  'text',\n",
       "  'classification',\n",
       "  'much',\n",
       "  'easier'],\n",
       " ['scikit',\n",
       "  'open',\n",
       "  'source',\n",
       "  'library',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'features',\n",
       "  'various',\n",
       "  'classification',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'algorithm'],\n",
       " ['involving',\n",
       "  'web',\n",
       "  'scrapping',\n",
       "  'predictive',\n",
       "  'modeling',\n",
       "  'classification',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'solve',\n",
       "  'anaytics',\n",
       "  'author',\n",
       "  'identification',\n",
       "  'challenge'],\n",
       " ['develop', 'lstm', 'models', 'time', 'series', 'forecasting'],\n",
       " ['timedistributed', 'layer', 'long', 'short', 'term', 'memory', 'networks'],\n",
       " ['multi',\n",
       "  'step',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'networks'],\n",
       " ['tune', 'lstm', 'hyperparameters', 'keras', 'time', 'series', 'forecasting'],\n",
       " ['develop',\n",
       "  'lstm',\n",
       "  'models',\n",
       "  'multi',\n",
       "  'step',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting',\n",
       "  'household',\n",
       "  'power',\n",
       "  'consumption'],\n",
       " ['develop',\n",
       "  'baseline',\n",
       "  'forecasts',\n",
       "  'multi',\n",
       "  'site',\n",
       "  'multivariate',\n",
       "  'air',\n",
       "  'pollution',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['add',\n",
       "  'numbers',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'lstm',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'network'],\n",
       " ['tactics', 'combat', 'imbalanced', 'classes', 'dataset'],\n",
       " ['multi',\n",
       "  'step',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting',\n",
       "  'household',\n",
       "  'electricity',\n",
       "  'consumption'],\n",
       " ['time', 'series', 'prediction', 'keras'],\n",
       " ['develop',\n",
       "  'multilayer',\n",
       "  'perceptron',\n",
       "  'models',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['implement', 'nearest', 'neighbors', 'scratch'],\n",
       " ['convert', 'time', 'series', 'supervised', 'problem'],\n",
       " ['time', 'series', 'forecasting', 'supervised'],\n",
       " ['grid', 'search', 'hyperparameters', 'models', 'keras'],\n",
       " ['reshape',\n",
       "  'input',\n",
       "  'data',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'networks',\n",
       "  'keras'],\n",
       " ['grid', 'search', 'models', 'time', 'series', 'forecasting'],\n",
       " ['grid',\n",
       "  'search',\n",
       "  'triple',\n",
       "  'exponential',\n",
       "  'smoothing',\n",
       "  'time',\n",
       "  'series',\n",
       "  'forecasting'],\n",
       " ['prepare', 'photo', 'caption', 'dataset', 'training', 'model'],\n",
       " ['dealing',\n",
       "  'plethora',\n",
       "  'data',\n",
       "  'information',\n",
       "  'world',\n",
       "  'today',\n",
       "  'expectation',\n",
       "  'predict',\n",
       "  'forecast',\n",
       "  'gain',\n",
       "  'competitive',\n",
       "  'advantage',\n",
       "  'based'],\n",
       " ['dealing',\n",
       "  'outliers',\n",
       "  'like',\n",
       "  'searching',\n",
       "  'needle',\n",
       "  'haystack',\n",
       "  'guest',\n",
       "  'repost',\n",
       "  'jacob',\n",
       "  'joseph',\n",
       "  'outlier',\n",
       "  'observation',\n",
       "  'poin'],\n",
       " ['using',\n",
       "  'platt',\n",
       "  'scaling',\n",
       "  'isotonic',\n",
       "  'regression',\n",
       "  'minimize',\n",
       "  'logloss',\n",
       "  'error'],\n",
       " ['non', 'linear', 'regression', 'decision', 'trees'],\n",
       " ['avoid',\n",
       "  'exploding',\n",
       "  'gradients',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'gradient',\n",
       "  'clipping'],\n",
       " ['build', 'simple', 'model', 'using', 'azureml'],\n",
       " ['framework', 'build', 'logistic', 'regression'],\n",
       " ['jackknife',\n",
       "  'logistic',\n",
       "  'linear',\n",
       "  'regression',\n",
       "  'clustering',\n",
       "  'predictions',\n",
       "  'data'],\n",
       " ['introduction', 'gradient', 'descent', 'algorithm', 'along', 'variants'],\n",
       " ['predicting', 'car', 'prices', 'part', 'linear', 'regression', 'data'],\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words_nostops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['prepp', 'interview', 'potential', 'interview', 'question', 'answer', 'help', 'crack', 'interview']]\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('answer', 1),\n",
       "  ('crack', 1),\n",
       "  ('help', 1),\n",
       "  ('interview', 3),\n",
       "  ('potential', 1),\n",
       "  ('prepp', 1),\n",
       "  ('question', 1)],\n",
       " [('cluster', 2),\n",
       "  ('clustering', 1),\n",
       "  ('common', 1),\n",
       "  ('feature', 1),\n",
       "  ('group', 1),\n",
       "  ('mean', 4),\n",
       "  ('purpose', 1),\n",
       "  ('similar', 1),\n",
       "  ('thing', 1)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=5,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.042*\"year\" + 0.023*\"particularly\" + 0.020*\"new\" + 0.019*\"testing\" + '\n",
      "  '0.018*\"base\" + 0.018*\"follow\" + 0.017*\"task\" + 0.016*\"component\" + '\n",
      "  '0.015*\"last\" + 0.015*\"post\"'),\n",
      " (1,\n",
      "  '0.045*\"start\" + 0.039*\"web\" + 0.022*\"open_source\" + 0.020*\"ask\" + '\n",
      "  '0.020*\"linkedin\" + 0.020*\"decision_tree\" + 0.019*\"help\" + 0.019*\"ml\" + '\n",
      "  '0.018*\"class\" + 0.018*\"visualize\"'),\n",
      " (2,\n",
      "  '0.080*\"introduction\" + 0.050*\"model\" + 0.035*\"write\" + 0.027*\"gentle\" + '\n",
      "  '0.025*\"linear\" + 0.024*\"implement\" + 0.024*\"error\" + 0.023*\"large\" + '\n",
      "  '0.022*\"residual\" + 0.020*\"adam\"'),\n",
      " (3,\n",
      "  '0.070*\"use\" + 0.067*\"datum\" + 0.040*\"algorithm\" + 0.037*\"data\" + '\n",
      "  '0.023*\"work\" + 0.022*\"post\" + 0.022*\"classification\" + 0.020*\"tensorflow\" + '\n",
      "  '0.020*\"explain\" + 0.016*\"world\"'),\n",
      " (4,\n",
      "  '0.117*\"regression\" + 0.053*\"understand\" + 0.038*\"scratch\" + 0.024*\"handle\" '\n",
      "  '+ 0.022*\"question\" + 0.021*\"scientist\" + 0.017*\"become\" + '\n",
      "  '0.016*\"case_study\" + 0.016*\"boost\" + 0.015*\"gradient\"'),\n",
      " (5,\n",
      "  '0.062*\"use\" + 0.053*\"library\" + 0.047*\"visualization\" + 0.036*\"recognition\" '\n",
      "  '+ 0.031*\"level\" + 0.024*\"train\" + 0.023*\"generate\" + 0.019*\"come\" + '\n",
      "  '0.013*\"make\" + 0.012*\"search\"'),\n",
      " (6,\n",
      "  '0.112*\"datum\" + 0.034*\"use\" + 0.027*\"know\" + 0.024*\"business\" + 0.021*\"lab\" '\n",
      "  '+ 0.020*\"js\" + 0.020*\"jupyter\" + 0.020*\"tool\" + 0.018*\"give\" + 0.018*\"way\"'),\n",
      " (7,\n",
      "  '0.153*\"network\" + 0.118*\"neural\" + 0.044*\"kera\" + 0.034*\"model\" + '\n",
      "  '0.034*\"develop\" + 0.025*\"generative\" + 0.019*\"convolutional\" + '\n",
      "  '0.017*\"analytic\" + 0.017*\"adversarial\" + 0.017*\"frequently\"'),\n",
      " (8,\n",
      "  '0.119*\"series\" + 0.112*\"time\" + 0.051*\"forecasting\" + 0.051*\"model\" + '\n",
      "  '0.027*\"part\" + 0.022*\"term_memory\" + 0.022*\"long_short\" + 0.021*\"dynamic\" + '\n",
      "  '0.018*\"predict\" + 0.015*\"sequence\"'),\n",
      " (9,\n",
      "  '0.065*\"image\" + 0.041*\"forecast\" + 0.035*\"dataset\" + 0.032*\"build\" + '\n",
      "  '0.024*\"statistical\" + 0.022*\"hypothesis\" + 0.019*\"value\" + 0.019*\"concept\" '\n",
      "  '+ 0.018*\"must\" + 0.017*\"document\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.622803096962387\n",
      "\n",
      "Coherence Score:  0.5482140016903182\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el11721112254945824992417318\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el11721112254945824992417318_data = {\"mdsDat\": {\"x\": [-0.24164473592050018, 0.18083161887752136, 0.1261715410544037, 0.14367152105279293, 0.04347129827926656, -0.18770767053571621, 0.018690081862478693, 0.008314647494413208, -0.10742889375000464, 0.01563059158534479], \"y\": [-0.0958605776984721, -0.2801899877280343, 0.008724819966565156, 0.00180860119231428, 0.15845792520662205, -0.08063168378998788, 0.0951177118057201, 0.11271425198538554, -0.001264427563405544, 0.0811233666232924], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [16.24451446533203, 13.74306869506836, 10.681095123291016, 10.445385932922363, 9.709444046020508, 9.69766616821289, 7.8331990242004395, 7.656472206115723, 7.3405256271362305, 6.6486334800720215]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Freq\": [180.0, 186.0, 139.0, 107.0, 101.0, 79.0, 74.0, 178.0, 141.0, 43.0, 46.0, 52.0, 36.0, 51.0, 33.0, 35.0, 64.0, 30.0, 40.0, 26.0, 27.0, 39.0, 26.0, 23.0, 23.0, 23.0, 32.0, 30.0, 25.0, 25.0, 51.268577575683594, 30.045717239379883, 31.604562759399414, 27.622333526611328, 27.47869300842285, 21.817584991455078, 19.900487899780273, 19.906164169311523, 19.893211364746094, 19.838172912597656, 17.737455368041992, 17.458892822265625, 17.397293090820312, 16.96251678466797, 15.245376586914062, 15.08832836151123, 15.016532897949219, 13.713627815246582, 13.39802074432373, 13.267773628234863, 12.983760833740234, 12.61181926727295, 12.55235767364502, 12.453262329101562, 12.374443054199219, 12.101763725280762, 12.058825492858887, 11.640352249145508, 11.486903190612793, 11.480125427246094, 55.566810607910156, 16.2930850982666, 97.96223449707031, 92.8735122680664, 30.687652587890625, 19.15414810180664, 180.19491577148438, 138.9482879638672, 52.249019622802734, 39.88432312011719, 29.642520904541016, 22.564252853393555, 20.260541915893555, 20.168519973754883, 19.771987915039062, 16.65454864501953, 13.150629997253418, 12.796903610229492, 12.267157554626465, 11.810748100280762, 11.496231079101562, 11.500265121459961, 11.410673141479492, 10.047795295715332, 9.698334693908691, 9.551192283630371, 9.550678253173828, 9.517498970031738, 9.177124977111816, 9.156843185424805, 9.014403343200684, 9.014345169067383, 9.014345169067383, 9.00991439819336, 8.490971565246582, 7.706147193908691, 40.317203521728516, 73.39583587646484, 24.920854568481445, 22.87538719177246, 22.280841827392578, 22.16897964477539, 20.6900691986084, 20.10232162475586, 18.55781364440918, 15.150294303894043, 13.618008613586426, 12.6900053024292, 12.485506057739258, 11.852598190307617, 11.482226371765137, 11.431069374084473, 10.91115665435791, 9.715841293334961, 9.641372680664062, 9.402626991271973, 9.23033618927002, 9.23033618927002, 9.23033618927002, 9.23033618927002, 9.230376243591309, 9.230376243591309, 9.230376243591309, 9.230376243591309, 7.987930774688721, 9.094786643981934, 7.3964362144470215, 32.320735931396484, 45.90937042236328, 10.501952171325684, 106.51720428466797, 100.67090606689453, 46.005149841308594, 19.4301700592041, 19.4301700592041, 19.229717254638672, 15.855661392211914, 13.395392417907715, 12.850579261779785, 11.629898071289062, 11.598694801330566, 10.871506690979004, 10.72533130645752, 9.597105979919434, 9.505143165588379, 9.472867965698242, 8.499499320983887, 8.489171028137207, 7.851457118988037, 6.745824813842773, 24.180692672729492, 6.270272731781006, 6.012944221496582, 8.685691833496094, 4.699183940887451, 4.640822887420654, 4.2437520027160645, 3.9848694801330566, 3.75101375579834, 3.5162336826324463, 5.783421039581299, 45.742652893066406, 4.286351203918457, 34.832550048828125, 18.7707576751709, 16.9599609375, 15.512950897216797, 14.899463653564453, 14.89427661895752, 14.238710403442383, 13.154004096984863, 12.818180084228516, 12.527250289916992, 12.067822456359863, 11.470973014831543, 11.45556926727295, 11.172467231750488, 11.024967193603516, 10.727622032165527, 9.83993148803711, 9.723418235778809, 9.734557151794434, 9.600317001342773, 9.419267654418945, 9.293516159057617, 9.292285919189453, 9.292285919189453, 9.290557861328125, 7.233239650726318, 6.884329319000244, 5.671060562133789, 4.988418102264404, 4.988418102264404, 7.1660895347595215, 12.699487686157227, 7.8494086265563965, 19.546836853027344, 17.626235961914062, 16.553701400756836, 16.32435417175293, 16.527267456054688, 15.219761848449707, 15.124784469604492, 14.835370063781738, 13.203473091125488, 11.931796073913574, 11.662857055664062, 11.533123016357422, 11.277848243713379, 10.783243179321289, 10.749351501464844, 10.435303688049316, 10.099226951599121, 8.0806884765625, 7.9949188232421875, 7.9949188232421875, 7.9949188232421875, 7.994826316833496, 7.994826316833496, 7.994826316833496, 7.993962287902832, 7.912499904632568, 6.245973587036133, 5.2142486572265625, 4.821716785430908, 4.790492057800293, 22.515737533569336, 93.17059326171875, 12.042668342590332, 28.26955795288086, 78.86812591552734, 35.838401794433594, 25.674802780151367, 15.923256874084473, 14.995126724243164, 13.855260848999023, 11.125943183898926, 10.907377243041992, 10.459733963012695, 9.958064079284668, 8.605320930480957, 6.999847888946533, 6.681280136108398, 5.905580043792725, 5.896829605102539, 5.352585315704346, 5.044241905212402, 5.013160705566406, 4.2936787605285645, 4.281235694885254, 3.834193229675293, 3.792734384536743, 3.441349983215332, 3.1563875675201416, 3.1538288593292236, 3.11599063873291, 3.0187573432922363, 2.9996697902679443, 2.9996697902679443, 2.889173984527588, 10.02518367767334, 4.448944568634033, 3.48988676071167, 3.1083176136016846, 42.6489143371582, 26.714855194091797, 22.761289596557617, 15.56876277923584, 14.684881210327148, 12.218910217285156, 11.48636531829834, 10.88288402557373, 10.806818008422852, 10.020538330078125, 10.011270523071289, 9.760147094726562, 9.373313903808594, 8.717249870300293, 8.43757152557373, 8.43836784362793, 8.43836784362793, 8.43836784362793, 12.46622085571289, 6.74839973449707, 6.3391499519348145, 5.8942341804504395, 5.873333930969238, 5.610446929931641, 5.5305962562561035, 5.380859851837158, 4.923285484313965, 4.919378757476807, 4.47976016998291, 4.382122039794922, 20.727249145507812, 9.978404998779297, 33.221168518066406, 29.315673828125, 22.53704261779785, 19.360950469970703, 15.06603717803955, 14.203903198242188, 11.779030799865723, 8.193774223327637, 7.651395320892334, 7.524206638336182, 7.378242015838623, 7.244961261749268, 7.244961261749268, 7.1643266677856445, 7.163754940032959, 6.584941387176514, 6.370234489440918, 6.1821441650390625, 6.181092262268066, 6.167199611663818, 6.1670637130737305, 6.116583347320557, 6.052231311798096, 6.052172660827637, 6.052172660827637, 6.052172660827637, 6.0520830154418945, 6.0520830154418945, 6.0520830154418945, 6.051220893859863, 6.051220893859863, 6.051220893859863, 39.142887115478516, 25.412927627563477, 22.46206283569336, 11.403641700744629, 11.394036293029785, 11.380928993225098, 10.782827377319336, 10.720602035522461, 10.435995101928711, 10.326623916625977, 10.192017555236816, 9.733352661132812, 8.975422859191895, 8.709285736083984, 8.325318336486816, 8.282698631286621, 8.236804962158203, 8.236804962158203, 8.236804962158203, 7.884115219116211, 7.158933639526367, 7.043356895446777, 6.08460807800293, 5.087782859802246, 4.63862419128418, 4.066772937774658, 3.9417519569396973, 3.9016454219818115, 3.778611898422241, 3.493290901184082, 3.169328451156616, 8.074514389038086, 9.778911590576172, 12.590611457824707, 9.496315956115723, 5.343138217926025], \"Term\": [\"network\", \"datum\", \"neural\", \"series\", \"time\", \"regression\", \"introduction\", \"use\", \"model\", \"image\", \"forecasting\", \"kera\", \"understand\", \"data\", \"library\", \"year\", \"algorithm\", \"visualization\", \"develop\", \"start\", \"forecast\", \"write\", \"scratch\", \"web\", \"recognition\", \"dataset\", \"know\", \"generative\", \"gentle\", \"build\", \"data\", \"classification\", \"work\", \"tensorflow\", \"explain\", \"world\", \"cluster\", \"cnn\", \"solve\", \"text\", \"popular\", \"feature\", \"find\", \"recurrent\", \"tree\", \"resource\", \"prediction\", \"computer\", \"insight\", \"publish\", \"algorithms\", \"application\", \"look\", \"focus\", \"recently\", \"detail\", \"unsupervised\", \"apis\", \"improve\", \"speech\", \"algorithm\", \"performance\", \"use\", \"datum\", \"post\", \"analysis\", \"network\", \"neural\", \"kera\", \"develop\", \"generative\", \"convolutional\", \"analytic\", \"adversarial\", \"frequently\", \"lstm\", \"support\", \"various\", \"translation\", \"core\", \"scikit\", \"picture\", \"hyperparameter\", \"interesting\", \"sound\", \"intuitive\", \"commerce\", \"artificial\", \"encoder_decoder\", \"detection\", \"inspir\", \"heidenreich\", \"hunter\", \"gan\", \"course\", \"day\", \"model\", \"introduction\", \"gentle\", \"linear\", \"implement\", \"error\", \"large\", \"residual\", \"adam\", \"provide\", \"cross_validation\", \"natural_language\", \"paper\", \"link\", \"weight\", \"statistic\", \"fit\", \"sentiment\", \"browser\", \"option\", \"vik\", \"json\", \"pain\", \"paruchuri\", \"invade\", \"like\", \"geitgey\", \"interested\", \"overfitt\", \"correct\", \"challenge\", \"write\", \"model\", \"processing\", \"series\", \"time\", \"forecasting\", \"long_short\", \"term_memory\", \"dynamic\", \"predict\", \"sequence\", \"multi\", \"explore\", \"classifier\", \"industry\", \"mix\", \"brief\", \"relate\", \"layer\", \"grid_search\", \"predictive\", \"modeling\", \"many\", \"part\", \"appear\", \"household\", \"code\", \"price\", \"attention\", \"consumption\", \"previous\", \"average\", \"right\", \"rnn\", \"model\", \"training\", \"year\", \"particularly\", \"new\", \"testing\", \"base\", \"follow\", \"task\", \"component\", \"last\", \"available\", \"however\", \"calculate\", \"correlation\", \"numpy\", \"department\", \"face\", \"matplotlib\", \"output\", \"practitioner\", \"abstract\", \"explanation\", \"rank\", \"wolfram\", \"mathematica\", \"apart\", \"number\", \"automate\", \"trend\", \"chain\", \"markov\", \"click\", \"post\", \"open_source\", \"business\", \"lab\", \"js\", \"tool\", \"jupyter\", \"give\", \"way\", \"simple\", \"may\", \"framework\", \"professional\", \"nonparametric\", \"create\", \"discuss\", \"optimize\", \"event\", \"segmentation\", \"cloud\", \"heart\", \"doctor\", \"making\", \"zeppelin\", \"apache\", \"secret\", \"telling\", \"thereby\", \"bayesian\", \"fundamental\", \"probability\", \"deal\", \"know\", \"datum\", \"problem\", \"use\", \"regression\", \"understand\", \"scratch\", \"handle\", \"question\", \"scientist\", \"become\", \"case_study\", \"boost\", \"logistic\", \"power\", \"random_forest\", \"review\", \"result\", \"stop\", \"topic\", \"go\", \"compare\", \"type\", \"art\", \"adaboost\", \"online\", \"student\", \"estimate\", \"analyze\", \"buy\", \"tuning\", \"lasso\", \"ridge\", \"bag\", \"gradient\", \"nlp\", \"get\", \"algorithm\", \"image\", \"forecast\", \"dataset\", \"statistical\", \"hypothesis\", \"concept\", \"must\", \"document\", \"step\", \"good\", \"significance\", \"launch\", \"high\", \"static\", \"beginner\", \"advance\", \"littl\", \"vital\", \"value\", \"test\", \"practice\", \"difference\", \"read\", \"arima\", \"implementation\", \"batch\", \"parameter\", \"size\", \"choose\", \"first\", \"build\", \"get\", \"library\", \"visualization\", \"recognition\", \"level\", \"train\", \"generate\", \"come\", \"make\", \"search\", \"perform\", \"memory\", \"monte\", \"carlo\", \"original\", \"expect\", \"selection\", \"software\", \"attempt\", \"outlier\", \"order\", \"zoom\", \"often\", \"alphago\", \"reg\", \"statsmodel\", \"scipy\", \"avai\", \"integrate\", \"ipython\", \"produce\", \"datacamp\", \"pdf\", \"use\", \"start\", \"web\", \"ask\", \"linkedin\", \"decision_tree\", \"help\", \"ml\", \"class\", \"visualize\", \"author\", \"com\", \"notebook\", \"xgboost\", \"best\", \"ensemble\", \"opt\", \"probl\", \"tempting\", \"tune\", \"easy\", \"rate\", \"object\", \"solution\", \"interview\", \"basic\", \"list\", \"stack\", \"commonly\", \"programming\", \"well\", \"technique\", \"originally\", \"open_source\", \"know\", \"algorithm\"], \"Total\": [180.0, 186.0, 139.0, 107.0, 101.0, 79.0, 74.0, 178.0, 141.0, 43.0, 46.0, 52.0, 36.0, 51.0, 33.0, 35.0, 64.0, 30.0, 40.0, 26.0, 27.0, 39.0, 26.0, 23.0, 23.0, 23.0, 32.0, 30.0, 25.0, 25.0, 51.9366569519043, 30.718685150146484, 32.32588577270508, 28.290576934814453, 28.146865844726562, 22.48577880859375, 20.568443298339844, 20.574420928955078, 20.561328887939453, 20.506242752075195, 18.405574798583984, 18.126890182495117, 18.065629959106445, 17.659343719482422, 15.913431167602539, 15.756396293640137, 15.684510231018066, 14.382311820983887, 14.06625747680664, 13.93669605255127, 13.651819229125977, 13.279826164245605, 13.220911979675293, 13.121665954589844, 13.044363021850586, 12.77169132232666, 12.72753620147705, 12.30938720703125, 12.154932022094727, 12.149038314819336, 64.86219787597656, 18.400676727294922, 178.90565490722656, 186.6434783935547, 48.80848693847656, 26.89356803894043, 180.87892150878906, 139.6322479248047, 52.93299102783203, 40.56836700439453, 30.326648712158203, 23.24820899963379, 20.944583892822266, 20.852651596069336, 20.45705223083496, 17.33859634399414, 13.835201263427734, 13.48188304901123, 12.95156478881836, 12.494953155517578, 12.18019962310791, 12.184640884399414, 12.094738006591797, 10.734699249267578, 10.383599281311035, 10.235273361206055, 10.235292434692383, 10.2014799118042, 9.861150741577148, 9.841062545776367, 9.699109077453613, 9.699257850646973, 9.699257850646973, 9.698888778686523, 9.174921035766602, 8.390263557434082, 141.79441833496094, 74.07813262939453, 25.602922439575195, 23.55756378173828, 22.9630069732666, 22.851200103759766, 21.372459411621094, 20.78461265563965, 19.240140914916992, 15.833680152893066, 14.300287246704102, 13.372154235839844, 13.167903900146484, 12.535811424255371, 12.16445255279541, 12.113204956054688, 11.593268394470215, 10.397994041442871, 10.325279235839844, 10.087287902832031, 9.913049697875977, 9.913049697875977, 9.913049697875977, 9.913049697875977, 9.91309928894043, 9.91309928894043, 9.91309928894043, 9.91309928894043, 8.670072555541992, 9.916948318481445, 8.078636169433594, 39.011985778808594, 141.79441833496094, 14.936469078063965, 107.19702911376953, 101.35073852539062, 46.68494415283203, 20.109996795654297, 20.109996795654297, 19.91019058227539, 16.53561019897461, 14.075233459472656, 13.530405044555664, 12.310708999633789, 12.27952766418457, 11.551579475402832, 11.405319213867188, 10.278009414672852, 10.185038566589355, 10.152734756469727, 9.179567337036133, 9.169012069702148, 8.531296730041504, 7.425979137420654, 26.751821517944336, 6.95026159286499, 6.6927947998046875, 9.675439834594727, 5.3790364265441895, 5.32066535949707, 4.9236040115356445, 4.665126323699951, 4.434961318969727, 4.196117401123047, 7.754594326019287, 141.79441833496094, 8.838665962219238, 35.51424789428711, 19.454681396484375, 17.641868591308594, 16.194631576538086, 15.581110954284668, 15.576017379760742, 14.920744895935059, 13.836113929748535, 13.499813079833984, 13.209051132202148, 12.749589920043945, 12.152541160583496, 12.137207984924316, 11.854447364807129, 11.706794738769531, 11.409421920776367, 10.523184776306152, 10.406492233276367, 10.419894218444824, 10.284012794494629, 10.107664108276367, 9.975350379943848, 9.975253105163574, 9.975253105163574, 9.975324630737305, 7.914849758148193, 7.566174030303955, 6.352757453918457, 5.669990062713623, 5.669990062713623, 10.717251777648926, 48.80848693847656, 21.051294326782227, 20.23960304260254, 18.318979263305664, 17.246814727783203, 17.018089294433594, 17.240310668945312, 15.912651062011719, 15.81765079498291, 15.52818489074707, 13.896284103393555, 12.624940872192383, 12.35558795928955, 12.226099014282227, 11.970643997192383, 11.476218223571777, 11.442381858825684, 11.128210067749023, 10.792000770568848, 8.773542404174805, 8.687784194946289, 8.687784194946289, 8.687784194946289, 8.687793731689453, 8.687793731689453, 8.687793731689453, 8.687850952148438, 8.668408393859863, 6.938709259033203, 5.907197952270508, 5.514421463012695, 5.483356475830078, 32.63409423828125, 186.6434783935547, 17.706127166748047, 178.90565490722656, 79.54322814941406, 36.52372741699219, 26.35635757446289, 16.598709106445312, 15.670134544372559, 14.530291557312012, 11.802160263061523, 11.58245849609375, 11.134725570678711, 10.63302993774414, 9.280357360839844, 7.6748762130737305, 7.356358528137207, 6.580578327178955, 6.574018478393555, 6.027969837188721, 5.719257354736328, 5.688144683837891, 4.968703269958496, 4.95631742477417, 4.509357929229736, 4.467789649963379, 4.1170806884765625, 3.831743001937866, 3.8288166522979736, 3.791102647781372, 3.694096326828003, 3.674769401550293, 3.674769401550293, 3.5642027854919434, 12.793816566467285, 6.939099311828613, 15.629008293151855, 64.86219787597656, 43.33798599243164, 27.403982162475586, 23.450347900390625, 16.257692337036133, 15.373785018920898, 12.907862663269043, 12.175525665283203, 11.571967124938965, 11.495756149291992, 10.709457397460938, 10.700234413146973, 10.449617385864258, 10.063637733459473, 9.411870956420898, 9.126483917236328, 9.12805461883545, 9.12805461883545, 9.12805461883545, 13.666386604309082, 7.437277793884277, 7.028260707855225, 6.5831475257873535, 6.562303066253662, 6.299356460571289, 6.219514846801758, 6.074270725250244, 5.612295627593994, 5.608321666717529, 5.168924331665039, 5.071013927459717, 25.971378326416016, 15.629008293151855, 33.93070602416992, 30.025161743164062, 23.2469425201416, 20.072214126586914, 15.775456428527832, 14.913413047790527, 12.488687515258789, 8.903188705444336, 8.360888481140137, 8.233646392822266, 8.088587760925293, 7.954343795776367, 7.954343795776367, 7.873781681060791, 7.87409782409668, 7.294626235961914, 7.080359935760498, 6.891757011413574, 6.890590190887451, 6.876815319061279, 6.876894474029541, 6.827153205871582, 6.762016773223877, 6.76199197769165, 6.76199197769165, 6.76199197769165, 6.762000560760498, 6.762000560760498, 6.762000560760498, 6.762402534484863, 6.762402534484863, 6.762402534484863, 178.90565490722656, 26.103755950927734, 23.153047561645508, 12.0945405960083, 12.085415840148926, 12.071660041809082, 11.473846435546875, 11.411477088928223, 11.12684440612793, 11.017536163330078, 10.882676124572754, 10.42501449584961, 9.667557716369629, 9.39991569519043, 9.016186714172363, 8.973368644714355, 8.927839279174805, 8.927839279174805, 8.927839279174805, 8.595879554748535, 7.849647045135498, 7.734045028686523, 6.775571823120117, 5.778443336486816, 5.329229354858398, 4.757418155670166, 4.6325201988220215, 4.592330455780029, 4.469306945800781, 4.183999061584473, 3.8600847721099854, 10.061948776245117, 12.77314281463623, 21.051294326782227, 32.63409423828125, 64.86219787597656], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.8044999837875366, 1.795300006866455, 1.794800043106079, 1.7934999465942383, 1.7934000492095947, 1.7871999740600586, 1.7843999862670898, 1.7843999862670898, 1.7843999862670898, 1.7842999696731567, 1.780400037765503, 1.7798999547958374, 1.7797000408172607, 1.7771999835968018, 1.7745000123977661, 1.7740999460220337, 1.773900032043457, 1.7697999477386475, 1.7687000036239624, 1.7682000398635864, 1.767199993133545, 1.7657999992370605, 1.7654999494552612, 1.7651000022888184, 1.7647000551223755, 1.7634999752044678, 1.7633999586105347, 1.7615000009536743, 1.7609000205993652, 1.7608000040054321, 1.6627000570297241, 1.6957999467849731, 1.2151000499725342, 1.1195000410079956, 1.3533999919891357, 1.4780000448226929, 1.9808000326156616, 1.979699969291687, 1.97160005569458, 1.9675999879837036, 1.9617999792099, 1.954800009727478, 1.9514000415802002, 1.951300024986267, 1.950600028038025, 1.9443999528884888, 1.933899998664856, 1.9325000047683716, 1.930299997329712, 1.9283000230789185, 1.926800012588501, 1.926800012588501, 1.9263999462127686, 1.9184999465942383, 1.9163999557495117, 1.9155000448226929, 1.9154000282287598, 1.9151999950408936, 1.9127000570297241, 1.912600040435791, 1.9113999605178833, 1.9113999605178833, 1.9113999605178833, 1.9108999967575073, 1.9071999788284302, 1.8996000289916992, 0.7269999980926514, 2.227400064468384, 2.209700107574463, 2.2072999477386475, 2.2065000534057617, 2.206399917602539, 2.204200029373169, 2.2032999992370605, 2.2005999088287354, 2.1926000118255615, 2.1877999305725098, 2.184299945831299, 2.183500051498413, 2.1807000637054443, 2.178999900817871, 2.1786999702453613, 2.176100015640259, 2.168800115585327, 2.1682000160217285, 2.1663999557495117, 2.165299892425537, 2.165299892425537, 2.165299892425537, 2.165299892425537, 2.165299892425537, 2.165299892425537, 2.165299892425537, 2.165299892425537, 2.1547000408172607, 2.1501998901367188, 2.1484999656677246, 2.0485000610351562, 1.1089999675750732, 1.8845000267028809, 2.2525999546051025, 2.2523000240325928, 2.24429988861084, 2.224600076675415, 2.224600076675415, 2.2242000102996826, 2.2170000076293945, 2.2095000743865967, 2.2074999809265137, 2.2021000385284424, 2.2019999027252197, 2.1982998847961426, 2.197499990463257, 2.190500020980835, 2.1898999214172363, 2.18969988822937, 2.181999921798706, 2.181999921798706, 2.1760001182556152, 2.162899971008301, 2.1579999923706055, 2.156100034713745, 2.151900053024292, 2.151099920272827, 2.1238999366760254, 2.122299909591675, 2.1103999614715576, 2.1013998985290527, 2.0915000438690186, 2.082200050354004, 1.9657000303268433, 1.1276999711990356, 1.5353000164031982, 2.312700033187866, 2.296299934387207, 2.2927000522613525, 2.289099931716919, 2.2873001098632812, 2.2873001098632812, 2.2853000164031982, 2.2815001010894775, 2.2802999019622803, 2.279099941253662, 2.277100086212158, 2.274399995803833, 2.2743000984191895, 2.2727999687194824, 2.2720999717712402, 2.2704999446868896, 2.264899969100952, 2.26419997215271, 2.2639999389648438, 2.2632999420166016, 2.2614998817443848, 2.2613000869750977, 2.2611000537872314, 2.2611000537872314, 2.260999917984009, 2.242000102996826, 2.237600088119507, 2.218600034713745, 2.2039999961853027, 2.2039999961853027, 1.9296000003814697, 0.9857000112533569, 1.3454999923706055, 2.2985000610351562, 2.2946999073028564, 2.29229998588562, 2.2916998863220215, 2.2909998893737793, 2.288800001144409, 2.2885000705718994, 2.287600040435791, 2.282099962234497, 2.2767999172210693, 2.275599956512451, 2.274899959564209, 2.273699998855591, 2.2709999084472656, 2.2708001136779785, 2.2690000534057617, 2.266900062561035, 2.250999927520752, 2.250200033187866, 2.250200033187866, 2.250200033187866, 2.250200033187866, 2.250200033187866, 2.250200033187866, 2.25, 2.242000102996826, 2.228100061416626, 2.2084999084472656, 2.1989998817443848, 2.198199987411499, 1.9621000289916992, 1.6384999752044678, 1.9478000402450562, 0.48820000886917114, 2.538300037384033, 2.527899980545044, 2.5206000804901123, 2.5053000450134277, 2.5027999877929688, 2.4992001056671143, 2.487799882888794, 2.4867000579833984, 2.484299898147583, 2.4811999797821045, 2.471299886703491, 2.454699993133545, 2.450500011444092, 2.4386000633239746, 2.4381000995635986, 2.427999973297119, 2.4212000370025635, 2.4205000400543213, 2.4007999897003174, 2.400399923324585, 2.3845999240875244, 2.382999897003174, 2.367500066757202, 2.3529000282287598, 2.3529000282287598, 2.3506999015808105, 2.344899892807007, 2.3438000679016113, 2.3438000679016113, 2.3368000984191895, 2.3029000759124756, 2.102299928665161, 1.0475000143051147, -0.49140000343322754, 2.5536000728607178, 2.5441999435424805, 2.539799928665161, 2.5262999534606934, 2.5237998962402344, 2.5148000717163086, 2.511399984359741, 2.50819993019104, 2.5078001022338867, 2.5030999183654785, 2.5030999183654785, 2.5013999938964844, 2.4986000061035156, 2.493000030517578, 2.4911000728607178, 2.4911000728607178, 2.4911000728607178, 2.4911000728607178, 2.4776999950408936, 2.472399950027466, 2.466399908065796, 2.4591000080108643, 2.458699941635132, 2.4537999629974365, 2.452199935913086, 2.4484000205993652, 2.4386000633239746, 2.438499927520752, 2.426500082015991, 2.4235999584198, 2.344099998474121, 2.1208999156951904, 2.59060001373291, 2.5878000259399414, 2.580699920654297, 2.575700044631958, 2.565700054168701, 2.562999963760376, 2.553299903869629, 2.528700113296509, 2.523099899291992, 2.521699905395508, 2.5197999477386475, 2.5183000564575195, 2.5183000564575195, 2.5172998905181885, 2.517199993133545, 2.509399890899658, 2.5060999393463135, 2.5030999183654785, 2.5030999183654785, 2.5027999877929688, 2.5027999877929688, 2.5018999576568604, 2.5009000301361084, 2.5009000301361084, 2.5009000301361084, 2.5009000301361084, 2.5007998943328857, 2.5007998943328857, 2.5007998943328857, 2.5006000995635986, 2.5006000995635986, 2.5006000995635986, 1.0921000242233276, 2.6839001178741455, 2.680500030517578, 2.651900053024292, 2.6517999172210693, 2.6517999172210693, 2.6486001014709473, 2.6482999324798584, 2.646699905395508, 2.6459999084472656, 2.64520001411438, 2.6421000957489014, 2.6364998817443848, 2.634399890899658, 2.63100004196167, 2.63070011138916, 2.630199909210205, 2.630199909210205, 2.630199909210205, 2.624300003051758, 2.6187000274658203, 2.6171998977661133, 2.6031999588012695, 2.5834999084472656, 2.572000026702881, 2.5539000034332275, 2.549299955368042, 2.547800064086914, 2.5429000854492188, 2.5302999019622803, 2.5136001110076904, 2.4907000064849854, 2.4435999393463135, 2.196700096130371, 1.4763000011444092, 0.2143000066280365], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.3013999462127686, -3.8357999324798584, -3.7852001190185547, -3.9198999404907227, -3.925100088119507, -4.155799865722656, -4.247700214385986, -4.247499942779541, -4.2480998039245605, -4.250899791717529, -4.362800121307373, -4.378600120544434, -4.382199764251709, -4.40749979019165, -4.514200210571289, -4.524600028991699, -4.529300212860107, -4.620100021362305, -4.643400192260742, -4.653200149536133, -4.674799919128418, -4.70389986038208, -4.708600044250488, -4.7164998054504395, -4.722899913787842, -4.745100021362305, -4.748700141906738, -4.783999919891357, -4.797299861907959, -4.797900199890137, -3.220900058746338, -4.447800159454346, -2.653899908065796, -2.7072999477386475, -3.8145999908447266, -4.285999774932861, -1.8772000074386597, -2.137200117111206, -3.1152000427246094, -3.3852999210357666, -3.6821000576019287, -3.954900026321411, -4.062600135803223, -4.0671000480651855, -4.086999893188477, -4.258600234985352, -4.494800090789795, -4.52209997177124, -4.564300060272217, -4.60230016708374, -4.629300117492676, -4.628900051116943, -4.63670015335083, -4.763899803161621, -4.799300193786621, -4.814599990844727, -4.814700126647949, -4.8180999755859375, -4.854599952697754, -4.856800079345703, -4.872399806976318, -4.872499942779541, -4.872499942779541, -4.872900009155273, -4.932300090789795, -5.029300212860107, -3.374500036239624, -2.5232999324798584, -3.6034998893737793, -3.6891000270843506, -3.7155001163482666, -3.7204999923706055, -3.789599895477295, -3.8183999061584473, -3.8982999324798584, -4.101200103759766, -4.207799911499023, -4.27839994430542, -4.294600009918213, -4.346700191497803, -4.378399848937988, -4.382900238037109, -4.4293999671936035, -4.545499801635742, -4.553100109100342, -4.578199863433838, -4.596700191497803, -4.596700191497803, -4.596700191497803, -4.596700191497803, -4.596700191497803, -4.596700191497803, -4.596700191497803, -4.596700191497803, -4.741300106048584, -4.611499786376953, -4.81820011138916, -3.3434998989105225, -2.992500066757202, -4.467599868774414, -2.1285998821258545, -2.184999942779541, -2.968100070953369, -3.8301000595092773, -3.8301000595092773, -3.840399980545044, -4.033400058746338, -4.202000141143799, -4.243500232696533, -4.343299865722656, -4.3460001945495605, -4.410799980163574, -4.424300193786621, -4.535399913787842, -4.545100212097168, -4.548500061035156, -4.656899929046631, -4.658100128173828, -4.736199855804443, -4.888000011444092, -3.611299991607666, -4.961100101470947, -5.002999782562256, -4.635200023651123, -5.249499797821045, -5.26200008392334, -5.351399898529053, -5.414400100708008, -5.474899768829346, -5.5395002365112305, -5.041900157928467, -2.973900079727173, -5.3414998054504395, -3.17330002784729, -3.7915000915527344, -3.8929998874664307, -3.9821999073028564, -4.022500038146973, -4.022900104522705, -4.06790018081665, -4.14709997177124, -4.172999858856201, -4.195899963378906, -4.23330020904541, -4.283999919891357, -4.285399913787842, -4.310400009155273, -4.323699951171875, -4.35099983215332, -4.437399864196777, -4.4492998123168945, -4.448200225830078, -4.461999893188477, -4.481100082397461, -4.494500160217285, -4.494699954986572, -4.494699954986572, -4.494800090789795, -4.745100021362305, -4.794600009918213, -4.988500118255615, -5.116700172424316, -5.116700172424316, -4.754499912261963, -4.182300090789795, -4.663400173187256, -3.749799966812134, -3.8531999588012695, -3.9159998893737793, -3.930000066757202, -3.91759991645813, -4.0, -4.00629997253418, -4.025599956512451, -4.142099857330322, -4.2434000968933105, -4.266200065612793, -4.277400016784668, -4.299799919128418, -4.344600200653076, -4.347799777984619, -4.377399921417236, -4.410200119018555, -4.6331000328063965, -4.643799781799316, -4.643799781799316, -4.643799781799316, -4.643799781799316, -4.643799781799316, -4.643799781799316, -4.643899917602539, -4.654200077056885, -4.890699863433838, -5.071199893951416, -5.149499893188477, -5.156000137329102, -3.6084001064300537, -2.188199996948242, -4.2342000007629395, -3.3808000087738037, -2.1412999629974365, -2.9300999641418457, -3.2636001110076904, -3.741300106048584, -3.8013999462127686, -3.8803999423980713, -4.099800109863281, -4.119699954986572, -4.161600112915039, -4.210700035095215, -4.3566999435424805, -4.563199996948242, -4.609799861907959, -4.7332000732421875, -4.7347002029418945, -4.831500053405762, -4.890900135040283, -4.896999835968018, -5.052000045776367, -5.054900169372559, -5.16510009765625, -5.176000118255615, -5.273200035095215, -5.3597002029418945, -5.360499858856201, -5.372600078582764, -5.404300212860107, -5.410600185394287, -5.410600185394287, -5.4481000900268555, -4.203999996185303, -5.01639986038208, -5.259200096130371, -5.375, -2.733299970626831, -3.2011001110076904, -3.3612000942230225, -3.740999937057495, -3.799499988555908, -3.983299970626831, -4.045100212097168, -4.099100112915039, -4.106100082397461, -4.181700229644775, -4.182600021362305, -4.208000183105469, -4.2484002113342285, -4.321000099182129, -4.353600025177002, -4.353499889373779, -4.353499889373779, -4.353499889373779, -3.9632999897003174, -4.577000141143799, -4.639500141143799, -4.712299823760986, -4.71589994430542, -4.76170015335083, -4.776000022888184, -4.803400039672852, -4.892300128936768, -4.893099784851074, -4.986700057983398, -5.008800029754639, -3.4547998905181885, -4.1859002113342285, -2.940999984741211, -3.065999984741211, -3.3289999961853027, -3.4809000492095947, -3.7316999435424805, -3.790600061416626, -3.977799892425537, -4.340799808502197, -4.409299850463867, -4.426000118255615, -4.4456000328063965, -4.463799953460693, -4.463799953460693, -4.474999904632568, -4.475100040435791, -4.5594000816345215, -4.59250020980835, -4.622499942779541, -4.622700214385986, -4.624899864196777, -4.624899864196777, -4.6331000328063965, -4.643700122833252, -4.643700122833252, -4.643700122833252, -4.643700122833252, -4.643700122833252, -4.643700122833252, -4.643700122833252, -4.643899917602539, -4.643899917602539, -4.643899917602539, -2.776900053024292, -3.1098999977111816, -3.233299970626831, -3.9112000465393066, -3.912100076675415, -3.9131999015808105, -3.9672000408172607, -3.9730000495910645, -3.9999001026153564, -4.01039981842041, -4.023499965667725, -4.0696001052856445, -4.150700092315674, -4.180799961090088, -4.225800037384033, -4.230999946594238, -4.236499786376953, -4.236499786376953, -4.236499786376953, -4.280300140380859, -4.376800060272217, -4.393099784851074, -4.539400100708008, -4.718299865722656, -4.810699939727783, -4.942299842834473, -4.973499774932861, -4.983699798583984, -5.0157999992370605, -5.094299793243408, -5.1915998458862305, -4.256400108337402, -4.064899921417236, -3.8122000694274902, -4.094200134277344, -4.669300079345703]}, \"token.table\": {\"Topic\": [5, 7, 3, 8, 2, 1, 7, 10, 1, 9, 1, 3, 6, 2, 7, 6, 5, 1, 4, 1, 8, 7, 2, 10, 9, 4, 10, 5, 9, 5, 4, 7, 5, 10, 8, 6, 7, 8, 10, 7, 4, 3, 6, 8, 6, 7, 5, 9, 7, 5, 3, 8, 10, 1, 4, 2, 5, 6, 1, 1, 4, 10, 9, 2, 10, 7, 5, 1, 8, 4, 2, 2, 3, 5, 2, 6, 3, 1, 9, 8, 1, 6, 2, 6, 10, 5, 1, 2, 2, 8, 6, 6, 8, 4, 10, 2, 10, 3, 7, 6, 9, 1, 5, 4, 5, 1, 1, 8, 3, 1, 5, 8, 4, 6, 2, 6, 2, 3, 9, 2, 3, 6, 7, 8, 6, 7, 8, 3, 7, 4, 7, 6, 2, 10, 8, 4, 5, 2, 2, 8, 8, 3, 8, 1, 4, 1, 2, 9, 3, 2, 10, 3, 2, 3, 9, 6, 3, 6, 2, 6, 10, 6, 3, 7, 5, 8, 4, 9, 9, 3, 3, 3, 10, 10, 8, 7, 4, 1, 2, 9, 6, 4, 5, 5, 5, 6, 9, 4, 10, 1, 2, 3, 4, 6, 7, 10, 4, 9, 4, 8, 3, 2, 2, 5, 1, 7, 6, 10, 5, 5, 10, 9, 7, 5, 10, 10, 6, 3, 9, 9, 1, 10, 9, 5, 3, 3, 3, 8, 4, 10, 5, 3, 9, 9, 1, 3, 2, 1, 1, 5, 6, 10, 7, 8, 5, 4, 1, 4, 4, 4, 6, 10, 1, 6, 3, 8, 9, 6, 10, 3, 1, 7, 7, 5, 10, 8, 1, 9, 1, 9, 7, 4, 3, 1, 7, 7, 7, 4, 4, 9, 7, 2, 9, 7, 9, 6, 6, 9, 3, 4, 4, 8, 6, 8, 9, 10, 1, 2, 1, 10, 10, 8, 3, 8, 9, 8, 7, 7, 2, 5, 2, 10, 6, 10, 1, 4, 8, 5, 1, 6, 4, 6, 7, 9, 4, 5, 2, 1, 5, 10, 7, 7, 7, 1, 1, 2, 3, 6, 7, 8, 9, 3, 8, 2, 3, 9, 10, 8, 6, 10, 3, 10, 5, 1, 1, 3, 5, 10, 5, 6, 9], \"Freq\": [0.9723830819129944, 0.8870442509651184, 0.9875187277793884, 0.876418948173523, 0.9591106176376343, 0.863368809223175, 0.04625190049409866, 0.07708650082349777, 0.952254056930542, 0.8873092532157898, 0.7064886093139648, 0.18591806292533875, 0.0743672251701355, 0.9549008011817932, 0.783531904220581, 0.9208321571350098, 0.9022262692451477, 0.974865734577179, 0.8632768392562866, 0.9789285063743591, 0.9524782299995422, 0.8070508241653442, 0.9802499413490295, 0.9095012545585632, 0.8706052899360657, 0.9397320747375488, 0.9188916087150574, 0.925170361995697, 0.8873113989830017, 0.9841736555099487, 0.9019244313240051, 0.841702938079834, 0.9627041220664978, 0.840792179107666, 0.8231440782546997, 0.8647141456604004, 0.9320327639579773, 0.8765698075294495, 0.8872931003570557, 0.8980912566184998, 0.9729510545730591, 0.9684967994689941, 0.19251962006092072, 0.8085824251174927, 0.9881616830825806, 0.7913265228271484, 0.9051604866981506, 0.8800222873687744, 0.9497120380401611, 0.8818357586860657, 0.8664829134941101, 0.7738553881645203, 0.8987274169921875, 0.9766042828559875, 0.977236270904541, 0.2799225151538849, 0.6531525254249573, 0.9118323922157288, 0.9723633527755737, 0.9720808267593384, 0.9301902651786804, 0.9592312574386597, 0.9608696103096008, 0.9770116806030273, 0.8949933648109436, 0.8790212273597717, 0.9395701885223389, 0.9734179377555847, 0.9296659231185913, 0.8124130368232727, 0.9893234968185425, 0.9603877663612366, 0.9075372219085693, 0.9063040018081665, 0.8719421029090881, 0.9189146161079407, 0.9790013432502747, 0.9819654226303101, 0.8872586488723755, 0.980795681476593, 0.4982761740684509, 0.4982761740684509, 0.9534861445426941, 0.9118502736091614, 0.9112251400947571, 0.9396252632141113, 0.9395779967308044, 0.914535403251648, 0.9859898686408997, 0.9114181399345398, 0.9585039019584656, 0.920833170413971, 0.9505730271339417, 0.954285204410553, 0.8917598128318787, 0.9126724004745483, 0.8915269374847412, 0.9627503156661987, 0.7829335331916809, 0.8986170887947083, 0.8889907598495483, 0.9592542052268982, 0.8904134631156921, 0.9747610688209534, 0.9641154408454895, 0.9378332495689392, 0.9410133957862854, 0.7887969017028809, 0.9488264918327332, 0.9145179986953735, 0.9630189538002014, 0.9852582812309265, 0.9853283762931824, 0.9504994750022888, 0.9776579737663269, 0.8464249968528748, 0.9279413819313049, 0.9078896045684814, 0.9387522339820862, 0.9892289638519287, 0.97645103931427, 0.06398358941078186, 0.1919507533311844, 0.6398358345031738, 0.9426462054252625, 0.8742393851280212, 0.9337541460990906, 0.15632551908493042, 0.7816275954246521, 0.8715007901191711, 0.9639303684234619, 0.920833170413971, 0.9279060363769531, 0.958702027797699, 0.8943088054656982, 0.8964864611625671, 0.9412067532539368, 0.9279060363769531, 0.9094864130020142, 0.9756868481636047, 0.9922011494636536, 0.9580627083778381, 0.9647054672241211, 0.9049824476242065, 0.9522507190704346, 0.9241974949836731, 0.9279202818870544, 0.8873113989830017, 0.9078896045684814, 0.9315584897994995, 0.9382219314575195, 0.985446035861969, 0.9770134687423706, 0.9078896045684814, 0.8873113989830017, 0.9856892824172974, 0.9078941941261292, 0.9860610961914062, 0.9823741316795349, 0.7047843933105469, 0.2757852077484131, 0.9825875163078308, 0.9825729131698608, 0.8163777589797974, 0.9629762768745422, 0.9569728374481201, 0.8864606618881226, 0.9465821981430054, 0.9725704193115234, 0.9078896045684814, 0.9763318300247192, 0.9572575688362122, 0.9101879596710205, 0.863460898399353, 0.876418948173523, 0.9404656887054443, 0.9448037147521973, 0.9832907319068909, 0.9804715514183044, 0.8985544443130493, 0.920833170413971, 0.9426366090774536, 0.8818357586860657, 0.9022327661514282, 0.950282633304596, 0.9355018734931946, 0.8654168248176575, 0.9644622802734375, 0.9639418125152588, 0.007052463945001364, 0.28209856152534485, 0.32441332936286926, 0.32441332936286926, 0.035262320190668106, 0.007052463945001364, 0.014104927890002728, 0.9377238154411316, 0.8800222873687744, 0.9607990384101868, 0.9034517407417297, 0.9721694588661194, 0.9951408505439758, 0.9954720735549927, 0.9636167287826538, 0.2882218360900879, 0.5764436721801758, 0.981506884098053, 0.9309486746788025, 0.884413480758667, 0.9279217720031738, 0.8855341076850891, 0.8788436055183411, 0.8952972888946533, 0.38002413511276245, 0.6175392270088196, 0.8960734605789185, 0.9613382816314697, 0.8922120928764343, 0.8724969029426575, 0.889026403427124, 0.15657854080200195, 0.7828927040100098, 0.8707526922225952, 0.960938572883606, 0.9227142930030823, 0.9078941941261292, 0.9113067984580994, 0.8909010291099548, 0.8971351981163025, 0.07476126402616501, 0.9766286611557007, 0.9078941941261292, 0.8872586488723755, 0.9716230034828186, 0.8695332407951355, 0.10869165509939194, 0.9848464131355286, 0.9779645800590515, 0.6351354718208313, 0.26634711027145386, 0.02048823982477188, 0.08195295929908752, 0.9697902202606201, 0.8536962866783142, 0.9597026705741882, 0.9676086902618408, 0.9563575387001038, 0.8725040555000305, 0.8574258685112, 0.9295344948768616, 0.9067134261131287, 0.8960734605789185, 0.2823881208896637, 0.6777315139770508, 0.7364525198936462, 0.267800897359848, 0.8872586488723755, 0.9712204933166504, 0.7170173525810242, 0.9473476409912109, 0.9327892065048218, 0.9572349190711975, 0.9120668172836304, 0.9022239446640015, 0.9050891399383545, 0.9143131375312805, 0.9199376106262207, 0.9893774390220642, 0.9626631736755371, 0.8873124718666077, 0.9931706786155701, 0.9818323254585266, 0.962250292301178, 0.9519943594932556, 0.9117739796638489, 0.9515577554702759, 0.8163777589797974, 0.9532621502876282, 0.7737348675727844, 0.12895581126213074, 0.963504433631897, 0.9031050801277161, 0.8873124718666077, 0.9864792823791504, 0.9568361043930054, 0.9208321571350098, 0.9266122579574585, 0.9596105217933655, 0.9617239832878113, 0.923608124256134, 0.9981619715690613, 0.9345589876174927, 0.9659854173660278, 0.891532301902771, 0.8474145531654358, 0.8652849197387695, 0.972699761390686, 0.9630572199821472, 0.9054214358329773, 0.8710174560546875, 0.9577165842056274, 0.9562392234802246, 0.908099889755249, 0.9841495156288147, 0.8873124718666077, 0.9568748474121094, 0.9126837849617004, 0.728671669960022, 0.9396321773529053, 0.9382909536361694, 0.09938432276248932, 0.7950745820999146, 0.9208261370658875, 0.8960734605789185, 0.9897288680076599, 0.9448037147521973, 0.9412046074867249, 0.9879817366600037, 0.9753127694129944, 0.9228914380073547, 0.9965393543243408, 0.9401760697364807, 0.8294666409492493, 0.9508441090583801, 0.4525569975376129, 0.4525569975376129, 0.9265289902687073, 0.9426000118255615, 0.9444717764854431, 0.9306784868240356, 0.8121066093444824, 0.8050389885902405, 0.9856606125831604, 0.9428375959396362, 0.5477747321128845, 0.039126768708229065, 0.011179076507687569, 0.15650707483291626, 0.011179076507687569, 0.01676861383020878, 0.21799199283123016, 0.07317223399877548, 0.8780667781829834, 0.964257001876831, 0.9078941941261292, 0.9658565521240234, 0.9076439738273621, 0.876418948173523, 0.9483076930046082, 0.9501988887786865, 0.9042741656303406, 0.7771850228309631, 0.9022327661514282, 0.9899187088012695, 0.9783961772918701, 0.8202607035636902, 0.1537988930940628, 0.9574553966522217, 0.9855199456214905, 0.9208321571350098, 0.8724868297576904], \"Term\": [\"abstract\", \"adaboost\", \"adam\", \"advance\", \"adversarial\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"alphago\", \"analysis\", \"analysis\", \"analysis\", \"analytic\", \"analyze\", \"apache\", \"apart\", \"apis\", \"appear\", \"application\", \"arima\", \"art\", \"artificial\", \"ask\", \"attempt\", \"attention\", \"author\", \"automate\", \"avai\", \"available\", \"average\", \"bag\", \"base\", \"basic\", \"batch\", \"bayesian\", \"become\", \"beginner\", \"best\", \"boost\", \"brief\", \"browser\", \"build\", \"build\", \"business\", \"buy\", \"calculate\", \"carlo\", \"case_study\", \"chain\", \"challenge\", \"choose\", \"class\", \"classification\", \"classifier\", \"click\", \"click\", \"cloud\", \"cluster\", \"cnn\", \"code\", \"com\", \"come\", \"commerce\", \"commonly\", \"compare\", \"component\", \"computer\", \"concept\", \"consumption\", \"convolutional\", \"core\", \"correct\", \"correlation\", \"course\", \"create\", \"cross_validation\", \"data\", \"datacamp\", \"dataset\", \"datum\", \"datum\", \"day\", \"deal\", \"decision_tree\", \"department\", \"detail\", \"detection\", \"develop\", \"difference\", \"discuss\", \"doctor\", \"document\", \"dynamic\", \"easy\", \"encoder_decoder\", \"ensemble\", \"error\", \"estimate\", \"event\", \"expect\", \"explain\", \"explanation\", \"explore\", \"face\", \"feature\", \"find\", \"first\", \"fit\", \"focus\", \"follow\", \"forecast\", \"forecasting\", \"framework\", \"frequently\", \"fundamental\", \"gan\", \"geitgey\", \"generate\", \"generative\", \"gentle\", \"get\", \"get\", \"get\", \"give\", \"go\", \"good\", \"gradient\", \"gradient\", \"grid_search\", \"handle\", \"heart\", \"heidenreich\", \"help\", \"high\", \"household\", \"however\", \"hunter\", \"hyperparameter\", \"hypothesis\", \"image\", \"implement\", \"implementation\", \"improve\", \"industry\", \"insight\", \"inspir\", \"integrate\", \"interested\", \"interesting\", \"interview\", \"introduction\", \"intuitive\", \"invade\", \"ipython\", \"js\", \"json\", \"jupyter\", \"kera\", \"know\", \"know\", \"lab\", \"large\", \"lasso\", \"last\", \"launch\", \"layer\", \"level\", \"library\", \"like\", \"linear\", \"link\", \"linkedin\", \"list\", \"littl\", \"logistic\", \"long_short\", \"look\", \"lstm\", \"make\", \"making\", \"many\", \"markov\", \"mathematica\", \"matplotlib\", \"may\", \"memory\", \"mix\", \"ml\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modeling\", \"monte\", \"multi\", \"must\", \"natural_language\", \"network\", \"neural\", \"new\", \"nlp\", \"nlp\", \"nonparametric\", \"notebook\", \"number\", \"numpy\", \"object\", \"often\", \"online\", \"open_source\", \"open_source\", \"opt\", \"optimize\", \"option\", \"order\", \"original\", \"originally\", \"originally\", \"outlier\", \"output\", \"overfitt\", \"pain\", \"paper\", \"parameter\", \"part\", \"part\", \"particularly\", \"paruchuri\", \"pdf\", \"perform\", \"performance\", \"performance\", \"picture\", \"popular\", \"post\", \"post\", \"post\", \"post\", \"power\", \"practice\", \"practitioner\", \"predict\", \"prediction\", \"predictive\", \"previous\", \"price\", \"probability\", \"probl\", \"problem\", \"problem\", \"processing\", \"processing\", \"produce\", \"professional\", \"programming\", \"provide\", \"publish\", \"question\", \"random_forest\", \"rank\", \"rate\", \"read\", \"recently\", \"recognition\", \"recurrent\", \"reg\", \"regression\", \"relate\", \"residual\", \"resource\", \"result\", \"review\", \"ridge\", \"right\", \"rnn\", \"rnn\", \"scientist\", \"scikit\", \"scipy\", \"scratch\", \"search\", \"secret\", \"segmentation\", \"selection\", \"sentiment\", \"sequence\", \"series\", \"significance\", \"simple\", \"size\", \"software\", \"solution\", \"solve\", \"sound\", \"speech\", \"stack\", \"start\", \"static\", \"statistic\", \"statistical\", \"statsmodel\", \"step\", \"stop\", \"student\", \"support\", \"task\", \"technique\", \"technique\", \"telling\", \"tempting\", \"tensorflow\", \"term_memory\", \"test\", \"testing\", \"text\", \"thereby\", \"time\", \"tool\", \"topic\", \"train\", \"training\", \"training\", \"translation\", \"tree\", \"trend\", \"tune\", \"tuning\", \"type\", \"understand\", \"unsupervised\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"value\", \"value\", \"various\", \"vik\", \"visualization\", \"visualize\", \"vital\", \"way\", \"web\", \"weight\", \"well\", \"wolfram\", \"work\", \"world\", \"write\", \"write\", \"xgboost\", \"year\", \"zeppelin\", \"zoom\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 8, 3, 9, 1, 7, 5, 10, 6, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el11721112254945824992417318\", ldavis_el11721112254945824992417318_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el11721112254945824992417318\", ldavis_el11721112254945824992417318_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el11721112254945824992417318\", ldavis_el11721112254945824992417318_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "3     -0.241645 -0.095861       1        1  16.244514\n",
       "7      0.180832 -0.280190       2        1  13.743069\n",
       "2      0.126172  0.008725       3        1  10.681095\n",
       "8      0.143672  0.001809       4        1  10.445386\n",
       "0      0.043471  0.158458       5        1   9.709444\n",
       "6     -0.187708 -0.080632       6        1   9.697666\n",
       "4      0.018690  0.095118       7        1   7.833199\n",
       "9      0.008315  0.112714       8        1   7.656472\n",
       "5     -0.107429 -0.001264       9        1   7.340526\n",
       "1      0.015631  0.081123      10        1   6.648633, topic_info=     Category        Freq           Term       Total  loglift  logprob\n",
       "term                                                                  \n",
       "51    Default  180.000000        network  180.000000  30.0000  30.0000\n",
       "19    Default  186.000000          datum  186.000000  29.0000  29.0000\n",
       "52    Default  139.000000         neural  139.000000  28.0000  28.0000\n",
       "104   Default  107.000000         series  107.000000  27.0000  27.0000\n",
       "56    Default  101.000000           time  101.000000  26.0000  26.0000\n",
       "274   Default   79.000000     regression   79.000000  25.0000  25.0000\n",
       "21    Default   74.000000   introduction   74.000000  24.0000  24.0000\n",
       "26    Default  178.000000            use  178.000000  23.0000  23.0000\n",
       "72    Default  141.000000          model  141.000000  22.0000  22.0000\n",
       "235   Default   43.000000          image   43.000000  21.0000  21.0000\n",
       "307   Default   46.000000    forecasting   46.000000  20.0000  20.0000\n",
       "452   Default   52.000000           kera   52.000000  19.0000  19.0000\n",
       "187   Default   36.000000     understand   36.000000  18.0000  18.0000\n",
       "447   Default   51.000000           data   51.000000  17.0000  17.0000\n",
       "262   Default   33.000000        library   33.000000  16.0000  16.0000\n",
       "704   Default   35.000000           year   35.000000  15.0000  15.0000\n",
       "28    Default   64.000000      algorithm   64.000000  14.0000  14.0000\n",
       "240   Default   30.000000  visualization   30.000000  13.0000  13.0000\n",
       "29    Default   40.000000        develop   40.000000  12.0000  12.0000\n",
       "85    Default   26.000000          start   26.000000  11.0000  11.0000\n",
       "395   Default   27.000000       forecast   27.000000  10.0000  10.0000\n",
       "149   Default   39.000000          write   39.000000   9.0000   9.0000\n",
       "461   Default   26.000000        scratch   26.000000   8.0000   8.0000\n",
       "213   Default   23.000000            web   23.000000   7.0000   7.0000\n",
       "250   Default   23.000000    recognition   23.000000   6.0000   6.0000\n",
       "87    Default   23.000000        dataset   23.000000   5.0000   5.0000\n",
       "22    Default   32.000000           know   32.000000   4.0000   4.0000\n",
       "1599  Default   30.000000     generative   30.000000   3.0000   3.0000\n",
       "380   Default   25.000000         gentle   25.000000   2.0000   2.0000\n",
       "385   Default   25.000000          build   25.000000   1.0000   1.0000\n",
       "...       ...         ...            ...         ...      ...      ...\n",
       "2     Topic10   10.782827           help   11.473846   2.6486  -3.9672\n",
       "113   Topic10   10.720602             ml   11.411477   2.6483  -3.9730\n",
       "494   Topic10   10.435995          class   11.126844   2.6467  -3.9999\n",
       "423   Topic10   10.326624      visualize   11.017536   2.6460  -4.0104\n",
       "893   Topic10   10.192018         author   10.882676   2.6452  -4.0235\n",
       "59    Topic10    9.733353            com   10.425014   2.6421  -4.0696\n",
       "144   Topic10    8.975423       notebook    9.667558   2.6365  -4.1507\n",
       "429   Topic10    8.709286        xgboost    9.399916   2.6344  -4.1808\n",
       "229   Topic10    8.325318           best    9.016187   2.6310  -4.2258\n",
       "315   Topic10    8.282699       ensemble    8.973369   2.6307  -4.2310\n",
       "2105  Topic10    8.236805            opt    8.927839   2.6302  -4.2365\n",
       "2106  Topic10    8.236805          probl    8.927839   2.6302  -4.2365\n",
       "2107  Topic10    8.236805       tempting    8.927839   2.6302  -4.2365\n",
       "1172  Topic10    7.884115           tune    8.595880   2.6243  -4.2803\n",
       "768   Topic10    7.158934           easy    7.849647   2.6187  -4.3768\n",
       "1194  Topic10    7.043357           rate    7.734045   2.6172  -4.3931\n",
       "541   Topic10    6.084608         object    6.775572   2.6032  -4.5394\n",
       "114   Topic10    5.087783       solution    5.778443   2.5835  -4.7183\n",
       "3     Topic10    4.638624      interview    5.329229   2.5720  -4.8107\n",
       "301   Topic10    4.066773          basic    4.757418   2.5539  -4.9423\n",
       "1551  Topic10    3.941752           list    4.632520   2.5493  -4.9735\n",
       "305   Topic10    3.901645          stack    4.592330   2.5478  -4.9837\n",
       "1349  Topic10    3.778612       commonly    4.469307   2.5429  -5.0158\n",
       "304   Topic10    3.493291    programming    4.183999   2.5303  -5.0943\n",
       "319   Topic10    3.169328           well    3.860085   2.5136  -5.1916\n",
       "25    Topic10    8.074514      technique   10.061949   2.4907  -4.2564\n",
       "100   Topic10    9.778912     originally   12.773143   2.4436  -4.0649\n",
       "1259  Topic10   12.590611    open_source   21.051294   2.1967  -3.8122\n",
       "22    Topic10    9.496316           know   32.634094   1.4763  -4.0942\n",
       "28    Topic10    5.343138      algorithm   64.862198   0.2143  -4.6693\n",
       "\n",
       "[364 rows x 6 columns], token_table=      Topic      Freq           Term\n",
       "term                                \n",
       "586       5  0.972383       abstract\n",
       "1977      7  0.887044       adaboost\n",
       "960       3  0.987519           adam\n",
       "2122      8  0.876419        advance\n",
       "1691      2  0.959111    adversarial\n",
       "28        1  0.863369      algorithm\n",
       "28        7  0.046252      algorithm\n",
       "28       10  0.077087      algorithm\n",
       "265       1  0.952254     algorithms\n",
       "2084      9  0.887309        alphago\n",
       "33        1  0.706489       analysis\n",
       "33        3  0.185918       analysis\n",
       "33        6  0.074367       analysis\n",
       "202       2  0.954901       analytic\n",
       "528       7  0.783532        analyze\n",
       "2108      6  0.920832         apache\n",
       "2114      5  0.902226          apart\n",
       "414       1  0.974866           apis\n",
       "306       4  0.863277         appear\n",
       "366       1  0.978929    application\n",
       "383       8  0.952478          arima\n",
       "520       7  0.807051            art\n",
       "171       2  0.980250     artificial\n",
       "1055     10  0.909501            ask\n",
       "180       9  0.870605        attempt\n",
       "1407      4  0.939732      attention\n",
       "893      10  0.918892         author\n",
       "735       5  0.925170       automate\n",
       "2111      9  0.887311           avai\n",
       "567       5  0.984174      available\n",
       "...     ...       ...            ...\n",
       "852       7  0.805039           type\n",
       "187       7  0.985661     understand\n",
       "133       1  0.942838   unsupervised\n",
       "26        1  0.547775            use\n",
       "26        2  0.039127            use\n",
       "26        3  0.011179            use\n",
       "26        6  0.156507            use\n",
       "26        7  0.011179            use\n",
       "26        8  0.016769            use\n",
       "26        9  0.217992            use\n",
       "963       3  0.073172          value\n",
       "963       8  0.878067          value\n",
       "287       2  0.964257        various\n",
       "2104      3  0.907894            vik\n",
       "240       9  0.965857  visualization\n",
       "423      10  0.907644      visualize\n",
       "2124      8  0.876419          vital\n",
       "327       6  0.948308            way\n",
       "213      10  0.950199            web\n",
       "1157      3  0.904274         weight\n",
       "319      10  0.777185           well\n",
       "2095      5  0.902233        wolfram\n",
       "270       1  0.989919           work\n",
       "354       1  0.978396          world\n",
       "149       3  0.820261          write\n",
       "149       5  0.153799          write\n",
       "429      10  0.957455        xgboost\n",
       "704       5  0.985520           year\n",
       "2110      6  0.920832       zeppelin\n",
       "931       9  0.872487           zoom\n",
       "\n",
       "[361 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 8, 3, 9, 1, 7, 5, 10, 6, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 2  has Coherence Value of 0.4077\n",
      "Num Topics = 8  has Coherence Value of 0.4728\n",
      "Num Topics = 14  has Coherence Value of 0.4508\n",
      "Num Topics = 20  has Coherence Value of 0.4776\n",
      "Num Topics = 26  has Coherence Value of 0.5065\n",
      "Num Topics = 32  has Coherence Value of 0.5476\n",
      "Num Topics = 38  has Coherence Value of 0.5437\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.060*\"time\" + 0.060*\"series\" + 0.044*\"forecasting\" + 0.044*\"model\" + '\n",
      "  '0.032*\"kera\" + 0.025*\"scratch\" + 0.025*\"grid\" + 0.025*\"search\" + '\n",
      "  '0.025*\"datum\" + 0.019*\"short\"'),\n",
      " (1,\n",
      "  '0.069*\"develop\" + 0.051*\"model\" + 0.042*\"forecasting\" + 0.039*\"time\" + '\n",
      "  '0.039*\"series\" + 0.038*\"neural\" + 0.037*\"network\" + 0.035*\"step\" + '\n",
      "  '0.035*\"multi\" + 0.023*\"convolutional\"'),\n",
      " (2,\n",
      "  '0.069*\"kera\" + 0.048*\"sequence\" + 0.034*\"neural\" + 0.030*\"develop\" + '\n",
      "  '0.030*\"prediction\" + 0.024*\"network\" + 0.021*\"encoder\" + 0.021*\"decoder\" + '\n",
      "  '0.021*\"library\" + 0.021*\"model\"')]\n"
     ]
    }
   ],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = lda_model\n",
    "#model_list[1]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop deep learning models for univar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9141</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to grid search hyperparameters for deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4721</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>time series prediction with deep learning in k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to prepare a photo caption dataset for tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>regression tutorial with the keras deep learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.8999</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to use word embedding layers for deep lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>how to develop a deep learning bag-of-words mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9159</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>multi-class classification tutorial with the k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>save and load your keras deep learning models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>how to use the keras functional api for deep l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             6.0              0.7424   \n",
       "1            1             0.0              0.9141   \n",
       "2            2             5.0              0.4721   \n",
       "3            3             0.0              0.9272   \n",
       "4            4             2.0              0.8626   \n",
       "5            5             6.0              0.8999   \n",
       "6            6             1.0              0.9454   \n",
       "7            7             1.0              0.9159   \n",
       "8            8             3.0              0.8925   \n",
       "9            9             2.0              0.8626   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  develop, time, series, forecasting, model, neu...   \n",
       "1  series, time, model, forecasting, grid, search...   \n",
       "2  series, time, forecasting, lstm, develop, mult...   \n",
       "3  series, time, model, forecasting, grid, search...   \n",
       "4  kera, sequence, prediction, encoder, decoder, ...   \n",
       "5  develop, time, series, forecasting, model, neu...   \n",
       "6  develop, network, neural, kera, model, convolu...   \n",
       "7  develop, network, neural, kera, model, convolu...   \n",
       "8  model, datum, kera, sequence, classification, ...   \n",
       "9  kera, sequence, prediction, encoder, decoder, ...   \n",
       "\n",
       "                                                Text  \n",
       "0  how to develop deep learning models for univar...  \n",
       "1  how to grid search hyperparameters for deep le...  \n",
       "2  time series prediction with deep learning in k...  \n",
       "3  how to prepare a photo caption dataset for tra...  \n",
       "4  regression tutorial with the keras deep learni...  \n",
       "5  how to use word embedding layers for deep lear...  \n",
       "6  how to develop a deep learning bag-of-words mo...  \n",
       "7  multi-class classification tutorial with the k...  \n",
       "8      save and load your keras deep learning models  \n",
       "9  how to use the keras functional api for deep l...  "
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: x[1],reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9501</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to reshape input data for long short-term ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>how to develop a deep learning bag-of-words mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9430</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>how to develop an encoder-decoder model for se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>sequence classification with lstm recurrent ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9338</td>\n",
       "      <td>univariate, forecasting, time, series, grid, n...</td>\n",
       "      <td>how to grid search naive methods for univariat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>how to develop lstm models for multi-step time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9606</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop autoregressive forecasting mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9114</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>how to implement the decision tree algorithm f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.9501   \n",
       "1        1.0              0.9454   \n",
       "2        2.0              0.9430   \n",
       "3        3.0              0.9352   \n",
       "4        4.0              0.9338   \n",
       "5        5.0              0.9596   \n",
       "6        6.0              0.9606   \n",
       "7        7.0              0.9114   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  series, time, model, forecasting, grid, search...   \n",
       "1  develop, network, neural, kera, model, convolu...   \n",
       "2  kera, sequence, prediction, encoder, decoder, ...   \n",
       "3  model, datum, kera, sequence, classification, ...   \n",
       "4  univariate, forecasting, time, series, grid, n...   \n",
       "5  series, time, forecasting, lstm, develop, mult...   \n",
       "6  develop, time, series, forecasting, model, neu...   \n",
       "7  scratch, implement, algorithm, naive, tree, cl...   \n",
       "\n",
       "                                                Text  \n",
       "0  how to reshape input data for long short-term ...  \n",
       "1  how to develop a deep learning bag-of-words mo...  \n",
       "2  how to develop an encoder-decoder model for se...  \n",
       "3  sequence classification with lstm recurrent ne...  \n",
       "4  how to grid search naive methods for univariat...  \n",
       "5  how to develop lstm models for multi-step time...  \n",
       "6  how to develop autoregressive forecasting mode...  \n",
       "7  how to implement the decision tree algorithm f...  "
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9501</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to reshape input data for long short-term ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>how to develop a deep learning bag-of-words mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9430</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>how to develop an encoder-decoder model for se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>sequence classification with lstm recurrent ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9338</td>\n",
       "      <td>univariate, forecasting, time, series, grid, n...</td>\n",
       "      <td>how to grid search naive methods for univariat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>how to develop lstm models for multi-step time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9606</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop autoregressive forecasting mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9114</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>how to implement the decision tree algorithm f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.9501   \n",
       "1        1.0              0.9454   \n",
       "2        2.0              0.9430   \n",
       "3        3.0              0.9352   \n",
       "4        4.0              0.9338   \n",
       "5        5.0              0.9596   \n",
       "6        6.0              0.9606   \n",
       "7        7.0              0.9114   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  series, time, model, forecasting, grid, search...   \n",
       "1  develop, network, neural, kera, model, convolu...   \n",
       "2  kera, sequence, prediction, encoder, decoder, ...   \n",
       "3  model, datum, kera, sequence, classification, ...   \n",
       "4  univariate, forecasting, time, series, grid, n...   \n",
       "5  series, time, forecasting, lstm, develop, mult...   \n",
       "6  develop, time, series, forecasting, model, neu...   \n",
       "7  scratch, implement, algorithm, naive, tree, cl...   \n",
       "\n",
       "                                                Text  \n",
       "0  how to reshape input data for long short-term ...  \n",
       "1  how to develop a deep learning bag-of-words mo...  \n",
       "2  how to develop an encoder-decoder model for se...  \n",
       "3  sequence classification with lstm recurrent ne...  \n",
       "4  how to grid search naive methods for univariat...  \n",
       "5  how to develop lstm models for multi-step time...  \n",
       "6  how to develop autoregressive forecasting mode...  \n",
       "7  how to implement the decision tree algorithm f...  "
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.2167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.0</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>univariate, forecasting, time, series, grid, n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.0</td>\n",
       "      <td>univariate, forecasting, time, series, grid, n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.0</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.0</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.0</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7.0</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7.0</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.0</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3.0</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6.0</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3.0</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.0</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3.0</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.0</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.0</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dominant_Topic                                     Topic_Keywords  \\\n",
       "0              6.0  develop, time, series, forecasting, model, neu...   \n",
       "1              0.0  series, time, model, forecasting, grid, search...   \n",
       "2              5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "3              0.0  series, time, model, forecasting, grid, search...   \n",
       "4              2.0  kera, sequence, prediction, encoder, decoder, ...   \n",
       "5              6.0  develop, time, series, forecasting, model, neu...   \n",
       "6              1.0  develop, network, neural, kera, model, convolu...   \n",
       "7              1.0  develop, network, neural, kera, model, convolu...   \n",
       "8              3.0  model, datum, kera, sequence, classification, ...   \n",
       "9              2.0  kera, sequence, prediction, encoder, decoder, ...   \n",
       "10             3.0  model, datum, kera, sequence, classification, ...   \n",
       "11             0.0  series, time, model, forecasting, grid, search...   \n",
       "12             1.0  develop, network, neural, kera, model, convolu...   \n",
       "13             6.0  develop, time, series, forecasting, model, neu...   \n",
       "14             0.0  series, time, model, forecasting, grid, search...   \n",
       "15             0.0  series, time, model, forecasting, grid, search...   \n",
       "16             7.0  scratch, implement, algorithm, naive, tree, cl...   \n",
       "17             6.0  develop, time, series, forecasting, model, neu...   \n",
       "18             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "19             4.0  univariate, forecasting, time, series, grid, n...   \n",
       "20             6.0  develop, time, series, forecasting, model, neu...   \n",
       "21             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "22             1.0  develop, network, neural, kera, model, convolu...   \n",
       "23             6.0  develop, time, series, forecasting, model, neu...   \n",
       "24             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "25             4.0  univariate, forecasting, time, series, grid, n...   \n",
       "26             3.0  model, datum, kera, sequence, classification, ...   \n",
       "27             7.0  scratch, implement, algorithm, naive, tree, cl...   \n",
       "28             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "29             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "30             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "31             2.0  kera, sequence, prediction, encoder, decoder, ...   \n",
       "32             0.0  series, time, model, forecasting, grid, search...   \n",
       "33             7.0  scratch, implement, algorithm, naive, tree, cl...   \n",
       "34             0.0  series, time, model, forecasting, grid, search...   \n",
       "35             6.0  develop, time, series, forecasting, model, neu...   \n",
       "36             0.0  series, time, model, forecasting, grid, search...   \n",
       "37             6.0  develop, time, series, forecasting, model, neu...   \n",
       "38             1.0  develop, network, neural, kera, model, convolu...   \n",
       "39             7.0  scratch, implement, algorithm, naive, tree, cl...   \n",
       "40             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "41             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "42             2.0  kera, sequence, prediction, encoder, decoder, ...   \n",
       "43             6.0  develop, time, series, forecasting, model, neu...   \n",
       "44             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "45             3.0  model, datum, kera, sequence, classification, ...   \n",
       "46             6.0  develop, time, series, forecasting, model, neu...   \n",
       "47             6.0  develop, time, series, forecasting, model, neu...   \n",
       "48             6.0  develop, time, series, forecasting, model, neu...   \n",
       "49             6.0  develop, time, series, forecasting, model, neu...   \n",
       "50             3.0  model, datum, kera, sequence, classification, ...   \n",
       "51             1.0  develop, network, neural, kera, model, convolu...   \n",
       "52             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "53             1.0  develop, network, neural, kera, model, convolu...   \n",
       "54             1.0  develop, network, neural, kera, model, convolu...   \n",
       "55             1.0  develop, network, neural, kera, model, convolu...   \n",
       "56             1.0  develop, network, neural, kera, model, convolu...   \n",
       "57             3.0  model, datum, kera, sequence, classification, ...   \n",
       "58             0.0  series, time, model, forecasting, grid, search...   \n",
       "59             5.0  series, time, forecasting, lstm, develop, mult...   \n",
       "\n",
       "    Num_Documents  Perc_Documents  \n",
       "0             9.0          0.1500  \n",
       "1            10.0          0.1667  \n",
       "2             4.0          0.0667  \n",
       "3             6.0          0.1000  \n",
       "4             2.0          0.0333  \n",
       "5            12.0          0.2000  \n",
       "6            13.0          0.2167  \n",
       "7             4.0          0.0667  \n",
       "8             NaN             NaN  \n",
       "9             NaN             NaN  \n",
       "10            NaN             NaN  \n",
       "11            NaN             NaN  \n",
       "12            NaN             NaN  \n",
       "13            NaN             NaN  \n",
       "14            NaN             NaN  \n",
       "15            NaN             NaN  \n",
       "16            NaN             NaN  \n",
       "17            NaN             NaN  \n",
       "18            NaN             NaN  \n",
       "19            NaN             NaN  \n",
       "20            NaN             NaN  \n",
       "21            NaN             NaN  \n",
       "22            NaN             NaN  \n",
       "23            NaN             NaN  \n",
       "24            NaN             NaN  \n",
       "25            NaN             NaN  \n",
       "26            NaN             NaN  \n",
       "27            NaN             NaN  \n",
       "28            NaN             NaN  \n",
       "29            NaN             NaN  \n",
       "30            NaN             NaN  \n",
       "31            NaN             NaN  \n",
       "32            NaN             NaN  \n",
       "33            NaN             NaN  \n",
       "34            NaN             NaN  \n",
       "35            NaN             NaN  \n",
       "36            NaN             NaN  \n",
       "37            NaN             NaN  \n",
       "38            NaN             NaN  \n",
       "39            NaN             NaN  \n",
       "40            NaN             NaN  \n",
       "41            NaN             NaN  \n",
       "42            NaN             NaN  \n",
       "43            NaN             NaN  \n",
       "44            NaN             NaN  \n",
       "45            NaN             NaN  \n",
       "46            NaN             NaN  \n",
       "47            NaN             NaN  \n",
       "48            NaN             NaN  \n",
       "49            NaN             NaN  \n",
       "50            NaN             NaN  \n",
       "51            NaN             NaN  \n",
       "52            NaN             NaN  \n",
       "53            NaN             NaN  \n",
       "54            NaN             NaN  \n",
       "55            NaN             NaN  \n",
       "56            NaN             NaN  \n",
       "57            NaN             NaN  \n",
       "58            NaN             NaN  \n",
       "59            NaN             NaN  "
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "develop, time, series, forecasting, model, neural, step, network, multi, kera                    13\n",
       "series, time, forecasting, lstm, develop, multi, model, network, step, forecast                  12\n",
       "develop, network, neural, kera, model, convolutional, sentiment, recognition, review, predict    10\n",
       "series, time, model, forecasting, grid, search, datum, supervise, kera, plan                      9\n",
       "model, datum, kera, sequence, classification, classifi, smartphone, recurrent, save, run          6\n",
       "scratch, implement, algorithm, naive, tree, classifi, decision, baye, anaconda, setup             4\n",
       "kera, sequence, prediction, encoder, decoder, model, develop, library, api, functional            4\n",
       "univariate, forecasting, time, series, grid, naive, search, selection, feature, model             2\n",
       "Name: Keywords, dtype: int64"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topic.Keywords.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop deep learning models for univar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9141</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to grid search hyperparameters for deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4721</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>time series prediction with deep learning in k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to prepare a photo caption dataset for tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>regression tutorial with the keras deep learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.8999</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to use word embedding layers for deep lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>how to develop a deep learning bag-of-words mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9159</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>multi-class classification tutorial with the k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>save and load your keras deep learning models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>how to use the keras functional api for deep l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>evaluate the performance of deep learning mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to grid search deep learning models for ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6978</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>object recognition with convolutional neural n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop a word-level neural language mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8952</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>time series forecasting as supervised learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9141</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to convert a time series to a supervised l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.8614</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>how to setup a python environment for machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9397</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>your first machine learning project in r step-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9158</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>8 tactics to combat imbalanced classes in your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8038</td>\n",
       "      <td>univariate, forecasting, time, series, grid, n...</td>\n",
       "      <td>feature selection for machine learning in python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop machine learning models for mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8262</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>multi-step time series forecasting with machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>how to develop a word embedding model for pred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9606</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop autoregressive forecasting mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>how to develop lstm models for time series for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9338</td>\n",
       "      <td>univariate, forecasting, time, series, grid, n...</td>\n",
       "      <td>how to grid search naive methods for univariat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9119</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>how to model human activity from smartphone data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.6724</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>how to implement the backpropagation algorithm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>how to develop multilayer perceptron models fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>how to use the timedistributed layer for long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9557</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>multi-step time series forecasting with long s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9430</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>how to develop an encoder-decoder model for se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4552</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>tutorial to implement k-nearest neighbors in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>naive bayes classifier from scratch in python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to grid search triple exponential smoothin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9305</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to create an arima model for time series f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9501</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>how to reshape input data for long short-term ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9305</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop a reusable framework to spot-ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>how to use small experiments to develop a capt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9114</td>\n",
       "      <td>scratch, implement, algorithm, naive, tree, cl...</td>\n",
       "      <td>how to implement the decision tree algorithm f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>how to develop lstm models for multi-step time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>how to tune lstm hyperparameters with keras fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>kera, sequence, prediction, encoder, decoder, ...</td>\n",
       "      <td>how to develop an encoder-decoder model with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>time series forecast study with python: monthl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>how to develop baseline forecasts for multi-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>how to run your first classifier in weka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9180</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop a neural machine translation sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop convolutional neural network mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>how to develop convolutional neural networks f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9397</td>\n",
       "      <td>develop, time, series, forecasting, model, neu...</td>\n",
       "      <td>understanding stateful lstm recurrent neural n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>sequence classification with lstm recurrent ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>handwritten digit recognition using convolutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>learn to add numbers with an encoder-decoder l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>how to develop an n-gram multichannel convolut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>text generation with lstm recurrent neural net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>how to develop 1d convolutional neural network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8170</td>\n",
       "      <td>develop, network, neural, kera, model, convolu...</td>\n",
       "      <td>python vs. r (vs. sas) - which tool should i l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8081</td>\n",
       "      <td>model, datum, kera, sequence, classification, ...</td>\n",
       "      <td>the most comprehensive data science learning p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>series, time, model, forecasting, grid, search...</td>\n",
       "      <td>planning late career shift to analytics / big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>series, time, forecasting, lstm, develop, mult...</td>\n",
       "      <td>a complete tutorial to learn data science in r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0             0             6.0              0.7424   \n",
       "1             1             0.0              0.9141   \n",
       "2             2             5.0              0.4721   \n",
       "3             3             0.0              0.9272   \n",
       "4             4             2.0              0.8626   \n",
       "5             5             6.0              0.8999   \n",
       "6             6             1.0              0.9454   \n",
       "7             7             1.0              0.9159   \n",
       "8             8             3.0              0.8925   \n",
       "9             9             2.0              0.8626   \n",
       "10           10             3.0              0.8925   \n",
       "11           11             0.0              0.9272   \n",
       "12           12             1.0              0.6978   \n",
       "13           13             6.0              0.9468   \n",
       "14           14             0.0              0.8952   \n",
       "15           15             0.0              0.9141   \n",
       "16           16             7.0              0.8614   \n",
       "17           17             6.0              0.9397   \n",
       "18           18             5.0              0.9158   \n",
       "19           19             4.0              0.8038   \n",
       "20           20             6.0              0.9569   \n",
       "21           21             5.0              0.8262   \n",
       "22           22             1.0              0.9454   \n",
       "23           23             6.0              0.9606   \n",
       "24           24             5.0              0.9287   \n",
       "25           25             4.0              0.9338   \n",
       "26           26             3.0              0.9119   \n",
       "27           27             7.0              0.6724   \n",
       "28           28             5.0              0.9381   \n",
       "29           29             5.0              0.9381   \n",
       "30           30             5.0              0.9557   \n",
       "31           31             2.0              0.9430   \n",
       "32           32             0.0              0.4552   \n",
       "33           33             7.0              0.8919   \n",
       "34           34             0.0              0.9442   \n",
       "35           35             6.0              0.9305   \n",
       "36           36             0.0              0.9501   \n",
       "37           37             6.0              0.9305   \n",
       "38           38             1.0              0.9382   \n",
       "39           39             7.0              0.9114   \n",
       "40           40             5.0              0.9596   \n",
       "41           41             5.0              0.9381   \n",
       "42           42             2.0              0.7974   \n",
       "43           43             6.0              0.9468   \n",
       "44           44             5.0              0.9596   \n",
       "45           45             3.0              0.8925   \n",
       "46           46             6.0              0.9180   \n",
       "47           47             6.0              0.9468   \n",
       "48           48             6.0              0.9523   \n",
       "49           49             6.0              0.9397   \n",
       "50           50             3.0              0.9352   \n",
       "51           51             1.0              0.9454   \n",
       "52           52             5.0              0.9454   \n",
       "53           53             1.0              0.9454   \n",
       "54           54             1.0              0.9382   \n",
       "55           55             1.0              0.9454   \n",
       "56           56             1.0              0.8170   \n",
       "57           57             3.0              0.8081   \n",
       "58           58             0.0              0.9369   \n",
       "59           59             5.0              0.4238   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   develop, time, series, forecasting, model, neu...   \n",
       "1   series, time, model, forecasting, grid, search...   \n",
       "2   series, time, forecasting, lstm, develop, mult...   \n",
       "3   series, time, model, forecasting, grid, search...   \n",
       "4   kera, sequence, prediction, encoder, decoder, ...   \n",
       "5   develop, time, series, forecasting, model, neu...   \n",
       "6   develop, network, neural, kera, model, convolu...   \n",
       "7   develop, network, neural, kera, model, convolu...   \n",
       "8   model, datum, kera, sequence, classification, ...   \n",
       "9   kera, sequence, prediction, encoder, decoder, ...   \n",
       "10  model, datum, kera, sequence, classification, ...   \n",
       "11  series, time, model, forecasting, grid, search...   \n",
       "12  develop, network, neural, kera, model, convolu...   \n",
       "13  develop, time, series, forecasting, model, neu...   \n",
       "14  series, time, model, forecasting, grid, search...   \n",
       "15  series, time, model, forecasting, grid, search...   \n",
       "16  scratch, implement, algorithm, naive, tree, cl...   \n",
       "17  develop, time, series, forecasting, model, neu...   \n",
       "18  series, time, forecasting, lstm, develop, mult...   \n",
       "19  univariate, forecasting, time, series, grid, n...   \n",
       "20  develop, time, series, forecasting, model, neu...   \n",
       "21  series, time, forecasting, lstm, develop, mult...   \n",
       "22  develop, network, neural, kera, model, convolu...   \n",
       "23  develop, time, series, forecasting, model, neu...   \n",
       "24  series, time, forecasting, lstm, develop, mult...   \n",
       "25  univariate, forecasting, time, series, grid, n...   \n",
       "26  model, datum, kera, sequence, classification, ...   \n",
       "27  scratch, implement, algorithm, naive, tree, cl...   \n",
       "28  series, time, forecasting, lstm, develop, mult...   \n",
       "29  series, time, forecasting, lstm, develop, mult...   \n",
       "30  series, time, forecasting, lstm, develop, mult...   \n",
       "31  kera, sequence, prediction, encoder, decoder, ...   \n",
       "32  series, time, model, forecasting, grid, search...   \n",
       "33  scratch, implement, algorithm, naive, tree, cl...   \n",
       "34  series, time, model, forecasting, grid, search...   \n",
       "35  develop, time, series, forecasting, model, neu...   \n",
       "36  series, time, model, forecasting, grid, search...   \n",
       "37  develop, time, series, forecasting, model, neu...   \n",
       "38  develop, network, neural, kera, model, convolu...   \n",
       "39  scratch, implement, algorithm, naive, tree, cl...   \n",
       "40  series, time, forecasting, lstm, develop, mult...   \n",
       "41  series, time, forecasting, lstm, develop, mult...   \n",
       "42  kera, sequence, prediction, encoder, decoder, ...   \n",
       "43  develop, time, series, forecasting, model, neu...   \n",
       "44  series, time, forecasting, lstm, develop, mult...   \n",
       "45  model, datum, kera, sequence, classification, ...   \n",
       "46  develop, time, series, forecasting, model, neu...   \n",
       "47  develop, time, series, forecasting, model, neu...   \n",
       "48  develop, time, series, forecasting, model, neu...   \n",
       "49  develop, time, series, forecasting, model, neu...   \n",
       "50  model, datum, kera, sequence, classification, ...   \n",
       "51  develop, network, neural, kera, model, convolu...   \n",
       "52  series, time, forecasting, lstm, develop, mult...   \n",
       "53  develop, network, neural, kera, model, convolu...   \n",
       "54  develop, network, neural, kera, model, convolu...   \n",
       "55  develop, network, neural, kera, model, convolu...   \n",
       "56  develop, network, neural, kera, model, convolu...   \n",
       "57  model, datum, kera, sequence, classification, ...   \n",
       "58  series, time, model, forecasting, grid, search...   \n",
       "59  series, time, forecasting, lstm, develop, mult...   \n",
       "\n",
       "                                                 Text  \n",
       "0   how to develop deep learning models for univar...  \n",
       "1   how to grid search hyperparameters for deep le...  \n",
       "2   time series prediction with deep learning in k...  \n",
       "3   how to prepare a photo caption dataset for tra...  \n",
       "4   regression tutorial with the keras deep learni...  \n",
       "5   how to use word embedding layers for deep lear...  \n",
       "6   how to develop a deep learning bag-of-words mo...  \n",
       "7   multi-class classification tutorial with the k...  \n",
       "8       save and load your keras deep learning models  \n",
       "9   how to use the keras functional api for deep l...  \n",
       "10  evaluate the performance of deep learning mode...  \n",
       "11  how to grid search deep learning models for ti...  \n",
       "12  object recognition with convolutional neural n...  \n",
       "13  how to develop a word-level neural language mo...  \n",
       "14     time series forecasting as supervised learning  \n",
       "15  how to convert a time series to a supervised l...  \n",
       "16  how to setup a python environment for machine ...  \n",
       "17  your first machine learning project in r step-...  \n",
       "18  8 tactics to combat imbalanced classes in your...  \n",
       "19   feature selection for machine learning in python  \n",
       "20  how to develop machine learning models for mul...  \n",
       "21  multi-step time series forecasting with machin...  \n",
       "22  how to develop a word embedding model for pred...  \n",
       "23  how to develop autoregressive forecasting mode...  \n",
       "24  how to develop lstm models for time series for...  \n",
       "25  how to grid search naive methods for univariat...  \n",
       "26   how to model human activity from smartphone data  \n",
       "27  how to implement the backpropagation algorithm...  \n",
       "28  how to develop multilayer perceptron models fo...  \n",
       "29  how to use the timedistributed layer for long ...  \n",
       "30  multi-step time series forecasting with long s...  \n",
       "31  how to develop an encoder-decoder model for se...  \n",
       "32  tutorial to implement k-nearest neighbors in p...  \n",
       "33      naive bayes classifier from scratch in python  \n",
       "34  how to grid search triple exponential smoothin...  \n",
       "35  how to create an arima model for time series f...  \n",
       "36  how to reshape input data for long short-term ...  \n",
       "37  how to develop a reusable framework to spot-ch...  \n",
       "38  how to use small experiments to develop a capt...  \n",
       "39  how to implement the decision tree algorithm f...  \n",
       "40  how to develop lstm models for multi-step time...  \n",
       "41  how to tune lstm hyperparameters with keras fo...  \n",
       "42  how to develop an encoder-decoder model with a...  \n",
       "43  time series forecast study with python: monthl...  \n",
       "44  how to develop baseline forecasts for multi-si...  \n",
       "45           how to run your first classifier in weka  \n",
       "46  how to develop a neural machine translation sy...  \n",
       "47  how to develop convolutional neural network mo...  \n",
       "48  how to develop convolutional neural networks f...  \n",
       "49  understanding stateful lstm recurrent neural n...  \n",
       "50  sequence classification with lstm recurrent ne...  \n",
       "51  handwritten digit recognition using convolutio...  \n",
       "52  learn to add numbers with an encoder-decoder l...  \n",
       "53  how to develop an n-gram multichannel convolut...  \n",
       "54  text generation with lstm recurrent neural net...  \n",
       "55  how to develop 1d convolutional neural network...  \n",
       "56  python vs. r (vs. sas) - which tool should i l...  \n",
       "57  the most comprehensive data science learning p...  \n",
       "58  planning late career shift to analytics / big ...  \n",
       "59  a complete tutorial to learn data science in r...  "
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.drop(['Unnamed: 0','Unnamed: 0.1'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_1,df_dominant_topic],1).to_csv('topic_cluster_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv('topic_cluster_0.csv')\n",
    "df_1 = pd.read_csv('topic_cluster_1.csv')\n",
    "df_2 = pd.read_csv('topic_cluster_2.csv')\n",
    "df_3 = pd.read_csv('topic_cluster_3.csv')\n",
    "df_4 = pd.read_csv('topic_cluster_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_0,df_1,df_2,df_3,df_4],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0','index'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('Keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['cluster'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Cluster_k</th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5654.000000</td>\n",
       "      <td>5706.0</td>\n",
       "      <td>5706.000000</td>\n",
       "      <td>5706.000000</td>\n",
       "      <td>5706.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>965.443403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2852.500000</td>\n",
       "      <td>37.311076</td>\n",
       "      <td>0.157310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>440.486959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1647.324649</td>\n",
       "      <td>20.298564</td>\n",
       "      <td>0.065637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>207.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>604.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1426.250000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>900.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2852.500000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1277.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4278.750000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5705.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.552900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word Count  Cluster_k  Document_No  Dominant_Topic  Topic_Perc_Contrib\n",
       "count  5654.000000     5706.0  5706.000000     5706.000000         5706.000000\n",
       "mean    965.443403        0.0  2852.500000       37.311076            0.157310\n",
       "std     440.486959        0.0  1647.324649       20.298564            0.065637\n",
       "min     207.000000        0.0     0.000000        0.000000            0.045400\n",
       "25%     604.250000        0.0  1426.250000       19.000000            0.110700\n",
       "50%     900.000000        0.0  2852.500000       38.000000            0.145000\n",
       "75%    1277.750000        0.0  4278.750000       56.000000            0.196300\n",
       "max    2009.000000        0.0  5705.000000       69.000000            0.552900"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Cluster_k == 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Cluster_k</th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17767.116667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.883057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5293.360991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.464249</td>\n",
       "      <td>2.404093</td>\n",
       "      <td>0.117751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12528.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13859.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15652.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19645.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32910.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.960600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word Count  Cluster_k  Document_No  Dominant_Topic  \\\n",
       "count     60.000000       60.0    60.000000       60.000000   \n",
       "mean   17767.116667        1.0    29.500000        3.500000   \n",
       "std     5293.360991        0.0    17.464249        2.404093   \n",
       "min    12528.000000        1.0     0.000000        0.000000   \n",
       "25%    13859.750000        1.0    14.750000        1.000000   \n",
       "50%    15652.500000        1.0    29.500000        4.000000   \n",
       "75%    19645.750000        1.0    44.250000        6.000000   \n",
       "max    32910.000000        1.0    59.000000        7.000000   \n",
       "\n",
       "       Topic_Perc_Contrib  \n",
       "count           60.000000  \n",
       "mean             0.883057  \n",
       "std              0.117751  \n",
       "min              0.423800  \n",
       "25%              0.892350  \n",
       "50%              0.929600  \n",
       "75%              0.945400  \n",
       "max              0.960600  "
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Cluster_k == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Cluster_k</th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>55191.125000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15904.630408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.44949</td>\n",
       "      <td>0.46291</td>\n",
       "      <td>0.018176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>38898.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>41201.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.964925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>51766.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>66693.750000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.979675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83127.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.983900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word Count  Cluster_k  Document_No  Dominant_Topic  \\\n",
       "count      8.000000        8.0      8.00000         8.00000   \n",
       "mean   55191.125000        2.0      3.50000         0.25000   \n",
       "std    15904.630408        0.0      2.44949         0.46291   \n",
       "min    38898.000000        2.0      0.00000         0.00000   \n",
       "25%    41201.000000        2.0      1.75000         0.00000   \n",
       "50%    51766.500000        2.0      3.50000         0.00000   \n",
       "75%    66693.750000        2.0      5.25000         0.25000   \n",
       "max    83127.000000        2.0      7.00000         1.00000   \n",
       "\n",
       "       Topic_Perc_Contrib  \n",
       "count            8.000000  \n",
       "mean             0.969500  \n",
       "std              0.018176  \n",
       "min              0.932900  \n",
       "25%              0.964925  \n",
       "50%              0.979000  \n",
       "75%              0.979675  \n",
       "max              0.983900  "
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Cluster_k == 2].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Cluster_k</th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.0</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7247.347432</td>\n",
       "      <td>3.0</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>3.649547</td>\n",
       "      <td>0.694141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1779.879719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.695698</td>\n",
       "      <td>2.322060</td>\n",
       "      <td>0.161329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5146.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5696.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.551500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6785.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8527.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12376.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.931200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word Count  Cluster_k  Document_No  Dominant_Topic  \\\n",
       "count    331.000000      331.0   331.000000      331.000000   \n",
       "mean    7247.347432        3.0   165.000000        3.649547   \n",
       "std     1779.879719        0.0    95.695698        2.322060   \n",
       "min     5146.000000        3.0     0.000000        0.000000   \n",
       "25%     5696.500000        3.0    82.500000        2.000000   \n",
       "50%     6785.000000        3.0   165.000000        4.000000   \n",
       "75%     8527.000000        3.0   247.500000        5.000000   \n",
       "max    12376.000000        3.0   330.000000        7.000000   \n",
       "\n",
       "       Topic_Perc_Contrib  \n",
       "count          331.000000  \n",
       "mean             0.694141  \n",
       "std              0.161329  \n",
       "min              0.247000  \n",
       "25%              0.551500  \n",
       "50%              0.711600  \n",
       "75%              0.824700  \n",
       "max              0.931200  "
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Cluster_k == 3].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Cluster_k</th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1510.000000</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1510.000000</td>\n",
       "      <td>1510.000000</td>\n",
       "      <td>1510.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3040.836424</td>\n",
       "      <td>4.0</td>\n",
       "      <td>754.500000</td>\n",
       "      <td>11.001325</td>\n",
       "      <td>0.402265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>809.855591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>436.043767</td>\n",
       "      <td>5.588376</td>\n",
       "      <td>0.127880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2339.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>377.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2841.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>754.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3616.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1131.750000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5132.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1509.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.794100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word Count  Cluster_k  Document_No  Dominant_Topic  Topic_Perc_Contrib\n",
       "count  1510.000000     1510.0  1510.000000     1510.000000         1510.000000\n",
       "mean   3040.836424        4.0   754.500000       11.001325            0.402265\n",
       "std     809.855591        0.0   436.043767        5.588376            0.127880\n",
       "min    2010.000000        4.0     0.000000        0.000000            0.130400\n",
       "25%    2339.000000        4.0   377.250000        6.000000            0.317700\n",
       "50%    2841.500000        4.0   754.500000       12.000000            0.382900\n",
       "75%    3616.750000        4.0  1131.750000       16.000000            0.477100\n",
       "max    5132.000000        4.0  1509.000000       19.000000            0.794100"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Cluster_k == 4].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
